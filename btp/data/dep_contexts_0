Following the notation in section 2.1 , the ij-th entry of the matrix W is defined as in CITATION Wj = (1 - e Wz ) where Cj is the total number of words in the j-th sentence pair .
For the simple bag-of-word bilingual LSA as described in Section 2.2.1 , after SVD on the sparse matrix using the toolkit SVDPACK CITATION , all source and target words are projected into a low-dimensional (R = 88) LSA-space .
Discriminative word alignment models , such as Ittycheriah and Roukos CITATION; Moore CITATION; Blunsom and Cohn CITATION , have received great amount of study recently .
For instance , the 1 most relaxed IBM Model-1 , which assumes that any source word can be generated by any target word equally re.g.rdless of distance , can be improved by demanding a Markov process of alignments as in HMM-based models CITATION , or implementing a distribution of number of target words linked to a source word as in IBM fertility-based models CITATION .
It can be applied to complicated models such IBM Model-4 CITATION .
The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing CITATION using all English sentences in the parallel training data .
LSA has been successfully applied to information retrieval CITATION , statistical langauge modeling CITATION and etc .
Alternative constructions of the matrix are possible using raw counts or TF-IDF CITATION .
It has been shown that human knowledge , in the form of a small amount of manually annotated parallel data to be used to seed or guide model training , can significantly improve word alignment F-measure and translation performance CITATION .
Our decoder is a phrase-based multi-stack imple-mentation of the log-linear model similar to Pharaoh CITATION .
Since Arabic is a morphologically rich language where affixes are attached to stem words to indicate gender , tense , case and etc , in order to reduce vocabulary size and address out-of-vocabulary words , we split Arabic words into affix and root according to a rule-based se.g.entation scheme CITATION with the help from the Buckwalter analyzer CITATION output .
Basic models in two translation directions are trained simultaneously where statistics of two directions are shared to learn symmetric translation lexicon and word alignments with high precision motivated by CITATION and CITATION .
In CITATION , bilingual semantic maps are constructed to guide word alignment .
As formulated in the competitive linking algorithm CITATION , the problem of word alignment can be re.g.rded as a process of word linkage disambiguation , that is , choosing correct associations among all competing hypothesis .
The example demos that due to reasonable constraints placed in word alignment training , the link to "_tK" is corrected and consequently we have accurate word translation for the Arabic singleton 7 Heuristics based on co-occurrence analysis , such as point-wise mutual information or Dice coefficients  , have been shown to be indicative for word alignments CITATION .
These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method CITATION .
By combining word alignments in two directions using heuristics CITATION , a single set of static word alignments is then formed .
We simply modify the GIZA++ toolkit CITATION by always weighting lexicon probabilities with soft constraints during iterative model training , and obtain 0.7% TER reduction on both sets and 0.4% BLEU improvement on the test set .
We measure translation performance by the BLEU score CITATION and Translation Error Rate (TER) CITATION with one reference for each hypothesis .
Scientists  CITATION augmented bilingual sentence pairs with part-of-speech tags as linguistic constraints for HMM-based word alignments .
While word alignments can help identifying semantic relations CITATION , we proceed in the reverse direction .
We shall take HMM-based word alignment model CITATION as an example and follow the notation of CITATION .
Our baseline word alignment model is the word-to-word Hidden Markov Model CITATION .
Many state-of-the-art SMT systems do not use trees and base the ordering decisions on surface phrases CITATION .
An important advantage of our model is that it is global , and does not decompose the task of ordering a target sentence into a series of local decisions , as in the recently proposed order models for Machine Transition CITATION .
Alternatively , order is modelled in terms of movement of automatically induced hierarchical structure of sentences CITATION .
These N-best lists are generated using approximate search and simpler models , as in the re-ranking approach of CITATION .
tings , even for a bi-gram language model CITATION .
Previous work has shown that it is useful to model target language order in terms of movement of syntactic constituents in constituency trees CITATION or dependency trees CITATION , which are obtained using a parser trained to determine linguistic constituency .
Our results show that combining features derived from the source and target dependency trees , distortion surface order-based features (like the distortion used in Pharaoh CITATION) and language model-like features results in a model which significantly outperforms models using only some of the information sources .
Pharaoh DISP: Displacement as used in Pharaoh CITATION .
These models are combined as feature functions in a (log)linear model for predicting a target sentence given a source sentence , in the framework proposed by CITATION .
The target dependency trees are obtained through projection of the source dependency trees , using the word alignment (we use GIZA++ CITATION) , ensuring better parallelism of the source and target structures .
The sentences were annotated with alignment (using GIZA++ CITATION) and syntactic dependency structures of the source and target , obtained as described in Section 2 .
Our model is discriminatively trained to select the best order (according to the BLEU measure) CITATION of an unordered target dependency tree from the space of possible orders .
Our algorithm for obtaining target dependency trees by projection of the source trees via the word alignment is the one used in the MT system of CITATION .
It follows the order model defined in CITATION .
Our baseline SMT system is the system of Scientists  CITATION .
The projection algorithm of CITATION defines heuristics for each of these problems .
1 Previous studies have shown that if both the source and target dependency trees represent linguistic constituency , the alignment between subtrees in the two languages is very complex CITATION .
A host of discriminative methods have been introduced CITATION .
2 We also investigated extraction-specific metrics: the frequency of interior nodes - a measure of how often the alignments violate the constituent structure of English parses - and a variant of the CPER metric of Ayan and Dorr CITATION .
* From Ayan and Dorr CITATION , grow-diag-final heuristic .
5 Similarly , we compared our Chinese results to the GIZA++ results in Ayan and Dorr CITATION .
Additionally , we evaluated our model with the transducer analog to the consistent phrase error rate (CPER) metric of Ayan and Dorr CITATION .
However , few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them CITATION .
Syntactic methods are an increasingly promising approach to statistical machine translation , being both algorithmically appealing CITATION and empirically successful CITATION .
Daume III and Marcu CITATION employs a syntax-aware distortion model for aligning summaries to documents , but condition upon the roots of the constituents that are jumped over during a transition , instead ofthose that are visited during a walk through the tree .
Our transductive learning algorithm , Algorithm 1 , is inspired by the Yarowsky algorithm CITATION .
Under certain precise conditions , as described in CITATION , we can analyze Algorithm 1 as minimizing the entropy of the distribution over translations of U .
One language pair creates data for another language pair and can be naturally used in a CITATION-style co-training algorithm .
These lists are rescored with the following models: (a) the different models used in the decoder which are described above , (b) two different features based on IBM Model l CITATION , (c) posterior probabilities for words , phrases , n-grams , and sentence length CITATION , all calculated over the N-best list and using the sentence probabilities which the baseline system assigns to the translation hypotheses .
In CITATION , a generative model for word alignment is trained using unsupervised learning on parallel text .
In CITATION co-training is applied to MT .
Along similar lines , CITATION combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences .
BLEU score using the algorithm described in CITATION .
for a detailed description see CITATION .
For details , see CITATION .
It overlaps with the original phrase tables , but also contains many new phrase pairs CITATION .
Self-training for SMT was proposed in CITATION .
Recently , Cabezas and Resnik CITATION experimented with incorporating WSD translations into Pharaoh , a state-of-the-art phrase-based MT system CITATION .
The relatively small improvement reported by Cabezas and Resnik CITATION without a statistical significance test appears to be inconclusive .
Note that comparing with the MT systems used in CITATION and CITATION , the Hiero system we are using represents a much stronger baseline MT system upon which the WSD system must improve .
Carpuat and Wu CITATION inte.g.ated the translation predictions from a Chinese WSD system CITATION into a Chinese-English word-based statistical MT system using the ISI ReWrite decoder CITATION .
Note that the experiments in CITATION did not use a state-of-the-art MT system , while the experiments in CITATION were not done using a full-fledged MT system and the evaluation was not on how well each source sentence was translated as a whole .
We obtain accuracy that compares favorably to the best participating system in the task CITATION .
For our experiments , we use the SVM implementation of CITATION as it is able to work on multi-class problems to output the classification probability for each class .
Capitalizing on the strength of the phrase-based approach , Chiang CITATION introduced a hierarchical phrase-based statistical MT system , Hiero , which achieves significantly better translation performance than Pharaoh CITATION , which is a state-of-the-art phrase-based statistical MT system .
In this paper , we successfully inte.g.ate a state-of-the-art WSD system into the state-of-the-art hierarchical phrase-based MT system , Hiero CITATION .
Hiero CITATION is a hierarchical phrase-based model for statistical machine translation , based on weighted synchronous context-free grammar (CFG) CITATION .
Similar to CITATION , we trained the Hiero system on the FBIS corpus , used the NIST MT 2002 evaluation test set as our development set to tune the feature weights , and the NIST MT 2003 evaluation test set as our test data .
Following CITATION , we used the version 11a NIST BLEU script with its default settings to calculate the BLEU scores CITATION based on case-insensitive n-gram matching , where n is up to 4 .
A n-gram language model adds a dependence on (n—1) neighboring target-side words CITATION , making decoding much more difficult but still polynomial; in this paper , we add features that depend on the neighboring source-side words , which does not affect decoding complexity at all because the source string is fixed .
The improvement of 0.57 is statistically significant at p  < 0.05 using the sign-test as described by Scientists  CITATION , with 374 (+1) , 318 (—1) and 227 (0) .
To perform translation , state-of-the-art MT systems use a statistical phrase-based approach CITATION by treating phrases as the basic units of translation .
The word alignments of both directions are then combined into a single set of alignments using the "diag-and" method of Scientists  CITATION .
Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results CITATION .
Our implemented WSD classifier uses the knowledge sources of local collocations , parts-of-speech (POS) , and surrounding words , following the successful approach of CITATION .
First , we performed word alignment on the FBIS parallel corpus using GIZA++ CITATION in both directions .
Hiero uses a general log-linear model CITATION where the weight of a derivation  D for a particular source sentence and its translation is where  ^  i is a feature function and  X i is the weight for feature  ^  i .
Using the MT 2002 test set , we ran the minimum-error rate training (MERT) CITATION with the decoder to tune the weights for each feature .
the English portion of the FBIS corpus and the Xinhua portion of the Gigaword corpus , we trained a tri-gram language model using the SRI Language Modelling Toolkit CITATION .
Although it has been argued that WSD does not yield better translation quality than a machine translation system alone , it has been recently shown that a WSD module that is developed following specific multilingual requirements can significantly improve the performance of a machine translation system CITATION .
Finally , MC-WSD CITATION is a multi-class averaged perceptron classifier using syntactic and narrow context features , with one component trained on the data provided by Senseval and other trained on WordNet glosses .
It is an interesting approach to learning which has been considered promising for several applications in natural language processing and has been explored for a few of them , namely POS-tagging , grammar acquisition and semantic parsing CITATION .
For example , Dang and Palmer CITATION also use a rich set of features with a traditional learning algorithm (maximum entropy) .
Linguistic knowledge is available in electronic resources suitable for practical use , such as WordNet CITATION , dictionaries and parsers .
There is not always a direct relation between the possible senses for a word in a (monolingual) lexicon and its translations to a particular language , so this represents a different task to WSD against a (monolingual) lexicon CITATION .
CLaC1 CITATION uses a Naive Bayes algorithm with a dynamically adjusted context window around the target word .
The sense with the highest count of overlapping words in its dictionary definition and in the sentence containing the target verb (excluding stop words)   CITATION ,   represented by has_overlapping(sentence , translation) : has_overlapping(snt 1 , voltar) .
Verbs and possible senses in our corpus Both corpora were lemmatized and part-of-speech (POS) tagged using Minipar CITATION and Mxpost CITATION , respectivelly .
These approaches have shown good results; particularly those using supervised learning CITATION .
WSD systems have generally been more successful in the disambiguation of nouns than other grammatical cate.g.ries CITATION .
Syntalex-3 CITATION is based on an ensemble of bagged decision trees with narrow context part-of-speech features and bigrams .
This is achieved using Inductive Logic Programming (ILP) CITATION , which has not yet been applied to WSD .
Inductive Logic Programming CITATION employs techniques from Machine Learning and Logic Programming to build first-order theories from examples and background knowledge , which are also represented by first-order clauses .
This corpus was automatically annotated with the translation of the verb using a tagging system based on parallel corpus , statistical information and translation dictionaries CITATION , followed by a manual revision .
All the knowledge sources were made available to be used by the inference engine , since previous experiments showed that they are all relevant CITATION .
We use the Aleph ILP system CITATION , which provides a complete inference engine and can be customized in various ways .
In the hybrid approaches that have been explored so far , deep knowledge , like selectional preferences , is either pre-processed into a vector representation to accommodate machine learning algorithms , or used in previous steps to filter out possible senses e.g. CITATION .
