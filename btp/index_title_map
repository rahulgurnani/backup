1865646 :: Training Tree Transducers :: natural_language_and_speech_data.txt :: Many probabilistic models for natural language are now written in terms of hierarchical tree structure. Tree-based modeling still lacks many of the standard tools taken for granted in (finite- state) string-based modeling. The theory of tree transducer automata provides a possible frame- work to draw on, as it has been worked out in an extensive literature. We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-to- tree and tree-to-string transducers. :: Many probabilistic models for natural language are now written in terms of hierarchical tree structure. Tree-based modeling still lacks many of the standard tools taken for granted in (finite- state) string-based modeling. The theory of tree transducer automata provides a possible frame- work to draw on, as it has been worked out in an extensive literature. We motivate the use of tree transducers for natural language and address the training problem for probabilistic tree-to- tree and tree-to-string transducers.
4231115 :: The Infinite Tree :: natural_language_and_speech_data.txt :: Historically, unsupervised learning tech- niques have lacked a principled technique for selecting the number of unseen compo- nents. Research into non-parametric priors, such as the Dirichlet process, has enabled in- stead the use of infinite models, in which the number of hidden categories is not fixed, but can grow with the amount of training data. Here we develop the infinite tree, a new infi- nite model capable of representing recursive branching structure over an arbitrarily large set of hidden categories. Specifically, we develop three infinite tree models, each of which enforces different independence as- sumptions, and for each model we define a simple direct assignmentsampling inference procedure. We demonstrate the utility of our models by doing unsupervised learning of part-of-speech tags from treebank depen- dency skeleton structure, achieving an accu- racy of 75.34%, and by doing unsupervised splitting of part-of-speech tags, which in- creases the accuracy of a generative depen- dency parser from 85.11% to 87.35%. :: Historically, unsupervised learning tech- niques have lacked a principled technique for selecting the number of unseen compo- nents. Research into non-parametric priors, such as the Dirichlet process, has enabled in- stead the use of infinite models, in which the number of hidden categories is not fixed, but can grow with the amount of training data. Here we develop the infinite tree, a new infi- nite model capable of representing recursive branching structure over an arbitrarily large set of hidden categories. Specifically, we develop three infinite tree models, each of which enforces different independence as- sumptions, and for each model we define a simple direct assignmentsampling inference procedure. We demonstrate the utility of our models by doing unsupervised learning of part-of-speech tags from treebank depen- dency skeleton structure, achieving an accu- racy of 75.34%, and by doing unsupervised splitting of part-of-speech tags, which in- creases the accuracy of a generative depen- dency parser from 85.11% to 87.35%.
144147 :: Error-Based and Entropy-Based Discretization of Continuous Features :: data_mining_data.txt :: 169 :: 169
16479 :: Text Generation and Systemic-Functional Linguistics: Experiences from English and Japanese :: natural_language_and_speech_data.txt :: istinctive benefits of functionaltheories ... for NLP?In fact, the usefulness of the system networks lies primarily in their ability, as a classificationformalism, to provide functional rather than structural descriptions. It is nocoincidence that Systemic Grammar and other functional descriptions of language (e.g.,Functional Unification Grammar; see Appelt [1983]) are widely used in text generation(the first half of the title currently under review). The question of their distinctive... :: istinctive benefits of functionaltheories ... for NLP?In fact, the usefulness of the system networks lies primarily in their ability, as a classificationformalism, to provide functional rather than structural descriptions. It is nocoincidence that Systemic Grammar and other functional descriptions of language (e.g.,Functional Unification Grammar; see Appelt [1983]) are widely used in text generation(the first half of the title currently under review). The question of their distinctive...
115479 :: Instance-Based Natural Language Generation :: natural_language_and_speech_data.txt :: This paper presents a bottom-up generator that makes use of Information Retrieval techniques to rank potential generation candidates by comparing them to a data base of stored instances. We introduce two general techniques to address the search problem, expectation-driven search and dynamic grammar rule selection, and present the architecture of an implemented generation system called Our approach uses a domain-specific generation grammar that is automatically derived from a semantically tagged treebank. We then evaluate the efficiency of our system. :: This paper presents a bottom-up generator that makes use of Information Retrieval techniques to rank potential generation candidates by comparing them to a data base of stored instances. We introduce two general techniques to address the search problem, expectation-driven search and dynamic grammar rule selection, and present the architecture of an implemented generation system called Our approach uses a domain-specific generation grammar that is automatically derived from a semantically tagged treebank. We then evaluate the efficiency of our system.
141312 :: Automatic Retrieval and Clustering of Similar Words :: natural_language_and_speech_data.txt :: Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is. :: Bootstrapping semantics from text is one of the greatest challenges in natural language learning. We first define a word similarity measure based on the distributional pattern of words. The similarity measure allows us to construct a thesaurus using a parsed corpus. We then present a new evaluation methodology for the automatically constructed thesaurus. The evaluation results show that the thesaurus is significantly closer to WordNet than Roget Thesaurus is.
2510330 :: Hybrid Natural Language Generation from Lexical Conceptual Structures :: natural_language_and_speech_data.txt :: This paper describes Lexogen, a system for generating natural-language sentences from Lexical Conceptual Structure, an interlingual representation. The system has been developed as part of a Chinese-English Machine Translation (MT) system; however, it is designed to be used for many other MT language pairs and natural language applications. The contributions of this work include: (1) development of a large-scale Hybrid Natural Language Generation system with language-independent components; (2) enhancements to an interlingual representation and asso- ciated algorithm for generation from ambiguous input; (3) development of an efficient reusable language-independent linearization module with a grammar description language that can be used with other systems; (4) improvements to an earlier algorithm for hierarchically mapping thematic roles to surface positions; and (5) development of a diagnostic tool for lexicon coverage and correct- ness and use of the tool for verification of English, Spanish, and Chinese lexicons. An evaluation of Chinese-English translation quality shows comparable performance with a commercial translation system. The generation system can also be extended to other languages and this is demonstrated and evaluated for Spanish. :: This paper describes Lexogen, a system for generating natural-language sentences from Lexical Conceptual Structure, an interlingual representation. The system has been developed as part of a Chinese-English Machine Translation (MT) system; however, it is designed to be used for many other MT language pairs and natural language applications. The contributions of this work include: (1) development of a large-scale Hybrid Natural Language Generation system with language-independent components; (2) enhancements to an interlingual representation and asso- ciated algorithm for generation from ambiguous input; (3) development of an efficient reusable language-independent linearization module with a grammar description language that can be used with other systems; (4) improvements to an earlier algorithm for hierarchically mapping thematic roles to surface positions; and (5) development of a diagnostic tool for lexicon coverage and correct- ness and use of the tool for verification of English, Spanish, and Chinese lexicons. An evaluation of Chinese-English translation quality shows comparable performance with a commercial translation system. The generation system can also be extended to other languages and this is demonstrated and evaluated for Spanish.
1278889 :: Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy :: common_data.txt :: This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task. :: This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task.
2414885 :: Hybrid Parsing: Using Probabilistic Models as Predictors for a Symbolic Parser :: natural_language_and_speech_data.txt :: In this paper we investigate the benefit of stochastic predictor components for the parsing quality which can be obtained with a rule-based dependency grammar. By in- cluding a chunker, a supertagger, a PP at- tacher, and a fast probabilistic parser we were able to improve upon the baseline by 3.2%, bringing the overall labelled accu- racy to 91.1% on the German NEGRA cor- pus. We attribute the successful integra- tion to the ability of the underlying gram- mar model to combine uncertain evidence in a soft manner, thus avoiding the prob- lem of error propagation. :: In this paper we investigate the benefit of stochastic predictor components for the parsing quality which can be obtained with a rule-based dependency grammar. By in- cluding a chunker, a supertagger, a PP at- tacher, and a fast probabilistic parser we were able to improve upon the baseline by 3.2%, bringing the overall labelled accu- racy to 91.1% on the German NEGRA cor- pus. We attribute the successful integra- tion to the ability of the underlying gram- mar model to combine uncertain evidence in a soft manner, thus avoiding the prob- lem of error propagation.
4231125 :: A Symbolic Approach to Near-Deterministic Surface Realisation using Tree Adjoining Grammar :: natural_language_and_speech_data.txt :: Surface realisers divide into those used in generation (NLG geared realisers) and those mirroring the parsing process (Reversible re- alisers). While the first rely on grammars not easily usable for parsing, it is unclear how the second type of realisers could be param- eterised to yield from among the set of pos- sible paraphrases, the paraphrase appropri- ate to a given generation context. In this pa- per, we present a surface realiser which com- bines a reversible grammar (used for pars- ing and doing semantic construction) with a symbolic means of selecting paraphrases. :: Surface realisers divide into those used in generation (NLG geared realisers) and those mirroring the parsing process (Reversible re- alisers). While the first rely on grammars not easily usable for parsing, it is unclear how the second type of realisers could be param- eterised to yield from among the set of pos- sible paraphrases, the paraphrase appropri- ate to a given generation context. In this pa- per, we present a surface realiser which com- bines a reversible grammar (used for pars- ing and doing semantic construction) with a symbolic means of selecting paraphrases.
648944 :: Feature structures based Tree Adjoining Grammars :: natural_language_and_speech_data.txt :: We have embedded Tree Adjoining Grammars (TAG) in a feature structure based unification system. The resulting system, Feature Structure based Tree Adjoining Grammars (FTAG), captures the principle of factoring dependencies and recursion, fundamental to TAG's. We show that FTAG has an enhanced descriptive capacity compared to TAG formalism. We consider some restricted versions of this system and some possible linguistic stipulations that can be made. We briefly describe a calculus to represent the structures used by this system, extending on the work of Rounds, and Kasper [Rounds et al. 1986, Kasper et al. 1986] involving the logical formulation of feature structures. :: We have embedded Tree Adjoining Grammars (TAG) in a feature structure based unification system. The resulting system, Feature Structure based Tree Adjoining Grammars (FTAG), captures the principle of factoring dependencies and recursion, fundamental to TAG's. We show that FTAG has an enhanced descriptive capacity compared to TAG formalism. We consider some restricted versions of this system and some possible linguistic stipulations that can be made. We briefly describe a calculus to represent the structures used by this system, extending on the work of Rounds, and Kasper [Rounds et al. 1986, Kasper et al. 1986] involving the logical formulation of feature structures.
17406 :: Word association norms, mutual information, and lexicography :: natural_language_and_speech_data.txt :: The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor. ) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words. :: The term word association is used in a very particular sense in the psycholinguistic literature. (Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor. ) We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word). This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora. (The standard method of obtaining word association norms, testing a few thousand subjects on a few hundred words, is both costly and unreliable.) The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words.
4727037 :: Lexicalized Markov Grammars for Sentence Compression :: natural_language_and_speech_data.txt :: We present a sentence compression system based on synchronous context-free grammars (SCFG), following the successful noisy-channel approach of (Knight and Marcu, 2000). We define a head- driven Markovization formulation of SCFG dele- tion rules, which allows us to lexicalize probabili- ties of constituent deletions. We also use a robust approach for tree-to-tree alignment between arbi- trary document-abstract parallel corpora, which lets us train lexicalized models with much more data than previous approaches relying exclusively on scarcely available document-compression corpora. Finally, we evaluate different Markovized models, and find that our selected best model is one that ex- ploits head-modifier bilexicalization to accurately distinguish adjuncts from complements, and that produces sentences that were judged more gram- matical than those generated by previous work. :: We present a sentence compression system based on synchronous context-free grammars (SCFG), following the successful noisy-channel approach of (Knight and Marcu, 2000). We define a head- driven Markovization formulation of SCFG dele- tion rules, which allows us to lexicalize probabili- ties of constituent deletions. We also use a robust approach for tree-to-tree alignment between arbi- trary document-abstract parallel corpora, which lets us train lexicalized models with much more data than previous approaches relying exclusively on scarcely available document-compression corpora. Finally, we evaluate different Markovized models, and find that our selected best model is one that ex- ploits head-modifier bilexicalization to accurately distinguish adjuncts from complements, and that produces sentences that were judged more gram- matical than those generated by previous work.
649061 :: Word Order Acquisition from Corpora :: natural_language_and_speech_data.txt :: In this paper we describe a method of acquiring word order from corpora. Word order is defined as the order of modifiers, or the order of phrasal units called 'bunsetsu' which depend on the same modifiee. The method uses a model which automatically discovers what the tendency of the word order in Japanese is by using various kinds of information in and around the target bunsetsus. This model shows us to what extent each piece of information contributes to deciding the word order and which word order tends to be selected when several kinds of information conflict. The contribution rate of each piece of information in deciding word order is efficiently learned by a model within a maximum entropy framework. The performance of this trained model can be evaluated by checking how many instances of word order selected by the model agree with those in the original text. In this paper, we show that even a raw corpus that has not been tagged can be used to train the model, if it is first analyzed by a parser. This is possible because the word order of the text in the corpus is correct. :: In this paper we describe a method of acquiring word order from corpora. Word order is defined as the order of modifiers, or the order of phrasal units called 'bunsetsu' which depend on the same modifiee. The method uses a model which automatically discovers what the tendency of the word order in Japanese is by using various kinds of information in and around the target bunsetsus. This model shows us to what extent each piece of information contributes to deciding the word order and which word order tends to be selected when several kinds of information conflict. The contribution rate of each piece of information in deciding word order is efficiently learned by a model within a maximum entropy framework. The performance of this trained model can be evaluated by checking how many instances of word order selected by the model agree with those in the original text. In this paper, we show that even a raw corpus that has not been tagged can be used to train the model, if it is first analyzed by a parser. This is possible because the word order of the text in the corpus is correct.
2414975 :: Automatic Classification of Verbs in Biomedical Texts :: natural_language_and_speech_data.txt :: Lexical classes, when tailored to the appli- cation and domain in question, can provide an effective means to deal with a num- ber of natural language processing (NLP) tasks. While manual construction of such classes is difficult, recent research shows that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy. We report a novel experiment where similar technol- ogy is applied to the important, challeng- ing domain of biomedicine. We show that the resulting classification, acquired from a corpus of biomedical journal articles, is highly accurate and strongly domain- specific. It can be used to aid BIO-NLP directly or as useful material for investi- gating the syntax and semantics of verbs in biomedical texts. :: Lexical classes, when tailored to the appli- cation and domain in question, can provide an effective means to deal with a num- ber of natural language processing (NLP) tasks. While manual construction of such classes is difficult, recent research shows that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy. We report a novel experiment where similar technol- ogy is applied to the important, challeng- ing domain of biomedicine. We show that the resulting classification, acquired from a corpus of biomedical journal articles, is highly accurate and strongly domain- specific. It can be used to aid BIO-NLP directly or as useful material for investi- gating the syntax and semantics of verbs in biomedical texts.
4640573 :: Support Vector Machines Applied to the Classification of Semantic Relations in Nominalized Noun Phrases :: natural_language_and_speech_data.txt :: The discovery of semantic relations in text plays an important role in many NLP appli- cations. This paper presents a method for the automatic classification of semantic relations in nominalized noun phrases. Nominalizations represent a subclass of NP constructions in which either the head or the modifier noun is derived from a verb while the other noun is an argument of this verb. Especially designed fea- tures are extracted automatically and used in a Support Vector Machine learning model. The paper presents preliminary results for the se- mantic classification of the most representative NP patterns using four distinct learning mod- els. :: The discovery of semantic relations in text plays an important role in many NLP appli- cations. This paper presents a method for the automatic classification of semantic relations in nominalized noun phrases. Nominalizations represent a subclass of NP constructions in which either the head or the modifier noun is derived from a verb while the other noun is an argument of this verb. Especially designed fea- tures are extracted automatically and used in a Support Vector Machine learning model. The paper presents preliminary results for the se- mantic classification of the most representative NP patterns using four distinct learning mod- els.
443543 :: Finding Parts in Very Large Corpora :: natural_language_and_speech_data.txt :: We present a method for extracting parts of objects from wholes (e.g. "speedometer" from "car"). Given a very large corpus our method finds part words with 55% accuracy for the top 50 words as ranked by the system. The part list could be scanned by an end-user and added to an existing ontology (such as WordNet), or used as a part of a rough semantic lexicon. :: We present a method for extracting parts of objects from wholes (e.g. "speedometer" from "car"). Given a very large corpus our method finds part words with 55% accuracy for the top 50 words as ranked by the system. The part list could be scanned by an end-user and added to an existing ontology (such as WordNet), or used as a part of a rough semantic lexicon.
4639227 :: Models for the Semantic Classification of Noun Phrases :: natural_language_and_speech_data.txt :: This paper presents an approach for detecting semantic relations in noun phrases. A learning algorithm, called semantic scattering, is used to automatically label complex nominals, gen- itives and adjectival noun phrases with the cor- responding semantic relation. :: This paper presents an approach for detecting semantic relations in noun phrases. A learning algorithm, called semantic scattering, is used to automatically label complex nominals, gen- itives and adjectival noun phrases with the cor- responding semantic relation.
503349 :: Query by committee :: common_data.txt :: We propose an algorithm called query by commitee, in which a committee of students is trained on the same data set. The next query is chosen according to the principle of maximal disagreement. The algorithm is studied for two toy models: the high-low game and perceptron learning of another perceptron. As the number of queries goes to infinity, the committee algorithm yields asymptotically finite information gain. This leads to generalization error that decreases exponentially with the number of examples. This in marked contrast to learning from randomly chosen inputs, for which the information gain approaches zero and the generalization error decreases with a relatively slow inverse power law. We suggest that asymptotically finite information gain may be an important characteristic of good query algorithms. :: We propose an algorithm called query by commitee, in which a committee of students is trained on the same data set. The next query is chosen according to the principle of maximal disagreement. The algorithm is studied for two toy models: the high-low game and perceptron learning of another perceptron. As the number of queries goes to infinity, the committee algorithm yields asymptotically finite information gain. This leads to generalization error that decreases exponentially with the number of examples. This in marked contrast to learning from randomly chosen inputs, for which the information gain approaches zero and the generalization error decreases with a relatively slow inverse power law. We suggest that asymptotically finite information gain may be an important characteristic of good query algorithms.
285244 :: Story Segmentation and Detection of Commercials in Broadcast News Video :: databases__data.txt :: The Informedia Digital Library Project (Wactlar96) allows full content indexing and retrieval of text, audio and video material. Segmentation is an integral process in the Informedia digital video library. The success of the Informedia project hinges on two critical assumptions: that we can extract sufficiently accurate speech recognition transcripts from the broadcast audio and that we can segment the broadcast into video paragraphs, or stories, that are useful for information retrieval. In previous papers (Hauptmann97, Witbrock97, Witbrock98), we have shown that speech recognition is sufficient for information retrieval of pre-segmented video news stories. In this paper we address the issue of segmentation and demonstrate that a fully automatic system can extract story boundaries using available audio, video and closed-captioning cues. The story segmentation step for the Informedia Digital Video Library splits full-length news broadcasts into individual news stories. During this phase the system also labels commercials as separate "stories". We explain how the Informedia system takes advantage of the closed captioning frequently broadcast with the news, how it extracts timing information by aligning the closed-captions with the result of the speech recognition, and how the system integrates closed-caption cues with the results of image and audio processing. :: The Informedia Digital Library Project (Wactlar96) allows full content indexing and retrieval of text, audio and video material. Segmentation is an integral process in the Informedia digital video library. The success of the Informedia project hinges on two critical assumptions: that we can extract sufficiently accurate speech recognition transcripts from the broadcast audio and that we can segment the broadcast into video paragraphs, or stories, that are useful for information retrieval. In previous papers (Hauptmann97, Witbrock97, Witbrock98), we have shown that speech recognition is sufficient for information retrieval of pre-segmented video news stories. In this paper we address the issue of segmentation and demonstrate that a fully automatic system can extract story boundaries using available audio, video and closed-captioning cues. The story segmentation step for the Informedia Digital Video Library splits full-length news broadcasts into individual news stories. During this phase the system also labels commercials as separate "stories". We explain how the Informedia system takes advantage of the closed captioning frequently broadcast with the news, how it extracts timing information by aligning the closed-captions with the result of the speech recognition, and how the system integrates closed-caption cues with the results of image and audio processing.
5559010 :: Explorations in Sentence Fusion :: natural_language_and_speech_data.txt :: Sentence fusion is a text-to-text (revision-like) gen- eration task which takes related sentences as input and merges these into a single output sentence. In this paper we describe our ongoing work on de- veloping a sentence fusion module for Dutch. We propose a generalized version of alignment which not only indicates which words and phrases should be aligned but also labels these in terms of a small set of primitive semantic relations, indicating how words and phrases from the two input sentences re- late to each other. It is shown that human label- ers can perform this task with a high agreement (F- score of .95). We then describe and evaluate our adaptation of an existing automatic alignment al- gorithm, and use the resulting alignments, plus the semantic labels, in a generalized fusion and gen- eration algorithm. A small-scale evaluation study reveals that most of the resulting sentences are ad- equate to good. :: Sentence fusion is a text-to-text (revision-like) gen- eration task which takes related sentences as input and merges these into a single output sentence. In this paper we describe our ongoing work on de- veloping a sentence fusion module for Dutch. We propose a generalized version of alignment which not only indicates which words and phrases should be aligned but also labels these in terms of a small set of primitive semantic relations, indicating how words and phrases from the two input sentences re- late to each other. It is shown that human label- ers can perform this task with a high agreement (F- score of .95). We then describe and evaluate our adaptation of an existing automatic alignment al- gorithm, and use the resulting alignments, plus the semantic labels, in a generalized fusion and gen- eration algorithm. A small-scale evaluation study reveals that most of the resulting sentences are ad- equate to good.
2122945 :: The Alignment Template Approach to Statistical Machine Translation :: natural_language_and_speech_data.txt :: A phrase-based statistical machine translation approach — the alignment template approach — is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source-channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German-English speech Verbmobil task, we analyze the effect of various sys- tem components. On the French-English Canadian Hansards task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese-English 2002 National Institute of Standards and Technology (NIST) machine transla- tion evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems. :: A phrase-based statistical machine translation approach — the alignment template approach — is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source-channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German-English speech Verbmobil task, we analyze the effect of various sys- tem components. On the French-English Canadian Hansards task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese-English 2002 National Institute of Standards and Technology (NIST) machine transla- tion evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.
1809513 :: Large Scale Semantic Construction for Tree Adjoining Grammars :: natural_language_and_speech_data.txt :: Although Tree Adjoining Grammars (TAG) are widely used for syntactic processing, there is to date no large scale TAG available which also supports semantic construction. In this paper, we present a highly factorised way of implementing a syntax/semantic interface in TAG. We then show how the resulting resource can be used to perform semantic construction either during or after derivation. :: Although Tree Adjoining Grammars (TAG) are widely used for syntactic processing, there is to date no large scale TAG available which also supports semantic construction. In this paper, we present a highly factorised way of implementing a syntax/semantic interface in TAG. We then show how the resulting resource can be used to perform semantic construction either during or after derivation.
2145853 :: The Proposition Bank: An Annotated Corpus of Semantic Roles :: natural_language_and_speech_data.txt :: The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated. We discuss the criteria used to define the sets of semantic roles used in the annotation pro- cess, and analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus, and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation, and the contribution of the empty "trace" categories of the Treebank. :: The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank. The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated. We discuss the criteria used to define the sets of semantic roles used in the annotation pro- cess, and analyze the frequency of syntactic/semantic alternations in the corpus. We describe an automatic system for semantic role tagging trained on the corpus, and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation, and the contribution of the empty "trace" categories of the Treebank.
845552 :: The Mathematic of Statistical Machine Translation: Parameter Estimation :: natural_language_and_speech_data.txt :: We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus. :: We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus.
5658172 :: Classification of Semantic Relationships between Nominals Using Pattern Clusters :: natural_language_and_speech_data.txt :: There are many possible different semantic re- lationships between nominals. Classification of such relationships is an important and dif- ficult task (for example, the well known noun compound classification task is a special case of this problem). We propose a novel pat- tern clusters method for nominal relationship (NR) classification. Pattern clusters are dis- covered in a large corpus independently of any particular training set, in an unsupervised manner. Each of the extracted clusters cor- responds to some unspecified semantic rela- tionship. The pattern clusters are then used to construct features for training and classifi- cation of specific inter-nominal relationships. Our NR classification evaluation strictly fol- lows the ACL SemEval-07 Task 4 datasets and protocol, obtaining an f-score of 70.6, as op- posed to 64.8 of the best previous work that did not use the manually provided WordNet sense disambiguation tags. :: There are many possible different semantic re- lationships between nominals. Classification of such relationships is an important and dif- ficult task (for example, the well known noun compound classification task is a special case of this problem). We propose a novel pat- tern clusters method for nominal relationship (NR) classification. Pattern clusters are dis- covered in a large corpus independently of any particular training set, in an unsupervised manner. Each of the extracted clusters cor- responds to some unspecified semantic rela- tionship. The pattern clusters are then used to construct features for training and classifi- cation of specific inter-nominal relationships. Our NR classification evaluation strictly fol- lows the ACL SemEval-07 Task 4 datasets and protocol, obtaining an f-score of 70.6, as op- posed to 64.8 of the best previous work that did not use the manually provided WordNet sense disambiguation tags.
4138526 :: Conditional and joint models for grapheme-to-phoneme conversion :: natural_language_and_speech_data.txt :: In this work, we introduce several models for grapheme-to- phoneme conversion: a conditional maximum entropy model, a joint maximum entropy n-gram model, and a joint maximum entropy n-gram model with syllabification. We examine the relative merits of conditional and joint models for this task, and find that joint models have many advantages. We show that the performance of our best model, the joint n-gram model, com- pares favorably with the best results for English grapheme-to- phoneme conversion reported in the literature, sometimes by a wide margin. In the latter part of this paper, we consider the task of merging pronunciation lexicons expressed in different phone sets. We show that models for grapheme-to-phoneme conver- sion can be adapted effectively to this task. :: In this work, we introduce several models for grapheme-to- phoneme conversion: a conditional maximum entropy model, a joint maximum entropy n-gram model, and a joint maximum entropy n-gram model with syllabification. We examine the relative merits of conditional and joint models for this task, and find that joint models have many advantages. We show that the performance of our best model, the joint n-gram model, com- pares favorably with the best results for English grapheme-to- phoneme conversion reported in the literature, sometimes by a wide margin. In the latter part of this paper, we consider the task of merging pronunciation lexicons expressed in different phone sets. We show that models for grapheme-to-phoneme conver- sion can be adapted effectively to this task.
216871 :: Improved Statistical Alignment Models :: natural_language_and_speech_data.txt :: In this paper, we present and compare various single-word based alignment models for statistical machine translation. We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications. We present different methods to combine alignments. As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies. :: In this paper, we present and compare various single-word based alignment models for statistical machine translation. We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications. We present different methods to combine alignments. As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.
2370785 :: Extended Lexical-Semantic Classification of English Verbs :: natural_language_and_speech_data.txt :: Lexical-semantic verb classifications have proved useful in supporting various natural lan- guage processing (NLP) tasks. The largest and the most widely deployed classification in En- glish is Levin's (1993) taxonomy of verbs and their classes. While this resource is attrac- tive in being extensive enough for some NLP use, it is not comprehensive. In this paper, we present a substantial extension to Levin's tax- onomy which incorporates 57 novel classes for verbs not covered (comprehensively) by Levin. We also introduce 106 novel diathesis alterna- tions, created as a side product of constructing the new classes. We demonstrate the utility of our novel classes by using them to support au- tomatic subcategorization acquisition and show that the resulting extended classification has extensive coverage over the English verb lex- icon. :: Lexical-semantic verb classifications have proved useful in supporting various natural lan- guage processing (NLP) tasks. The largest and the most widely deployed classification in En- glish is Levin's (1993) taxonomy of verbs and their classes. While this resource is attrac- tive in being extensive enough for some NLP use, it is not comprehensive. In this paper, we present a substantial extension to Levin's tax- onomy which incorporates 57 novel classes for verbs not covered (comprehensively) by Levin. We also introduce 106 novel diathesis alterna- tions, created as a side product of constructing the new classes. We demonstrate the utility of our novel classes by using them to support au- tomatic subcategorization acquisition and show that the resulting extended classification has extensive coverage over the English verb lex- icon.
2165315 :: WikiRelate! Computing Semantic Relatedness Using Wikipedia :: common_data.txt :: Wikipedia provides a knowledge base for computing word relatedness in a more structured fashion than a search en- gine and with more coverage than WordNet. In this work we present experiments on using Wikipedia for computing semantic relatedness and compare it to WordNet on various benchmarking datasets. Existing relatedness measures per- form better using Wikipedia than a baseline given by Google counts, and we show that Wikipedia outperforms WordNet when applied to the largest available dataset designed for that purpose. The best results on this dataset are obtained by in- tegrating Google, WordNet and Wikipedia based measures. We also show that including Wikipedia improves the perfor- mance of an NLP application processing naturally occurring texts. :: Wikipedia provides a knowledge base for computing word relatedness in a more structured fashion than a search en- gine and with more coverage than WordNet. In this work we present experiments on using Wikipedia for computing semantic relatedness and compare it to WordNet on various benchmarking datasets. Existing relatedness measures per- form better using Wikipedia than a baseline given by Google counts, and we show that Wikipedia outperforms WordNet when applied to the largest available dataset designed for that purpose. The best results on this dataset are obtained by in- tegrating Google, WordNet and Wikipedia based measures. We also show that including Wikipedia improves the perfor- mance of an NLP application processing naturally occurring texts.
4789796 :: A general feature space for automatic verb classification :: natural_language_and_speech_data.txt :: Abstract Lexical semantic,classes of verbs play an important role in structuring complex,predicate information in a lexicon, thereby avoiding redundancy and enabling generalizations across semantically similar verbs with respect to their usage. Such classes, however, require many person-years of expert e! ort to create manually, and methods are needed for automatically assigning verbs to appropriate classes. In this work, we develop and evaluate a feature space to support the automatic assignment,of verbs into a well-known lexical semantic classification that is frequently used in natural language processing. The feature space is general ‐ applicable to any,class distinctions within the target classification; broad ‐ tapping,into a variety of semantic,features of the classes; and inexpensive ‐ requiring no more than a POS tagger and chunker. We perform experiments using support vector machines (SVMs) with the proposed feature space, demonstrating a reduction in error rate ranging from 48% to 88% over a chance baseline accuracy, across classification tasks of varying di"cult y. In particular, we attain performance,comparable,to or better than that of feature sets manually,selected for the particular tasks. Our results show that the approach is generally applicable, and reduces the need for resource-intensive linguistic analysis for each new,classification task. We also perform a wide range of experiments to determine the most informative features in the feature space, finding that simple, easily extractable features su"ce for good verb classification performance. :: Abstract Lexical semantic,classes of verbs play an important role in structuring complex,predicate information in a lexicon, thereby avoiding redundancy and enabling generalizations across semantically similar verbs with respect to their usage. Such classes, however, require many person-years of expert e! ort to create manually, and methods are needed for automatically assigning verbs to appropriate classes. In this work, we develop and evaluate a feature space to support the automatic assignment,of verbs into a well-known lexical semantic classification that is frequently used in natural language processing. The feature space is general ‐ applicable to any,class distinctions within the target classification; broad ‐ tapping,into a variety of semantic,features of the classes; and inexpensive ‐ requiring no more than a POS tagger and chunker. We perform experiments using support vector machines (SVMs) with the proposed feature space, demonstrating a reduction in error rate ranging from 48% to 88% over a chance baseline accuracy, across classification tasks of varying di"cult y. In particular, we attain performance,comparable,to or better than that of feature sets manually,selected for the particular tasks. Our results show that the approach is generally applicable, and reduces the need for resource-intensive linguistic analysis for each new,classification task. We also perform a wide range of experiments to determine the most informative features in the feature space, finding that simple, easily extractable features su"ce for good verb classification performance.
845360 :: A Systematic Comparison of Various Statistical Alignment Models :: natural_language_and_speech_data.txt :: We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented. :: We present and compare various methods for computing word alignments using statistical or heuristic models. We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements. These statistical models are compared with two heuristic models based on the Dice coefficient. We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models. As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We evaluate the models on the German-English Verbmobil task and the French-English Hansards task. We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes. An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models. In the Appendix, we present an efficient training algorithm for the alignment models presented.
2073437 :: INFORMEDIATM: NEWS-ON-DEMAND EXPERIMENTS IN SPEECH RECOGNITION :: common_data.txt :: In theory, speech recognition technology can make any spoken words in video or audio media usable for text indexing, search and retrieval. This article describes the News-on-Demand application created within the InformediaTM Digital Video Library project and discusses how speech recognition is used in transcript cre- ation from video, alignment with closed-captioned transcripts, audio paragraph segmentation and a spoken query interface. Speech recognition accuracy varies dramatically depending on the quality and type of data used. Informal information retrieval test show that reasonable recall and precision can be obtained with only moderate speech recognition accuracy. :: In theory, speech recognition technology can make any spoken words in video or audio media usable for text indexing, search and retrieval. This article describes the News-on-Demand application created within the InformediaTM Digital Video Library project and discusses how speech recognition is used in transcript cre- ation from video, alignment with closed-captioned transcripts, audio paragraph segmentation and a spoken query interface. Speech recognition accuracy varies dramatically depending on the quality and type of data used. Informal information retrieval test show that reasonable recall and precision can be obtained with only moderate speech recognition accuracy.
2508377 :: A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data :: common_data.txt :: Abstract One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don’t have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting. :: Abstract One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don’t have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.
1345310 :: Tree automata techniques and applications :: algorithms_and_theory_data.txt :: 636 :: 636
5185699 :: A Re-examination of Query Expansion Using Lexical Resources :: natural_language_and_speech_data.txt :: Query expansion is an effective technique to improve the performance of information re- trieval systems. Although hand-crafted lexi- cal resources, such as WordNet, could provide more reliable related terms, previous stud- ies showed that query expansion using only WordNet leads to very limited performance improvement. One of the main challenges is how to assign appropriate weights to expanded terms. In this paper, we re-examine this prob- lem using recently proposed axiomatic ap- proaches and find that, with appropriate term weighting strategy, we are able to exploit the information from lexical resources to sig- nificantly improve the retrieval performance. Our empirical results on six TREC collec- tions show that query expansion using only hand-crafted lexical resources leads to signif- icant performance improvement. The perfor- mance can be further improved if the proposed method is combined with query expansion us- ing co-occurrence-based resources. :: Query expansion is an effective technique to improve the performance of information re- trieval systems. Although hand-crafted lexi- cal resources, such as WordNet, could provide more reliable related terms, previous stud- ies showed that query expansion using only WordNet leads to very limited performance improvement. One of the main challenges is how to assign appropriate weights to expanded terms. In this paper, we re-examine this prob- lem using recently proposed axiomatic ap- proaches and find that, with appropriate term weighting strategy, we are able to exploit the information from lexical resources to sig- nificantly improve the retrieval performance. Our empirical results on six TREC collec- tions show that query expansion using only hand-crafted lexical resources leads to signif- icant performance improvement. The perfor- mance can be further improved if the proposed method is combined with query expansion us- ing co-occurrence-based resources.
1823 :: Employing {EM} in pool-based active learning for text classification :: common_data.txt :: The paper shows how a text classifier's need for labeled training documents can be reduced by taking advantage of a large pool of unlabeled documents. We modify the Query-by-Committee (QBC) method of active learning to use the unlabeled pool for explicitly estimating document density when selecting examples for labeling. Then active learning is combined with Expectation-Maximization in order to ``fill in'' the class labels of those documents that remain unlabeled. Experimental results show that the improvements to active learning require less than two-thirds as many labeled training examples as previous QBC approaches, and that the combination of EM and active learning requires only slightly more than half as many labeled training examples to achieve the same accuracy as either the improved active learning or EM alone. :: The paper shows how a text classifier's need for labeled training documents can be reduced by taking advantage of a large pool of unlabeled documents. We modify the Query-by-Committee (QBC) method of active learning to use the unlabeled pool for explicitly estimating document density when selecting examples for labeling. Then active learning is combined with Expectation-Maximization in order to ``fill in'' the class labels of those documents that remain unlabeled. Experimental results show that the improvements to active learning require less than two-thirds as many labeled training examples as previous QBC approaches, and that the combination of EM and active learning requires only slightly more than half as many labeled training examples to achieve the same accuracy as either the improved active learning or EM alone.
1968466 :: Reining in CCG Chart Realization :: natural_language_and_speech_data.txt :: We present a novel ensemble of six methods for improv- ing the eciency of chart realization. The methods are couched in the framework of Combinatory Categorial Grammar (CCG), but we con- jecture that they can be adapted to related grammatical frameworks as well. The ensemble includes two new methods introduced here— feature-based licensing and instantiation of edges, and caching of cate- gory combinations—in addition to four previously introduced methods— index filtering, LF chunking, edge pruning based on n-gram scores, and anytime search. We compare the relative contributions of each method using two test grammars, and show that the methods work best in combi- nation. Our evaluation also indicates that despite the exponential worst- case complexity of the basic algorithm, the methods together can con- strain the realization problem suciently to meet the interactive needs of natural language dialogue systems. :: We present a novel ensemble of six methods for improv- ing the eciency of chart realization. The methods are couched in the framework of Combinatory Categorial Grammar (CCG), but we con- jecture that they can be adapted to related grammatical frameworks as well. The ensemble includes two new methods introduced here— feature-based licensing and instantiation of edges, and caching of cate- gory combinations—in addition to four previously introduced methods— index filtering, LF chunking, edge pruning based on n-gram scores, and anytime search. We compare the relative contributions of each method using two test grammars, and show that the methods work best in combi- nation. Our evaluation also indicates that despite the exponential worst- case complexity of the basic algorithm, the methods together can con- strain the realization problem suciently to meet the interactive needs of natural language dialogue systems.
5529044 :: Cohesive Phrase-Based Decoding for Statistical Machine Translation :: natural_language_and_speech_data.txt :: Phrase-based decoding produces state-of-the- art translations with no regard for syntax. We add syntax to this process with a cohesion constraint based on a dependency tree for the source sentence. The constraint allows the decoder to employ arbitrary, non-syntactic phrases, but ensures that those phrases are translated in an order that respects the source tree's structure. In this way, we target the phrasal decoder's weakness in order model- ing, without affecting its strengths. To fur- ther increase flexibility, we incorporate cohe- sion as a decoder feature, creating a soft con- straint. The resulting cohesive, phrase-based decoder is shown to produce translations that are preferred over non-cohesive output in both automatic and human evaluations. :: Phrase-based decoding produces state-of-the- art translations with no regard for syntax. We add syntax to this process with a cohesion constraint based on a dependency tree for the source sentence. The constraint allows the decoder to employ arbitrary, non-syntactic phrases, but ensures that those phrases are translated in an order that respects the source tree's structure. In this way, we target the phrasal decoder's weakness in order model- ing, without affecting its strengths. To fur- ther increase flexibility, we incorporate cohe- sion as a decoder feature, creating a soft con- straint. The resulting cohesive, phrase-based decoder is shown to produce translations that are preferred over non-cohesive output in both automatic and human evaluations.
2290211 :: Parsing Arguments of Nominalizations in English and Chinese :: natural_language_and_speech_data.txt :: In this paper, we use a machine learning frame- work for semantic argument parsing, and apply it to the task of parsing arguments of eventive nominalizations in the FrameNet database. We create a baseline system using a subset of fea- tures introduced by Gildea and Jurafsky (2002), which are directly applicable to nominal pred- icates. We then investigate new features which are designed to capture the novelties in nom- inal argument structure and show a significant performance improvement using these new fea- tures. We also investigate the parsing perfor- mance of nominalizations in Chinese and com- pare the salience of the features for the two lan- guages. :: In this paper, we use a machine learning frame- work for semantic argument parsing, and apply it to the task of parsing arguments of eventive nominalizations in the FrameNet database. We create a baseline system using a subset of fea- tures introduced by Gildea and Jurafsky (2002), which are directly applicable to nominal pred- icates. We then investigate new features which are designed to capture the novelties in nom- inal argument structure and show a significant performance improvement using these new fea- tures. We also investigate the parsing perfor- mance of nominalizations in Chinese and com- pare the salience of the features for the two lan- guages.
1967060 :: Automatic Verb Classication Using a General Feature Space :: natural_language_and_speech_data.txt :: We develop a general feature space that can be used for the semantic classication of English verbs. We design a technique to extract these features from a large corpus of English, while trying to maintain portability to other languages|the only language- specic tools we use to extract our core features are a part-of-speech tagger and a partial parser. We show that our general feature space reduces the chance error rate by 40% or more in ten experiments involving from two to thirteen verb classes. We also show that it usually performs as well as features that are selected using specic linguistic expertise, and that it is therefore unnecessary to manually do linguistic analysis for each class distinction of interest. Finally, we consider the use of an automatic feature selection technique, stepwise feature selection, and show that it does not work well with our feature space. :: We develop a general feature space that can be used for the semantic classication of English verbs. We design a technique to extract these features from a large corpus of English, while trying to maintain portability to other languages|the only language- specic tools we use to extract our core features are a part-of-speech tagger and a partial parser. We show that our general feature space reduces the chance error rate by 40% or more in ten experiments involving from two to thirteen verb classes. We also show that it usually performs as well as features that are selected using specic linguistic expertise, and that it is therefore unnecessary to manually do linguistic analysis for each class distinction of interest. Finally, we consider the use of an automatic feature selection technique, stepwise feature selection, and show that it does not work well with our feature space.
1270402 :: Probabilistic part-of-speech tagging using decision trees :: natural_language_and_speech_data.txt :: 683 :: 683
2409686 :: The NomBank Project: An Interim Report :: natural_language_and_speech_data.txt :: This paper describes NomBank, a project that will provide argument structure for instances of common nouns in the Penn Treebank II corpus. NomBank is part of a larger effort to add ad- ditional layers of annotation to the Penn Tree- bank II corpus. The University of Pennsylva- nia's PropBank, NomBank and other annota- tion projects taken together should lead to the creation of better tools for the automatic analy- sis of text. This paper describes the NomBank project in detail including its specications and the process involved in creating the resource. :: This paper describes NomBank, a project that will provide argument structure for instances of common nouns in the Penn Treebank II corpus. NomBank is part of a larger effort to add ad- ditional layers of annotation to the Penn Tree- bank II corpus. The University of Pennsylva- nia's PropBank, NomBank and other annota- tion projects taken together should lead to the creation of better tools for the automatic analy- sis of text. This paper describes the NomBank project in detail including its specications and the process involved in creating the resource.
4231096 :: Fully Unsupervised Discovery of Concept-Specific Relationships by Web Mining :: natural_language_and_speech_data.txt :: We present a web mining method for discov- ering and enhancing relationships in which a specified concept (word class) participates. We discover a whole range of relationships focused on the given concept, rather than generic known relationships as in most pre- vious work. Our method is based on cluster- ing patterns that contain concept words and other words related to them. We evaluate the method on three different rich concepts and find that in each case the method generates a broad variety of relationships with good pre- cision. :: We present a web mining method for discov- ering and enhancing relationships in which a specified concept (word class) participates. We discover a whole range of relationships focused on the given concept, rather than generic known relationships as in most pre- vious work. Our method is based on cluster- ing patterns that contain concept words and other words related to them. We evaluate the method on three different rich concepts and find that in each case the method generates a broad variety of relationships with good pre- cision.
368811 :: A Unified Approach To Shot Change Detection And Camera Motion Characterization :: multimedia_data.txt :: : This paper describes an original approach which jointly addresses two fundamentalissues of video partitioning which represent the early important stage of any contentbasedvideo indexing system. These two issues are the detection of shot changes, and thelabeling of the shot configuration related to the camera movement in terms of static shot,panning, traveling, zooming,... They are both derived from the computation, at each timeinstant, of the dominant motion in the image represented by a ... :: : This paper describes an original approach which jointly addresses two fundamentalissues of video partitioning which represent the early important stage of any contentbasedvideo indexing system. These two issues are the detection of shot changes, and thelabeling of the shot configuration related to the camera movement in terms of static shot,panning, traveling, zooming,... They are both derived from the computation, at each timeinstant, of the dominant motion in the image represented by a ...
1766593 :: Finding Predominant Word Senses in Untagged Text :: natural_language_and_speech_data.txt :: Abstract In word sense disambiguation (WSD), the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of hand - tagged data Whilst there are a few hand - tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similar - ity package to find predominant noun senses auto - matically The acquired predominant senses give a precision of 64% on the nouns of the SENSEVAL - 2 English all - words task This is a very promising result given that our method does not require any hand - tagged text, such as SemCor Furthermore, we demonstrate that our method discovers appropri - ate predominant senses for words from two domain - specific corpora :: Abstract In word sense disambiguation (WSD), the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of hand - tagged data Whilst there are a few hand - tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similar - ity package to find predominant noun senses auto - matically The acquired predominant senses give a precision of 64% on the nouns of the SENSEVAL - 2 English all - words task This is a very promising result given that our method does not require any hand - tagged text, such as SemCor Furthermore, we demonstrate that our method discovers appropri - ate predominant senses for words from two domain - specific corpora
39303757 :: Resegmentation of SWITCHBOARD :: natural_language_and_speech_data.txt :: 24 :: 24
2446635 :: Open Information Extraction from the Web :: common_data.txt :: Traditionally, Information Extraction (IE) has fo- cused on satisfying precise, narrow, pre-specified requests from small homogeneous corpora (e.g., extract the location and time of seminars from a set of announcements). Shifting to a new domain requires the user to name the target relations and to manually create new extraction rules or hand-tag new training examples. This manual labor scales linearly with the number of target relations. This paper introduces Open IE (OIE), a new ex- traction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input. The paper also introduces T EXTRUNNER, a fully implemented, highly scalable OIE system where the tuples are assigned a probability and indexed to support efficient extraction and explo- ration via user queries. We report on experiments over a 9,000,000 Web page corpus that compare TEXTRUNNER with KNOWITALL, a state-of-the-art Web IE system. TEXTRUNNER achieves an error reduction of 33% on a comparable set of extractions. Furthermore, in the amount of time it takes KNOWITALL to per- form extraction for a handful of pre-specified re- lations, T EXTRUNNER extracts a far broader set of facts reflecting orders of magnitude more rela- tions, discovered on the fly. We report statistics on TEXTRUNNER's 11,000,000 highest probability tuples, and show that they contain over 1,000,000 concrete facts and over 6,500,000 more abstract as- sertions. :: Traditionally, Information Extraction (IE) has fo- cused on satisfying precise, narrow, pre-specified requests from small homogeneous corpora (e.g., extract the location and time of seminars from a set of announcements). Shifting to a new domain requires the user to name the target relations and to manually create new extraction rules or hand-tag new training examples. This manual labor scales linearly with the number of target relations. This paper introduces Open IE (OIE), a new ex- traction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input. The paper also introduces T EXTRUNNER, a fully implemented, highly scalable OIE system where the tuples are assigned a probability and indexed to support efficient extraction and explo- ration via user queries. We report on experiments over a 9,000,000 Web page corpus that compare TEXTRUNNER with KNOWITALL, a state-of-the-art Web IE system. TEXTRUNNER achieves an error reduction of 33% on a comparable set of extractions. Furthermore, in the amount of time it takes KNOWITALL to per- form extraction for a handful of pre-specified re- lations, T EXTRUNNER extracts a far broader set of facts reflecting orders of magnitude more rela- tions, discovered on the fly. We report statistics on TEXTRUNNER's 11,000,000 highest probability tuples, and show that they contain over 1,000,000 concrete facts and over 6,500,000 more abstract as- sertions.
4930784 :: Sentence Simplification for Semantic Role Labeling :: natural_language_and_speech_data.txt :: Parse-tree paths are commonly used to incor- porate information from syntactic parses into NLP systems. These systems typically treat the pathsas atomic(or nearly atomic)features; these features are quite sparse due to the im- mense variety of syntactic expression. In this paper, we propose a general method for learn- ing how to iteratively simplify a sentence, thus decomposing complicated syntax into small, easy-to-process pieces. Our method applies a series of hand-written transformation rules corresponding to basic syntactic patterns — for example, one rule "depassivizes" a sen- tence. The model is parameterized by learned weights specifying preferences for some rules over others. After applying all possible trans- formations to a sentence, we are left with a set of candidate simplified sentences. We ap- ply our simplification system to semantic role labeling (SRL). As we do not have labeled ex- amples of correct simplifications, we use la- beled training data for the SRL task to jointly learn both the weights of the simplification model and of an SRL model, treating the sim- plification as a hidden variable. By extracting and labeling simplified sentences, this com- bined simplification/SRL system better gener- alizes across syntactic variation. It achieves a statistically significant 1.2% F1 measure in- crease over a strong baseline on the Conll- 2005 SRL task, attaining near-state-of-the-art performance. :: Parse-tree paths are commonly used to incor- porate information from syntactic parses into NLP systems. These systems typically treat the pathsas atomic(or nearly atomic)features; these features are quite sparse due to the im- mense variety of syntactic expression. In this paper, we propose a general method for learn- ing how to iteratively simplify a sentence, thus decomposing complicated syntax into small, easy-to-process pieces. Our method applies a series of hand-written transformation rules corresponding to basic syntactic patterns — for example, one rule "depassivizes" a sen- tence. The model is parameterized by learned weights specifying preferences for some rules over others. After applying all possible trans- formations to a sentence, we are left with a set of candidate simplified sentences. We ap- ply our simplification system to semantic role labeling (SRL). As we do not have labeled ex- amples of correct simplifications, we use la- beled training data for the SRL task to jointly learn both the weights of the simplification model and of an SRL model, treating the sim- plification as a hidden variable. By extracting and labeling simplified sentences, this com- bined simplification/SRL system better gener- alizes across syntactic variation. It achieves a statistically significant 1.2% F1 measure in- crease over a strong baseline on the Conll- 2005 SRL task, attaining near-state-of-the-art performance.
309114 :: Automatic Acquisition of Hyponyms from Large Text Corpora :: natural_language_and_speech_data.txt :: We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested. :: We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidance of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also be acquirable in this way. A subset of the acquisition algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested.
4416974 :: The Pyramid Method: Incorporating human content selection variation in summarization evaluation :: natural_language_and_speech_data.txt :: Human variation in content selection in summarization has given rise to some fundamental research questions: How can one incorporate the observed variation in suitable evaluation measures? How can such measures reect the fact that summaries conveying dieren t content can be equally good and informative? In this paper we address these very questions by proposing a method for analysis of multiple human abstracts into semantic content units. Such analysis allows us not only to quantify human variation in content selection, but also to assign empirical importance weight to dieren t content units. It serves as the basis for an evaluation method, the Pyramid Method, that incorporates the observed variation and is predictive of dieren t equally informative summaries. We discuss the reliability of content unit annotation, the properties of Pyramid scores, and their correlation with other evaluation methods. :: Human variation in content selection in summarization has given rise to some fundamental research questions: How can one incorporate the observed variation in suitable evaluation measures? How can such measures reect the fact that summaries conveying dieren t content can be equally good and informative? In this paper we address these very questions by proposing a method for analysis of multiple human abstracts into semantic content units. Such analysis allows us not only to quantify human variation in content selection, but also to assign empirical importance weight to dieren t content units. It serves as the basis for an evaluation method, the Pyramid Method, that incorporates the observed variation and is predictive of dieren t equally informative summaries. We discuss the reliability of content unit annotation, the properties of Pyramid scores, and their correlation with other evaluation methods.
4231151 :: Multilingual Transliteration Using Feature based Phonetic Method :: natural_language_and_speech_data.txt :: In this paper we investigate named entity transliteration based on a phonetic scoring method. The phonetic method is computed using phonetic features and carefully designed pseudo features. The proposed method is tested with four languages - Arabic, Chinese, Hindi and Korean - and one source language - English, using comparable corpora. The proposed method is developed from the phonetic method originally proposed in Tao et al. (2006). In contrast to the phonetic method in Tao et al. (2006) constructed on the basis of pure linguistic knowledge, the method in this study is trained using the Winnow machine learning algorithm. There is salient improvement in Hindi and Arabic compared to the previous study. Moreover, we demonstrate that the method can also achieve comparable results, when it is trained on language data different from the target language. The method can be applied both with minimal data, and without target language data for various languages. :: In this paper we investigate named entity transliteration based on a phonetic scoring method. The phonetic method is computed using phonetic features and carefully designed pseudo features. The proposed method is tested with four languages - Arabic, Chinese, Hindi and Korean - and one source language - English, using comparable corpora. The proposed method is developed from the phonetic method originally proposed in Tao et al. (2006). In contrast to the phonetic method in Tao et al. (2006) constructed on the basis of pure linguistic knowledge, the method in this study is trained using the Winnow machine learning algorithm. There is salient improvement in Hindi and Arabic compared to the previous study. Moreover, we demonstrate that the method can also achieve comparable results, when it is trained on language data different from the target language. The method can be applied both with minimal data, and without target language data for various languages.
4144153 :: Unsupervised and active learning in automatic speech recognition for call classification :: natural_language_and_speech_data.txt :: A key challenge in rapidly building spoken natural language dialog applications is minimizing the manual effort required in transcribing and labeling speech data. This task is not only expensive but also time consuming. We present a novel approach that aims at reducing the amount of manually transcribed in-domain data required for building automatic speech recognition (ASR) models in spoken language dialog systems. Our method is based on mining relevant text from various conversational systems and Web sites. An iterative process is employed where the performance of the models can be improved through both unsupervised and active learning of the ASR models. We have evaluated the robustness of our approach on a call classification task that has been selected from AT&T VoiceToneSM customer care. Our results indicate that with unsupervised learning it is possible to achieve a call classification performance that is only 1.5% lower than the upper bound set when using all available in-domain transcribed data. :: A key challenge in rapidly building spoken natural language dialog applications is minimizing the manual effort required in transcribing and labeling speech data. This task is not only expensive but also time consuming. We present a novel approach that aims at reducing the amount of manually transcribed in-domain data required for building automatic speech recognition (ASR) models in spoken language dialog systems. Our method is based on mining relevant text from various conversational systems and Web sites. An iterative process is employed where the performance of the models can be improved through both unsupervised and active learning of the ASR models. We have evaluated the robustness of our approach on a call classification task that has been selected from AT&T VoiceToneSM customer care. Our results indicate that with unsupervised learning it is possible to achieve a call classification performance that is only 1.5% lower than the upper bound set when using all available in-domain transcribed data.
2159636 :: Maximum entropy model-based baseball highlight detection and classification :: computer_vision_data.txt :: In this paper, we propose a novel system that is able to automatically detect and classify baseball highlights by seamlessly integrating image, audio, and speech clues using a unique framework based on maximum entropy model (MEM). What distinguishes our system is that we emphasize on the integration of multimedia features and the acquisition of domain knowledge through automatic machine learning processes. Our MEM-based framework provides a simple platform capable of integrating multimedia features as well as their contextual information in a uniform fashion. Unlike the Hidden Markov Model and the Bayes Network-based approaches, this framework does not need to explicitly segment and classify the input video into states during its data modeling process, and hence remarkably simplifies the training data creation and the highlight detection/classification tasks. Experimental evaluations demonstrate the superiority of our proposed baseball highlight detection system in its capability of detecting more major baseball highlights, and in its overall accuracy of detecting these major highlights. :: In this paper, we propose a novel system that is able to automatically detect and classify baseball highlights by seamlessly integrating image, audio, and speech clues using a unique framework based on maximum entropy model (MEM). What distinguishes our system is that we emphasize on the integration of multimedia features and the acquisition of domain knowledge through automatic machine learning processes. Our MEM-based framework provides a simple platform capable of integrating multimedia features as well as their contextual information in a uniform fashion. Unlike the Hidden Markov Model and the Bayes Network-based approaches, this framework does not need to explicitly segment and classify the input video into states during its data modeling process, and hence remarkably simplifies the training data creation and the highlight detection/classification tasks. Experimental evaluations demonstrate the superiority of our proposed baseball highlight detection system in its capability of detecting more major baseball highlights, and in its overall accuracy of detecting these major highlights.
4231052 :: Transductive learning for statistical machine translation :: natural_language_and_speech_data.txt :: Statistical machine translation systems are usually trained on large amounts of bilin- gual text and monolingual text in the tar- get language. In this paper we explore the use of transductive semi-supervised meth- ods for the effective use of monolingual data from the source language in order to im- prove translation quality. We propose sev- eral algorithms with this aim, and present the strengths and weaknesses of each one. We present detailed experimental evaluations on the French-English EuroParl data set and on data from the NIST Chinese-English large- data track. We show a significant improve- ment in translation quality on both tasks. :: Statistical machine translation systems are usually trained on large amounts of bilin- gual text and monolingual text in the tar- get language. In this paper we explore the use of transductive semi-supervised meth- ods for the effective use of monolingual data from the source language in order to im- prove translation quality. We propose sev- eral algorithms with this aim, and present the strengths and weaknesses of each one. We present detailed experimental evaluations on the French-English EuroParl data set and on data from the NIST Chinese-English large- data track. We show a significant improve- ment in translation quality on both tasks.
5452627 :: Summarizing Emails with Conversational Cohesion and Subjectivity :: natural_language_and_speech_data.txt :: In this paper, we study the problem of sum- marizing email conversations. We first build a sentence quotation graph that captures the conversation structure among emails. We adopt three cohesion measures: clue words, semantic similarity and cosine similarity as the weight of the edges. Second, we use two graph-based summarization approaches, Generalized ClueWordSummarizer and Page- Rank, to extract sentences as summaries. Third, we propose a summarization approach based on subjective opinions and integrate it with the graph-based ones. The empirical evaluation shows that the basic clue words have the highest accuracy among the three co- hesion measures. Moreover, subjective words can significantly improve accuracy. :: In this paper, we study the problem of sum- marizing email conversations. We first build a sentence quotation graph that captures the conversation structure among emails. We adopt three cohesion measures: clue words, semantic similarity and cosine similarity as the weight of the edges. Second, we use two graph-based summarization approaches, Generalized ClueWordSummarizer and Page- Rank, to extract sentences as summaries. Third, we propose a summarization approach based on subjective opinions and integrate it with the graph-based ones. The empirical evaluation shows that the basic clue words have the highest accuracy among the three co- hesion measures. Moreover, subjective words can significantly improve accuracy.
4231148 :: A Grammar-driven Convolution Tree Kernel for Semantic Role Classification :: natural_language_and_speech_data.txt :: Convolution tree kernel has shown promis- ing results in semantic role classification. However, it only carries out hard matching, which may lead to over-fitting and less ac- curate similarity measure. To remove the constraint, this paper proposes a grammar- driven convolution tree kernel for semantic role classification by introducing more lin- guistic knowledge into the standard tree kernel. The proposed grammar-driven tree kernel displays two advantages over the pre- vious one: 1) grammar-driven approximate substructure matching and 2) grammar- driven approximate tree node matching. The two improvements enable the grammar- driven tree kernel explore more linguistically motivated structure features than the previ- ous one. Experiments on the CoNLL-2005 SRL shared task show that the grammar- driven tree kernel significantly outperforms the previous non-grammar-driven one in SRL. Moreover, we present a composite kernel to integrate feature-based and tree kernel-based methods. Experimental results show that the composite kernel outperforms the previously best-reported methods. :: Convolution tree kernel has shown promis- ing results in semantic role classification. However, it only carries out hard matching, which may lead to over-fitting and less ac- curate similarity measure. To remove the constraint, this paper proposes a grammar- driven convolution tree kernel for semantic role classification by introducing more lin- guistic knowledge into the standard tree kernel. The proposed grammar-driven tree kernel displays two advantages over the pre- vious one: 1) grammar-driven approximate substructure matching and 2) grammar- driven approximate tree node matching. The two improvements enable the grammar- driven tree kernel explore more linguistically motivated structure features than the previ- ous one. Experiments on the CoNLL-2005 SRL shared task show that the grammar- driven tree kernel significantly outperforms the previous non-grammar-driven one in SRL. Moreover, we present a composite kernel to integrate feature-based and tree kernel-based methods. Experimental results show that the composite kernel outperforms the previously best-reported methods.
4231145 :: Exploiting Syntactic and Shallow Semantic Kernels for Question Answer Classification :: natural_language_and_speech_data.txt :: We study the impact of syntactic and shallow semantic information in automatic classifi- cation of questions and answers and answer re-ranking. We define (a) new tree struc- tures based on shallow semantics encoded in Predicate Argument Structures (PASs) and (b) new kernel functions to exploit the representational power of such structures with Support Vector Machines. Our ex- periments suggest that syntactic information helps tasks such as question/answer classifi- cation and that shallow semantics gives re- markable contribution when a reliable set of PASs can be extracted, e.g. from answers. :: We study the impact of syntactic and shallow semantic information in automatic classifi- cation of questions and answers and answer re-ranking. We define (a) new tree struc- tures based on shallow semantics encoded in Predicate Argument Structures (PASs) and (b) new kernel functions to exploit the representational power of such structures with Support Vector Machines. Our ex- periments suggest that syntactic information helps tasks such as question/answer classifi- cation and that shallow semantics gives re- markable contribution when a reliable set of PASs can be extracted, e.g. from answers.
165757 :: Transformation of Logic Programs :: common_data.txt :: Machine [ Warren, 1983 ] , and to perform further efficiency improvingtransformations [ Demoen, 1993; Neumerkel, 1993 ] .5.3.3 Annotations and MemoingIn the previous sections we have mainly considered transformations whichdo not make use of the extra-logical features of logic languages, like cuts,asserts, delay declarations, etc. In the literature, however, there are variouspapers which deal with transformation rules which preserve the operationalsemantics of full Prolog (see Section... :: Machine [ Warren, 1983 ] , and to perform further efficiency improvingtransformations [ Demoen, 1993; Neumerkel, 1993 ] .5.3.3 Annotations and MemoingIn the previous sections we have mainly considered transformations whichdo not make use of the extra-logical features of logic languages, like cuts,asserts, delay declarations, etc. In the literature, however, there are variouspapers which deal with transformation rules which preserve the operationalsemantics of full Prolog (see Section...
2145504 :: Contrastive Estimation: Training Log-Linear Models on Unlabeled Data :: natural_language_and_speech_data.txt :: Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shal- low parsing (Sha and Pereira, 2003) and named- entity extraction (McCallum and Li, 2003). CRFs are log-linear, allowing the incorporation of arbi- trary features into the model. To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist. We describe a novel approach, contrastive estimation. We show that the new technique can be intuitively understood as ex- ploiting implicit negative evidence and is computa- tionally efficient. Applied to a sequence labeling problem—POS tagging given a tagging dictionary and unlabeled text—contrastive estimation outper- forms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely re- cover by modeling additional features. :: Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shal- low parsing (Sha and Pereira, 2003) and named- entity extraction (McCallum and Li, 2003). CRFs are log-linear, allowing the incorporation of arbi- trary features into the model. To train on unlabeled data, we require unsupervised estimation methods for log-linear models; few exist. We describe a novel approach, contrastive estimation. We show that the new technique can be intuitively understood as ex- ploiting implicit negative evidence and is computa- tionally efficient. Applied to a sequence labeling problem—POS tagging given a tagging dictionary and unlabeled text—contrastive estimation outper- forms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely re- cover by modeling additional features.
22851 :: The Berkeley FrameNet Project :: natural_language_and_speech_data.txt :: FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, "Tools for Lexicon Building"). The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics. The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between "frame elements" and their syntactic realizations (e.g. grammatical function, phrase type, and other syntactic traits). This report will present the project's goals and workflow, and information about the computational tools that have been adapted or created in-house for this work. :: FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, "Tools for Lexicon Building"). The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics. The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between "frame elements" and their syntactic realizations (e.g. grammatical function, phrase type, and other syntactic traits). This report will present the project's goals and workflow, and information about the computational tools that have been adapted or created in-house for this work.
346030 :: A Maximum Entropy Approach to Identifying Sentence Boundaries :: common_data.txt :: We present a trainable model for identify- ing sentence boundaries in raw text. Given a corpus annotated with sentence bound- aries, our model learns to classify each oc- currence of ., ?, and ! as either a valid or in- valid sentence boundary. The training pro- cedure requires no hand-crafted rules, lex- ica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Roman- alphabet language. Performance is compa- rable to or better than the performance of similar systems, but we emphasize the sim- plicity of retraining for new domains. :: We present a trainable model for identify- ing sentence boundaries in raw text. Given a corpus annotated with sentence bound- aries, our model learns to classify each oc- currence of ., ?, and ! as either a valid or in- valid sentence boundary. The training pro- cedure requires no hand-crafted rules, lex- ica, part-of-speech tags, or domain-specific information. The model can therefore be trained easily on any genre of English, and should be trainable on any other Roman- alphabet language. Performance is compa- rable to or better than the performance of similar systems, but we emphasize the sim- plicity of retraining for new domains.
3331459 :: Linguistically Informed Statistical Models of Constituent Structure for Ordering in Sentence Realization :: natural_language_and_speech_data.txt :: We present several statistical models of syntactic constituent order for sentence realization. We compare several models, including simple joint models inspired by existing statistical parsing models, and several novel conditional models. The conditional models leverage a large set of linguistic features without manual feature selection. We apply and evaluate the models in sentence realization for French and German and find that a particular conditional model outperforms all others. We employ a version of that model in an evaluation on unordered trees from the Penn TreeBank. We offer this result on standard data as a reference-point for evaluations of ordering in sentence realization. :: We present several statistical models of syntactic constituent order for sentence realization. We compare several models, including simple joint models inspired by existing statistical parsing models, and several novel conditional models. The conditional models leverage a large set of linguistic features without manual feature selection. We apply and evaluate the models in sentence realization for French and German and find that a particular conditional model outperforms all others. We employ a version of that model in an evaluation on unordered trees from the Penn TreeBank. We offer this result on standard data as a reference-point for evaluations of ordering in sentence realization.
49119 :: A Graph Model for Unsupervised Lexical Acquisition :: natural_language_and_speech_data.txt :: This paper presents an unsupervised method for assembling semantic knowledge from a part-of-speech tagged corpus using graph algorithms. The graph model is built by linking pairs of words which participate in particular syntactic relationships. We focus on the symmetric relationship between pairs of nouns which occur together in lists. An incremental cluster-building algorithm using this part of the graph achieves 82% accuracy at a lexical acquisition task, evaluated against WordNet classes. The model naturally realises domain and corpus specific ambiguities as distinct components in the graph surrounding an ambiguous word. :: This paper presents an unsupervised method for assembling semantic knowledge from a part-of-speech tagged corpus using graph algorithms. The graph model is built by linking pairs of words which participate in particular syntactic relationships. We focus on the symmetric relationship between pairs of nouns which occur together in lists. An incremental cluster-building algorithm using this part of the graph achieves 82% accuracy at a lexical acquisition task, evaluated against WordNet classes. The model naturally realises domain and corpus specific ambiguities as distinct components in the graph surrounding an ambiguous word.
2374026 :: Joint Learning Improves Semantic Role Labeling :: natural_language_and_speech_data.txt :: Despite much recent progress on accu- rate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label se- quence models via Viterbi decoding. This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependen- cies between arguments. We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative log- linear models. This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a state- of-the art independent classifier for gold- standard parse trees on PropBank. :: Despite much recent progress on accu- rate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label se- quence models via Viterbi decoding. This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependen- cies between arguments. We show how to build a joint model of argument frames, incorporating novel features that model these interactions into discriminative log- linear models. This system achieves an error reduction of 22% on all arguments and 32% on core arguments over a state- of-the art independent classifier for gold- standard parse trees on PropBank.
385869 :: Building a large annotated corpus of English: the penn treebank :: natural_language_and_speech_data.txt :: this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989-1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1 :: this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989-1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1
4231071 :: Automated Vocabulary Acquisition and Interpretation in Multimodal Conversational Systems :: natural_language_and_speech_data.txt :: Motivated by psycholinguistic findings that eye gaze is tightly linked to human lan- guage production, we developed an unsuper- vised approach based on translation models to automatically learn the mappings between words and objects on a graphic display dur- ing human machine conversation. The ex- perimental results indicate that user eye gaze can provide useful information to establish such mappings, which have important impli- cations in automatically acquiring and inter- preting user vocabularies for conversational systems. :: Motivated by psycholinguistic findings that eye gaze is tightly linked to human lan- guage production, we developed an unsuper- vised approach based on translation models to automatically learn the mappings between words and objects on a graphic display dur- ing human machine conversation. The ex- perimental results indicate that user eye gaze can provide useful information to establish such mappings, which have important impli- cations in automatically acquiring and inter- preting user vocabularies for conversational systems.
4139591 :: Forest Rescoring: Faster Decoding with Integrated Language Models :: natural_language_and_speech_data.txt :: Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality. We develop faster approaches for this problem based on k-best parsing algo- rithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conven- tional beam-search method at the same lev- els of search error and translation accuracy. :: Efficient decoding has been a fundamental problem in machine translation, especially with an integrated language model which is essential for achieving good translation quality. We develop faster approaches for this problem based on k-best parsing algo- rithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conven- tional beam-search method at the same lev- els of search error and translation accuracy.
4231077 :: Word Sense Disambiguation Improves Statistical Machine Translation :: natural_language_and_speech_data.txt :: Recent research presents conflicting evi- dence on whether word sense disambigua- tion (WSD) systems can help to improve the performance of statistical machine transla- tion (MT) systems. In this paper, we suc- cessfully integrate a state-of-the-art WSD system into a state-of-the-art hierarchical phrase-based MT system, Hiero. We show for the first time that integrating a WSD sys- tem improves the performance of a state-of- the-art statistical MT system on an actual translation task. Furthermore, the improve- ment is statistically significant. :: Recent research presents conflicting evi- dence on whether word sense disambigua- tion (WSD) systems can help to improve the performance of statistical machine transla- tion (MT) systems. In this paper, we suc- cessfully integrate a state-of-the-art WSD system into a state-of-the-art hierarchical phrase-based MT system, Hiero. We show for the first time that integrating a WSD sys- tem improves the performance of a state-of- the-art statistical MT system on an actual translation task. Furthermore, the improve- ment is statistically significant.
5679 :: Discriminative Training and Maximum Entropy Models for Statistical Machine Translation :: natural_language_and_speech_data.txt :: We present a framework for statistical machine translation of natural languages based on direct maximum entropy mod- els, which contains the widely used sour- ce-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine transla- tion system is significantly improved us- ing this approach. :: We present a framework for statistical machine translation of natural languages based on direct maximum entropy mod- els, which contains the widely used sour- ce-channel approach as a special case. All knowledge sources are treated as feature functions, which depend on the source language sentence, the target language sentence and possible hidden variables. This approach allows a baseline machine translation system to be extended easily by adding new feature functions. We show that a baseline statistical machine transla- tion system is significantly improved us- ing this approach.
5442340 :: Unsupervised Discovery of Generic Relationships Using Pattern Clusters and its Evaluation by Automatically Generated SAT Analogy Questions :: natural_language_and_speech_data.txt :: We present a novel framework for the dis- covery and representation of general semantic relationships that hold between lexical items. We propose that each such relationship can be identified with a cluster of patterns that cap- tures this relationship. We give a fully unsu- pervised algorithm for pattern cluster discov- ery, which searches, clusters and merges high- frequency words-based patterns around ran- domly selected hook words. Pattern clusters can be used to extract instances of the corre- sponding relationships. To assess the quality of discovered relationships, we use the pattern clusters to automatically generate SAT anal- ogy questions. We also compare to a set of known relationships, achieving very good re- sults in both methods. The evaluation (done in both English and Russian) substantiates the premise that our pattern clusters indeed reflect relationships perceived by humans. :: We present a novel framework for the dis- covery and representation of general semantic relationships that hold between lexical items. We propose that each such relationship can be identified with a cluster of patterns that cap- tures this relationship. We give a fully unsu- pervised algorithm for pattern cluster discov- ery, which searches, clusters and merges high- frequency words-based patterns around ran- domly selected hook words. Pattern clusters can be used to extract instances of the corre- sponding relationships. To assess the quality of discovered relationships, we use the pattern clusters to automatically generate SAT anal- ogy questions. We also compare to a set of known relationships, achieving very good re- sults in both methods. The evaluation (done in both English and Russian) substantiates the premise that our pattern clusters indeed reflect relationships perceived by humans.
10114551 :: Unsupervised segmentation of words into morphemes - Challenge 2005 An Introduction and Evaluation Report :: natural_language_and_speech_data.txt :: The objective of the challenge for the unsupervised segmentation of words into morphemes, or shorter the Morpho Chal- lenge, was to design a statistical machine learning algorithm that segments words into the smallest meaning-bearing units of language, morphemes. Ideally, these are basic vocabulary units suitable for differ- ent tasks, such as speech and text un- derstanding, machine translation, infor- mation retrieval, and statistical language modeling. The segmentations were eval- uated in two complementary ways: Com- petition 1: The proposed morpheme seg- mentation were compared to a linguis- tic morpheme segmentation gold standard. Competition 2: Speech recognition ex- periments were performed, where statis- tical n-gram language models utilized the proposed word segments instead of entire words. Data sets were provided for three languages: Finnish, English, and Turk- ish. Participants were encouraged to ap- ply their algorithm to all of these test lan- guages. :: The objective of the challenge for the unsupervised segmentation of words into morphemes, or shorter the Morpho Chal- lenge, was to design a statistical machine learning algorithm that segments words into the smallest meaning-bearing units of language, morphemes. Ideally, these are basic vocabulary units suitable for differ- ent tasks, such as speech and text un- derstanding, machine translation, infor- mation retrieval, and statistical language modeling. The segmentations were eval- uated in two complementary ways: Com- petition 1: The proposed morpheme seg- mentation were compared to a linguis- tic morpheme segmentation gold standard. Competition 2: Speech recognition ex- periments were performed, where statis- tical n-gram language models utilized the proposed word segments instead of entire words. Data sets were provided for three languages: Finnish, English, and Turk- ish. Participants were encouraged to ap- ply their algorithm to all of these test lan- guages.
368362 :: Noun-Phrase Co-Occurence Statistics for Semi-Automatic Semantic Lexicon Construction :: natural_language_and_speech_data.txt :: Generating semantic lexicons semi-automatically could be a great time saver, relative to creating them by hand. In this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars. Our algorithm finds more correct terms and fewer incorrect ones than previous work in this area. Additionally, the entries that are generated potentially provide broader coverage of the category than would occur to an individual coding them by hand. Our algorithm finds many terms not included within Wordnet (many more than previous algorithms), and could be viewed as an "enhancer" of existing broad-coverage resources. :: Generating semantic lexicons semi-automatically could be a great time saver, relative to creating them by hand. In this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars. Our algorithm finds more correct terms and fewer incorrect ones than previous work in this area. Additionally, the entries that are generated potentially provide broader coverage of the category than would occur to an individual coding them by hand. Our algorithm finds many terms not included within Wordnet (many more than previous algorithms), and could be viewed as an "enhancer" of existing broad-coverage resources.
2384515 :: Towards Terascale Knowledge Acquisition :: natural_language_and_speech_data.txt :: Although vast amounts of textual data are freely available, many NLP algorithms exploit only a minute percentage of it. In this paper, we study the challenges of working at the terascale. We present an algorithm, designed for the terascale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method. We fo- cus on the accuracy of these two systems as a func- tion of processing time and corpus size. :: Although vast amounts of textual data are freely available, many NLP algorithms exploit only a minute percentage of it. In this paper, we study the challenges of working at the terascale. We present an algorithm, designed for the terascale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method. We fo- cus on the accuracy of these two systems as a func- tion of processing time and corpus size.
2441172 :: Topic modeling: beyond bag-of-words :: common_data.txt :: Some models of textual corpora employ text generation methods involving n-gram statis- tics, while others use latent topic variables inferred using the \bag-of-words" assump- tion, in which word order is ignored. Pre- viously, these methods have not been com- bined. In this work, I explore a hierarchi- cal generative probabilistic model that in- corporates both n-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchi- cal Dirichlet bigram language model. The model hyperparameters are inferred using a Gibbs EM algorithm. On two data sets, each of 150 documents, the new model exhibits better predictive accuracy than either a hi- erarchical Dirichlet bigram language model or a unigram topic model. Additionally, the inferred topics are less dominated by func- tion words than are topics discovered using unigram statistics, potentially making them more meaningful. :: Some models of textual corpora employ text generation methods involving n-gram statis- tics, while others use latent topic variables inferred using the \bag-of-words" assump- tion, in which word order is ignored. Pre- viously, these methods have not been com- bined. In this work, I explore a hierarchi- cal generative probabilistic model that in- corporates both n-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchi- cal Dirichlet bigram language model. The model hyperparameters are inferred using a Gibbs EM algorithm. On two data sets, each of 150 documents, the new model exhibits better predictive accuracy than either a hi- erarchical Dirichlet bigram language model or a unigram topic model. Additionally, the inferred topics are less dominated by func- tion words than are topics discovered using unigram statistics, potentially making them more meaningful.
5616851 :: Which Are the Best Features for Automatic Verb Classification :: natural_language_and_speech_data.txt :: In this work, we develop and evaluate a wide range of feature spaces for deriving Levin- style verb classifications (Levin, 1993). We perform the classification experiments using Bayesian Multinomial Regression (an effi- cient log-linear modeling framework which we found to outperform SVMs for this task) with the proposed feature spaces. Our exper- iments suggest that subcategorization frames are not the most effective features for auto- matic verb classification. A mixture of syntac- tic information and lexical information works best for this task. :: In this work, we develop and evaluate a wide range of feature spaces for deriving Levin- style verb classifications (Levin, 1993). We perform the classification experiments using Bayesian Multinomial Regression (an effi- cient log-linear modeling framework which we found to outperform SVMs for this task) with the proposed feature spaces. Our exper- iments suggest that subcategorization frames are not the most effective features for auto- matic verb classification. A mixture of syntac- tic information and lexical information works best for this task.
31 :: Multitask Learning :: machine_learning_and_pattern_recognition_data.txt :: . Multitask Learning is an approach to inductive transfer that improves generalization by using thedomain information contained in the training signals of related tasks as an inductive bias. It does this by learningtasks in parallel while using a shared representation; what is learned for each task can help other tasks be learnedbetter. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers taskrelatedness without the need of supervisory signals, and ... :: . Multitask Learning is an approach to inductive transfer that improves generalization by using thedomain information contained in the training signals of related tasks as an inductive bias. It does this by learningtasks in parallel while using a shared representation; what is learned for each task can help other tasks be learnedbetter. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers taskrelatedness without the need of supervisory signals, and ...
4231174 :: Learning Predictive Structures for Semantic Role Labeling of NomBank :: natural_language_and_speech_data.txt :: This paper presents a novel application of Alternating Structure Optimization (ASO) to the task of Semantic Role Labeling (SRL) of noun predicates in NomBank. ASO is a recently proposed linear multi-task learn- ing algorithm, which extracts the common structures of multiple tasks to improve accu- racy, via the use of auxiliary problems. In this paper, we explore a number of different auxiliary problems, and we are able to sig- nificantly improve the accuracy of the Nom- Bank SRL task using this approach. To our knowledge, our proposed approach achieves the highest accuracy published to date on the English NomBank SRL task. :: This paper presents a novel application of Alternating Structure Optimization (ASO) to the task of Semantic Role Labeling (SRL) of noun predicates in NomBank. ASO is a recently proposed linear multi-task learn- ing algorithm, which extracts the common structures of multiple tasks to improve accu- racy, via the use of auxiliary problems. In this paper, we explore a number of different auxiliary problems, and we are able to sig- nificantly improve the accuracy of the Nom- Bank SRL task using this approach. To our knowledge, our proposed approach achieves the highest accuracy published to date on the English NomBank SRL task.
4231196 :: Phonological Constraints and Morphological Preprocessing for Grapheme-to-Phoneme Conversion :: natural_language_and_speech_data.txt :: Grapheme-to-phoneme conversion (g2p) is a core component of any text-to-speech sys- tem. We show that adding simple syllab- ification and stress assignment constraints, namely 'one nucleus per syllable' and 'one main stress per word', to a joint n-gram model for g2p conversion leads to a dramatic improvement in conversion accuracy. Secondly, we assessed morphological pre- processing for g2p conversion. While mor- phological information has been incorpo- rated in some past systems, its contribution has never been quantitatively assessed for German. We compare the relevance of mor- phological preprocessing with respect to the morphological segmentation method, train- ing set size, the g2p conversion algorithm, and two languages, English and German. :: Grapheme-to-phoneme conversion (g2p) is a core component of any text-to-speech sys- tem. We show that adding simple syllab- ification and stress assignment constraints, namely 'one nucleus per syllable' and 'one main stress per word', to a joint n-gram model for g2p conversion leads to a dramatic improvement in conversion accuracy. Secondly, we assessed morphological pre- processing for g2p conversion. While mor- phological information has been incorpo- rated in some past systems, its contribution has never been quantitatively assessed for German. We compare the relevance of mor- phological preprocessing with respect to the morphological segmentation method, train- ing set size, the g2p conversion algorithm, and two languages, English and German.
4683321 :: Generating and selecting grammatical paraphrases :: natural_language_and_speech_data.txt :: Natural language has a high paraphrastic power yet not all paraphrases are appropriate for all contexts. In this paper, we present a TAG based surface re- aliser which supports both the generation and the selection of paraphrases. To deal with the combi- natorial explosion typical of such an NP-complete task, we introduce a number of new optimisations in a tabular, bottom-up surface realisation algo- rithm. We then show that one of these optimisations supports paraphrase selection. :: Natural language has a high paraphrastic power yet not all paraphrases are appropriate for all contexts. In this paper, we present a TAG based surface re- aliser which supports both the generation and the selection of paraphrases. To deal with the combi- natorial explosion typical of such an NP-complete task, we introduce a number of new optimisations in a tabular, bottom-up surface realisation algo- rithm. We then show that one of these optimisations supports paraphrase selection.
6294835 :: Automatic Image Annotation Using Auxiliary Text Information :: natural_language_and_speech_data.txt :: The availability of databases of images labeled with keywords is necessary for developing and evaluating image annotation models. Dataset collection is however a costly and time con- suming task. In this paper we exploit the vast resource of images available on the web. We create a database of pictures that are natu- rally embedded into news articles and propose to use their captions as a proxy for annota- tion keywords. Experimental results show that an image annotation model can be developed on this dataset alone without the overhead of manual annotation. We also demonstrate that the news article associated with the picture can be used to boost image annotation perfor- mance. :: The availability of databases of images labeled with keywords is necessary for developing and evaluating image annotation models. Dataset collection is however a costly and time con- suming task. In this paper we exploit the vast resource of images available on the web. We create a database of pictures that are natu- rally embedded into news articles and propose to use their captions as a proxy for annota- tion keywords. Experimental results show that an image annotation model can be developed on this dataset alone without the overhead of manual annotation. We also demonstrate that the news article associated with the picture can be used to boost image annotation perfor- mance.
10987639 :: A generation-oriented workbench for Performance Grammar: Capturing linear order variability in German and Dutch :: natural_language_and_speech_data.txt :: We describe a generation-oriented workbench for the Performance Grammar (PG) formalism, highlighting the treatment of certain word order and movement constraints in Dutch and German. PG enables a simple and uni- form treatment of a heterogeneous col- lection of linear order phenomena in the domain of verb constructions (variably known as Cross-serial De- pendencies, Verb Raising, Clause Un- ion, Extraposition, Third Construction, Particle Hopping, etc.). The central data structures enabling this feature are clausal "topologies": one-dimensional arrays associated with clauses, whose cells ("slots") provide landing sites for the constituents of the clause. Move- ment operations are enabled by unifica- tion of lateral slots of topologies at ad- jacent levels of the clause hierarchy. The PGW generator assists the gram- mar developer in testing whether the implemented syntactic knowledge al- lows all and only the well-formed per- mutations of constituents. :: We describe a generation-oriented workbench for the Performance Grammar (PG) formalism, highlighting the treatment of certain word order and movement constraints in Dutch and German. PG enables a simple and uni- form treatment of a heterogeneous col- lection of linear order phenomena in the domain of verb constructions (variably known as Cross-serial De- pendencies, Verb Raising, Clause Un- ion, Extraposition, Third Construction, Particle Hopping, etc.). The central data structures enabling this feature are clausal "topologies": one-dimensional arrays associated with clauses, whose cells ("slots") provide landing sites for the constituents of the clause. Move- ment operations are enabled by unifica- tion of lateral slots of topologies at ad- jacent levels of the clause hierarchy. The PGW generator assists the gram- mar developer in testing whether the implemented syntactic knowledge al- lows all and only the well-formed per- mutations of constituents.
5501233 :: Applying Alternating Structure Optimization to Word Sense Disambiguation :: natural_language_and_speech_data.txt :: This paper presents a new application of the recently proposed machine learning method Alternating Structure Optimiza- tion (ASO), to word sense disambiguation (WSD). Given a set of WSD problems and their respective labeled examples, we seek to improve overall performance on that set by using all the labeled exam- ples (irrespective of target words) for the entire set in learning a disambiguator for each individual problem. Thus, in effect, on each individual problem (e.g., disam- biguation of "art") we benefit from train- ing examples for other problems (e.g., disambiguation of "bar", "canal", and so forth). We empirically study the effective use of ASO for this purpose in the multi- task and semi-supervised learning config- urations. Our performance results rival or exceed those of the previous best sys- tems on several Senseval lexical sample task data sets. :: This paper presents a new application of the recently proposed machine learning method Alternating Structure Optimiza- tion (ASO), to word sense disambiguation (WSD). Given a set of WSD problems and their respective labeled examples, we seek to improve overall performance on that set by using all the labeled exam- ples (irrespective of target words) for the entire set in learning a disambiguator for each individual problem. Thus, in effect, on each individual problem (e.g., disam- biguation of "art") we benefit from train- ing examples for other problems (e.g., disambiguation of "bar", "canal", and so forth). We empirically study the effective use of ASO for this purpose in the multi- task and semi-supervised learning config- urations. Our performance results rival or exceed those of the previous best sys- tems on several Senseval lexical sample task data sets.
2165038 :: Learning Noun-Modifier Semantic Relations with Corpus-based and WordNet-based Features :: artificial_intelligence_data.txt :: We study the performance of two representations of word meaning in learning noun-modifier semantic relations. One representation is based on lexical resources, in particular WordNet, the other - on a corpus. We experimented with de- cision trees, instance-based learning and Support Vector Ma- chines. All these methods work well in this learning task. We report high precision, recall and F-score, and small variation in performance across several 10-fold cross-validation runs. The corpus-based method has the advantage of working with data without word-sense annotations and performs well over the baseline. The WordNet-based method, requiring word- sense annotated data, has higher precision. :: We study the performance of two representations of word meaning in learning noun-modifier semantic relations. One representation is based on lexical resources, in particular WordNet, the other - on a corpus. We experimented with de- cision trees, instance-based learning and Support Vector Ma- chines. All these methods work well in this learning task. We report high precision, recall and F-score, and small variation in performance across several 10-fold cross-validation runs. The corpus-based method has the advantage of working with data without word-sense annotations and performs well over the baseline. The WordNet-based method, requiring word- sense annotated data, has higher precision.
5742263 :: Applying a Grammar-Based Language Model to a Simplified Broadcast-News Transcription Task :: natural_language_and_speech_data.txt :: We propose a language model based on a precise, linguistically motivated grammar (a hand-crafted Head-driven Phrase Structure Grammar) and a statistical model estimating the probability of a parse tree. The language model is applied by means of an N-best rescor- ing step, which allows to directly measure the performance gains relative to the baseline sys- tem without rescoring. To demonstrate that our approach is feasible and beneficial for non-trivial broad-domain speech recognition tasks, we applied it to a simplified German broadcast-news transcription task. We report a significant reduction in word error rate com- pared to a state-of-the-art baseline system. :: We propose a language model based on a precise, linguistically motivated grammar (a hand-crafted Head-driven Phrase Structure Grammar) and a statistical model estimating the probability of a parse tree. The language model is applied by means of an N-best rescor- ing step, which allows to directly measure the performance gains relative to the baseline sys- tem without rescoring. To demonstrate that our approach is feasible and beneficial for non-trivial broad-domain speech recognition tasks, we applied it to a simplified German broadcast-news transcription task. We report a significant reduction in word error rate com- pared to a state-of-the-art baseline system.
2043904 :: Applied morphological processing of English :: natural_language_and_speech_data.txt :: 119 :: 119
2142135 :: Calibrating Features for Semantic Role Labeling :: natural_language_and_speech_data.txt :: This paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input, gener- ally a syntactic parse tree, has yet to be fully exploited. We propose an additional set of fea- tures and our experiments show that these fea- tures lead to fairly signicant improvements in the tasks we performed. We further show that dierent features are needed for dierent sub- tasks. Finally, we show that by using a Maxi- mum Entropy classier and fewer features, we achieved results comparable with the best previ- ously reported results obtained with SVM mod- els. We believe this is a clear indication that developing features that capture the right kind of information is crucial to advancing the state- of-the-art in semantic analysis. :: This paper takes a critical look at the features used in the semantic role tagging literature and show that the information in the input, gener- ally a syntactic parse tree, has yet to be fully exploited. We propose an additional set of fea- tures and our experiments show that these fea- tures lead to fairly signicant improvements in the tasks we performed. We further show that dierent features are needed for dierent sub- tasks. Finally, we show that by using a Maxi- mum Entropy classier and fewer features, we achieved results comparable with the best previ- ously reported results obtained with SVM mod- els. We believe this is a clear indication that developing features that capture the right kind of information is crucial to advancing the state- of-the-art in semantic analysis.
2383413 :: The Mathematics of Statistical Machine Translation: Parameter Estimation :: natural_language_and_speech_data.txt :: 782 :: 782
259468 :: Snowball: extracting relations from large plain-text collections :: common_data.txt :: Text documents often contain valuable structured data that is hidden in regular English sentences. This data is best ex- ploited if available as a relational table that we could use for answering precise queries or for running data mining tasks. We explore a technique for extracting such tables from doc- ument collections that requires only a handful of training ex- amples from users. These examples are used to generate extraction patterns, that in turn result in new tuples being extracted from the document collection. We build on this idea and present our Snowball system. Snowball introduces novel strategies for generating patterns and extracting tuples from plain-text documents. At each iteration of the extrac- tion process, Snowball evaluates the quality of these patterns and tuples without human intervention, and keeps only the most reliable ones for the next iteration. In this paper we also develop a scalable evaluation methodology and metrics for our task, and present a thorough experimental evaluation of Snowball and comparable techniques over a collection of more than 300,000 newspaper documents. :: Text documents often contain valuable structured data that is hidden in regular English sentences. This data is best ex- ploited if available as a relational table that we could use for answering precise queries or for running data mining tasks. We explore a technique for extracting such tables from doc- ument collections that requires only a handful of training ex- amples from users. These examples are used to generate extraction patterns, that in turn result in new tuples being extracted from the document collection. We build on this idea and present our Snowball system. Snowball introduces novel strategies for generating patterns and extracting tuples from plain-text documents. At each iteration of the extrac- tion process, Snowball evaluates the quality of these patterns and tuples without human intervention, and keeps only the most reliable ones for the next iteration. In this paper we also develop a scalable evaluation methodology and metrics for our task, and present a thorough experimental evaluation of Snowball and comparable techniques over a collection of more than 300,000 newspaper documents.
2922658 :: Data Mining: Practical Machine Learning Tools and Techniques :: data_mining_data.txt :: 3991 :: 3991
5153649 :: Vector-based Models of Semantic Composition :: natural_language_and_speech_data.txt :: This paper proposes a framework for repre- senting the meaning of phrases and sentences in vector space. Central to our approach is vector composition which we operationalize in terms of additive and multiplicative func- tions. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments. :: This paper proposes a framework for repre- senting the meaning of phrases and sentences in vector space. Central to our approach is vector composition which we operationalize in terms of additive and multiplicative func- tions. Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.
2464777 :: A Semantic Approach to Recognizing Textual Entailment :: natural_language_and_speech_data.txt :: Abstract Exhaustive extraction of semantic information from text is one of the formidable goals of state-of-the-art NLP systems. In this paper, we take a step closer to this objective. We combine,the semantic information provided by different resources and extract new semantic knowledge,to improve the performance of a recognizing textual entailment system. 1 Recognizing Textual Entailment :: Abstract Exhaustive extraction of semantic information from text is one of the formidable goals of state-of-the-art NLP systems. In this paper, we take a step closer to this objective. We combine,the semantic information provided by different resources and extract new semantic knowledge,to improve the performance of a recognizing textual entailment system. 1 Recognizing Textual Entailment
2385406 :: VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations :: natural_language_and_speech_data.txt :: Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks. We present a semi-automatic method for extracting fine-grained semantic relations between verbs. We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexico- syntactic patterns over the Web. On a set of 29,165 strongly associated verb pairs, our extraction algo- rithm yielded 65.5% accuracy. Analysis of error types shows that on the relation strength we achieved 75% accuracy. We provide the resource, called VERBOCEAN, for download at http://semantics.isi.edu/ocean/. :: Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks. We present a semi-automatic method for extracting fine-grained semantic relations between verbs. We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexico- syntactic patterns over the Web. On a set of 29,165 strongly associated verb pairs, our extraction algo- rithm yielded 65.5% accuracy. Analysis of error types shows that on the relation strength we achieved 75% accuracy. We provide the resource, called VERBOCEAN, for download at http://semantics.isi.edu/ocean/.
630783 :: Textual context analysis for information retrieval :: information_retrieval_data.txt :: 50 :: 50
5122856 :: Improving the Performance of the Random Walk Model for Answering Complex Questions :: natural_language_and_speech_data.txt :: We consider the problem of answering com- plex questions that require inferencing and synthesizing information from multiple doc- uments and can be seen as a kind of topic- oriented, informative multi-document summa- rization. The stochastic, graph-based method for computing the relative importance of tex- tual units (i.e. sentences) is very successful in generic summarization. In this method, a sentence is encoded as a vector in which each component represents the occurrence fre- quency (TF*IDF) of a word. However, the major limitation of the TF*IDF approach is that it only retains the frequency of the words and does not take into account the sequence, syntactic and semantic information. In this pa- per, we study the impact of syntactic and shal- low semantic information in the graph-based method for answering complex questions. :: We consider the problem of answering com- plex questions that require inferencing and synthesizing information from multiple doc- uments and can be seen as a kind of topic- oriented, informative multi-document summa- rization. The stochastic, graph-based method for computing the relative importance of tex- tual units (i.e. sentences) is very successful in generic summarization. In this method, a sentence is encoded as a vector in which each component represents the occurrence fre- quency (TF*IDF) of a word. However, the major limitation of the TF*IDF approach is that it only retains the frequency of the words and does not take into account the sequence, syntactic and semantic information. In this pa- per, we study the impact of syntactic and shal- low semantic information in the graph-based method for answering complex questions.
1834611 :: Feature Generation for Text Categorization Using World Knowledge :: common_data.txt :: We enhance machine learning algorithms for text categorization with generated features based on domain-specific and common-sense knowledge. This knowledge is represented using publicly available ontologies that contain hundreds of thousands of concepts, such as the Open Directory; these ontologies are further enriched by several orders of magnitude through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts, which in turn induce a set of generated features that augment the standard bag of words. Feature generation is accomplished through contextual analysis of document text, implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses the two main problems of natural language processing---synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the documents alone. Experimental results confirm improved performance, breaking through the plateau previously reached in the field. :: We enhance machine learning algorithms for text categorization with generated features based on domain-specific and common-sense knowledge. This knowledge is represented using publicly available ontologies that contain hundreds of thousands of concepts, such as the Open Directory; these ontologies are further enriched by several orders of magnitude through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts, which in turn induce a set of generated features that augment the standard bag of words. Feature generation is accomplished through contextual analysis of document text, implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses the two main problems of natural language processing---synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the documents alone. Experimental results confirm improved performance, breaking through the plateau previously reached in the field.
4231207 :: Sentence generation as a planning problem :: natural_language_and_speech_data.txt :: We translate sentence generation from TAG grammars with semantic and pragmatic in- formation into a planning problem by encod- ing the contribution of each word declara- tively and explicitly. This allows us to ex- ploit the performance of off-the-shelf plan- ners. It also opens up new perspectives on referring expression generation and the rela- tionship between language and action. :: We translate sentence generation from TAG grammars with semantic and pragmatic in- formation into a planning problem by encod- ing the contribution of each word declara- tively and explicitly. This allows us to ex- ploit the performance of off-the-shelf plan- ners. It also opens up new perspectives on referring expression generation and the rela- tionship between language and action.
845318 :: Building a Large Annotated Corpus of English: The Penn Treebank :: natural_language_and_speech_data.txt :: 1688 :: 1688
53284 :: Bleu: a Method for Automatic Evaluation of Machine Translation :: natural_language_and_speech_data.txt :: Human evaluations of machine translation are extensive but expensive. Human eval- uations can take months to finish and in- volve human labor that can not be reused. We propose a method of automatic ma- chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu- ation, and that has little marginal cost per run. We present this method as an auto- mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1 :: Human evaluations of machine translation are extensive but expensive. Human eval- uations can take months to finish and in- volve human labor that can not be reused. We propose a method of automatic ma- chine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evalu- ation, and that has little marginal cost per run. We present this method as an auto- mated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.1
631143 :: Query expansion using lexical-semantic relations :: information_retrieval_data.txt :: 460 :: 460
631142 :: Using WordNet to disambiguate word senses for text retrieval :: information_retrieval_data.txt :: This paper describes an automatic indexing procedure that uses the “IS-A” relations contained within WordNet and the set of nouns contained in a text to select a sense for each plysemous noun in the text. The result of the indexing procedure is a vector in which some of the terms represent word senses instead of word stems. Retrieval experiments comparing the effectivenss of these sense-based vectors vs. stem-based vectors show the stem-based vectors to be superior overall, although the sense-based vectors do improve the performance of some queries. The overall degradation is due in large part to the difficulty of disambiguating senses in short query statements. An analysis of these results suggests two conclusions: the IS-A links define a generalization/specialization hierarchy that is not sufficient to reliably select the correct sense of a noun from the set of fine sense distinctions in WordNet; and missing correct matches because of incorrect sense resolution has a much more deleterious effect on retrieval performance than does making spurious matches. :: This paper describes an automatic indexing procedure that uses the “IS-A” relations contained within WordNet and the set of nouns contained in a text to select a sense for each plysemous noun in the text. The result of the indexing procedure is a vector in which some of the terms represent word senses instead of word stems. Retrieval experiments comparing the effectivenss of these sense-based vectors vs. stem-based vectors show the stem-based vectors to be superior overall, although the sense-based vectors do improve the performance of some queries. The overall degradation is due in large part to the difficulty of disambiguating senses in short query statements. An analysis of these results suggests two conclusions: the IS-A links define a generalization/specialization hierarchy that is not sufficient to reliably select the correct sense of a noun from the set of fine sense distinctions in WordNet; and missing correct matches because of incorrect sense resolution has a much more deleterious effect on retrieval performance than does making spurious matches.
2047122 :: An Empirical Study of the Domain Dependence of Supervised Word Sense Disambiguation Systems :: natural_language_and_speech_data.txt :: This paper describes a set of experiments car- ried out to explore the domain dependence of alternative supervised Word Sense Disam- biguation algorithms. The aim of the work is threefold: studying the performance of these algorithms when tested on a dierent cor- pus from that they were trained on; explor- ing their ability to tune to new domains, and demonstrating empirically that the Lazy- Boosting algorithm outperforms state-of-the- art supervised WSD algorithms in both previ- ous situations. :: This paper describes a set of experiments car- ried out to explore the domain dependence of alternative supervised Word Sense Disam- biguation algorithms. The aim of the work is threefold: studying the performance of these algorithms when tested on a dierent cor- pus from that they were trained on; explor- ing their ability to tune to new domains, and demonstrating empirically that the Lazy- Boosting algorithm outperforms state-of-the- art supervised WSD algorithms in both previ- ous situations.
4231112 :: Guiding Statistical Word Alignment Models With Prior Knowledge :: natural_language_and_speech_data.txt :: We present a general framework to incor- porate prior knowledge such as heuristics or linguistic features in statistical generative word alignment models. Prior knowledge plays a role of probabilistic soft constraints between bilingual word pairs that shall be used to guide word alignment model train- ing. We investigate knowledge that can be derived automatically from entropy princi- ple and bilingual latent semantic analysis and show how they can be applied to im- prove translation performance. :: We present a general framework to incor- porate prior knowledge such as heuristics or linguistic features in statistical generative word alignment models. Prior knowledge plays a role of probabilistic soft constraints between bilingual word pairs that shall be used to guide word alignment model train- ing. We investigate knowledge that can be derived automatically from entropy princi- ple and bilingual latent semantic analysis and show how they can be applied to im- prove translation performance.
4231117 :: Generating Constituent Order in German Clauses :: natural_language_and_speech_data.txt :: We investigate the factors which determine constituent order in German clauses and pro- pose an algorithm which performs the task in two steps: First, the best candidate for the initial sentence position is chosen. Then, the order for the remaining constituents is determined. The rst task is more difcult than the second one because of properties of the German sentence-initial position. Ex- periments show a signicant improvement over competing approaches. Our algorithm is also more efcient than these. :: We investigate the factors which determine constituent order in German clauses and pro- pose an algorithm which performs the task in two steps: First, the best candidate for the initial sentence position is chosen. Then, the order for the remaining constituents is determined. The rst task is more difcult than the second one because of properties of the German sentence-initial position. Ex- periments show a signicant improvement over competing approaches. Our algorithm is also more efcient than these.
2371752 :: Generalized Inference with Multiple Semantic Role Labeling Systems :: natural_language_and_speech_data.txt :: We present an approach to semantic role labeling (SRL) that takes the output of multiple argument classifiers and com- bines them into a coherent predicate- argument output by solving an optimiza- tion problem. The optimization stage, which is solved via integer linear pro- gramming, takes into account both the rec- ommendation of the classifiers and a set of problem specific constraints, and is thus used both to clean the classification results and to ensure structural integrity of the fi- nal role labeling. We illustrate a signifi- cant improvement in overall SRL perfor- mance through this inference. :: We present an approach to semantic role labeling (SRL) that takes the output of multiple argument classifiers and com- bines them into a coherent predicate- argument output by solving an optimiza- tion problem. The optimization stage, which is solved via integer linear pro- gramming, takes into account both the rec- ommendation of the classifiers and a set of problem specific constraints, and is thus used both to clean the classification results and to ensure structural integrity of the fi- nal role labeling. We illustrate a signifi- cant improvement in overall SRL perfor- mance through this inference.
35930 :: Crosslinguistic Transfer in Automatic Verb Classification :: natural_language_and_speech_data.txt :: We investigate the use of multilingual data in the automatic classification of English verbs, and show that there is a useful transfer of information across languages. Specifically, we experiment with three lexical semantic classes of English verbs. We collect statistical features over a sample of English verbs from each of the classes, as well as over Chinese translations of those verbs. We use the English and Chinese data, alone and in combination, as training data for a machine learning algorithm whose output is an automatic verb classifier. We demonstrate that Chinese data is indeed useful in helping to classify the English verbs (at 82% accuracy), and furthermore that a multilingual combination of data outperforms the English data alone (85% accuracy). Moreover, our results using monolingual corpora show that it is not necessary to use a parallel corpus to extract the translations in order for this technique to be successful. :: We investigate the use of multilingual data in the automatic classification of English verbs, and show that there is a useful transfer of information across languages. Specifically, we experiment with three lexical semantic classes of English verbs. We collect statistical features over a sample of English verbs from each of the classes, as well as over Chinese translations of those verbs. We use the English and Chinese data, alone and in combination, as training data for a machine learning algorithm whose output is an automatic verb classifier. We demonstrate that Chinese data is indeed useful in helping to classify the English verbs (at 82% accuracy), and furthermore that a multilingual combination of data outperforms the English data alone (85% accuracy). Moreover, our results using monolingual corpora show that it is not necessary to use a parallel corpus to extract the translations in order for this technique to be successful.
695121 :: Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations :: data_mining_data.txt :: Witten and Frank's textbook was one of two books that I used for a data mining class in the Fall of 2001.The book covers all major methods of data mining that produce a knowledge representation as output. Knowledge representation is hereby understood as a representation that can be studied, understood, and interpreted by human beings, at least in principle. Thus, neural networks and genetic algorithms are excluded from the topics of this textbook. We need to say "can be understood in principle" because a large decision tree or a large rule set may be as hard to interpret as a neural network. The book first develops the basic machine learning and data mining methods. These include decision trees, classification and association rules, support vector machines, instance-based learning, Naive Bayes classifiers, clustering, and numeric prediction based on linear regression, regression trees, and model trees. It then goes deeper into evaluation and implementation issues. Next it moves on to deeper coverage of issues such as attribute selection, discretization, data cleansing, and combinations of multiple models (bagging, boosting, and stacking). The final chapter deals with advanced topics such as visual machine learning, text mining, and Web mining. :: Witten and Frank's textbook was one of two books that I used for a data mining class in the Fall of 2001.The book covers all major methods of data mining that produce a knowledge representation as output. Knowledge representation is hereby understood as a representation that can be studied, understood, and interpreted by human beings, at least in principle. Thus, neural networks and genetic algorithms are excluded from the topics of this textbook. We need to say "can be understood in principle" because a large decision tree or a large rule set may be as hard to interpret as a neural network. The book first develops the basic machine learning and data mining methods. These include decision trees, classification and association rules, support vector machines, instance-based learning, Naive Bayes classifiers, clustering, and numeric prediction based on linear regression, regression trees, and model trees. It then goes deeper into evaluation and implementation issues. Next it moves on to deeper coverage of issues such as attribute selection, discretization, data cleansing, and combinations of multiple models (bagging, boosting, and stacking). The final chapter deals with advanced topics such as visual machine learning, text mining, and Web mining.
1383708 :: Using a Semantic Concordance for Sense Identification :: natural_language_and_speech_data.txt :: This paper proposes benchmarks for systems of automatic sense identification. A textual corpus in which open-class words had been tagged both syntactically and semantically was used to explore three statistical strategies for sense identification: a guessing heuristic, a most-frequent heuristic, and a co-occurrence heuristic. When no information about sense-frequencies was available, the guessing heuristic using the numbers of alternative senses in WordNet was correct 45% of the time. When statistics for sense-frequencies were derived from a semantic concordance, the assumption that each word is used in its most frequently occurring sense was correct 69% of the time; when that figure was calculated for polysemous words alone, it dropped to 58%. And when a co-occurrence heuristic took advantage of prior occurrences of words together in the same sentences, little improvement was observed. The semantic concordance is still too small to estimate the potential limits of a co-occurrence heuristic. :: This paper proposes benchmarks for systems of automatic sense identification. A textual corpus in which open-class words had been tagged both syntactically and semantically was used to explore three statistical strategies for sense identification: a guessing heuristic, a most-frequent heuristic, and a co-occurrence heuristic. When no information about sense-frequencies was available, the guessing heuristic using the numbers of alternative senses in WordNet was correct 45% of the time. When statistics for sense-frequencies were derived from a semantic concordance, the assumption that each word is used in its most frequently occurring sense was correct 69% of the time; when that figure was calculated for polysemous words alone, it dropped to 58%. And when a co-occurrence heuristic took advantage of prior occurrences of words together in the same sentences, little improvement was observed. The semantic concordance is still too small to estimate the potential limits of a co-occurrence heuristic.
1766544 :: Discovering Relations among Named Entities from Large Corpora :: natural_language_and_speech_data.txt :: Discovering the significant relations embedded in documents would be very useful not only for infor- mation retrieval but also for question answering and summarization. Prior methods for relation discov- ery, however, needed large annotated corpora which cost a great deal of time and effort. We propose an unsupervised method for relation discovery from large corpora. The key idea is clustering pairs of named entities according to the similarity of con- text words intervening between the named entities. Our experiments using one year of newspapers re- veals not only that the relations among named enti- ties could be detected with high recall and precision, but also that appropriate labels could be automati- cally provided for the relations. :: Discovering the significant relations embedded in documents would be very useful not only for infor- mation retrieval but also for question answering and summarization. Prior methods for relation discov- ery, however, needed large annotated corpora which cost a great deal of time and effort. We propose an unsupervised method for relation discovery from large corpora. The key idea is clustering pairs of named entities according to the similarity of con- text words intervening between the named entities. Our experiments using one year of newspapers re- veals not only that the relations among named enti- ties could be detected with high recall and precision, but also that appropriate labels could be automati- cally provided for the relations.
4231057 :: Transforming Projective Bilexical Dependency Grammars into efficiently-parsable CFGs with Unfold-Fold :: natural_language_and_speech_data.txt :: This paper shows how to use the Unfold- Fold transformation to transform Projective Bilexical Dependency Grammars (PBDGs) into ambiguity-preserving weakly equiva- lent Context-Free Grammars (CFGs). These CFGs can be parsed in O(n3) time using a CKY algorithm with appropriate indexing, rather than the O(n5) time required by a naive encoding. Informally, using the CKY algorithm with such a CFG mimics the steps of the Eisner-Satta O(n3) PBDG parsing al- gorithm. This transformation makes all of the techniques developed for CFGs available to PBDGs. We demonstrate this by describ- ing a maximum posterior parse decoder for PBDGs. :: This paper shows how to use the Unfold- Fold transformation to transform Projective Bilexical Dependency Grammars (PBDGs) into ambiguity-preserving weakly equiva- lent Context-Free Grammars (CFGs). These CFGs can be parsed in O(n3) time using a CKY algorithm with appropriate indexing, rather than the O(n5) time required by a naive encoding. Informally, using the CKY algorithm with such a CFG mimics the steps of the Eisner-Satta O(n3) PBDG parsing al- gorithm. This transformation makes all of the techniques developed for CFGs available to PBDGs. We demonstrate this by describ- ing a maximum posterior parse decoder for PBDGs.
1704923 :: Multimodal Video Indexing: A Review of the State-of-the-art :: multimedia_data.txt :: Efficient and effective handling of video documents depends on the availability of indexes. Manual indexing is unfeasible for large video collections. In this paper we survey several methods aiming at automating this time and resource consuming process. Good reviews on single modality based video indexing have appeared in literature. Effective indexing, however, requires a multimodal approach in which either the most appropriate modality is selected or the different modalities are used in collaborative fashion. Therefore, instead of separately treating the different information sources involved, and their specific algorithms, we focus on the similarities and differences between the modalities. To that end we put forward a unifying and multimodal framework, which views a video document from the perspective of its author. This framework forms the guiding principle for identifying index types, for which automatic methods are found in literature. It furthermore forms the basis for categorizing these different methods. :: Efficient and effective handling of video documents depends on the availability of indexes. Manual indexing is unfeasible for large video collections. In this paper we survey several methods aiming at automating this time and resource consuming process. Good reviews on single modality based video indexing have appeared in literature. Effective indexing, however, requires a multimodal approach in which either the most appropriate modality is selected or the different modalities are used in collaborative fashion. Therefore, instead of separately treating the different information sources involved, and their specific algorithms, we focus on the similarities and differences between the modalities. To that end we put forward a unifying and multimodal framework, which views a video document from the perspective of its author. This framework forms the guiding principle for identifying index types, for which automatic methods are found in literature. It furthermore forms the basis for categorizing these different methods.
2414989 :: Probabilistic CFG with Latent Annotations :: natural_language_and_speech_data.txt :: This paper defines a generative probabilis- tic model of parse trees, which we call PCFG-LA. This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables. Fine- grained CFG rules are automatically in- duced from a parsed corpus by training a PCFG-LA model using an EM-algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are de- scribed and empirically compared. In ex- periments using the Penn WSJ corpus, our automatically trained model gave a per- formance of 86.6% (F , sentences 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection. :: This paper defines a generative probabilis- tic model of parse trees, which we call PCFG-LA. This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables. Fine- grained CFG rules are automatically in- duced from a parsed corpus by training a PCFG-LA model using an EM-algorithm. Because exact parsing with a PCFG-LA is NP-hard, several approximations are de- scribed and empirically compared. In ex- periments using the Penn WSJ corpus, our automatically trained model gave a per- formance of 86.6% (F , sentences 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.
102893 :: Automatic Labeling of Semantic Roles :: natural_language_and_speech_data.txt :: We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-specific semantic roles such as SPEAKER, MESSAGE, and TOPIC. The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible fillers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classifiers. Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con- stituents. At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall. Our study also allowed us to compare the usefulness of different features and feature- combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data. :: We present a system for identifying the semantic relationships, or semantic roles, filled by constituents of a sentence within a semantic frame. Given an input sentence and a target word and frame, the system labels constituents with either abstract semantic roles such as AGENT or PATIENT, or more domain-specific semantic roles such as SPEAKER, MESSAGE, and TOPIC. The system is based on statistical classifiers trained on roughly 50,000 sentences that were hand-annotated with semantic roles by the FrameNet semantic labeling project. We then parsed each training sentence into a syntactic tree and extracted various lexical and syntactic features, including the phrase type of each constituent, its grammatical function, and position in the sentence. These features were combined with knowledge of the predicate verb, noun, or adjective, as well as information such as the prior probabilities of various combinations of semantic roles. We used various lexical clustering algorithms to generalize across possible fillers of roles. Test sentences were parsed, were annotated with these features, and were then passed through the classifiers. Our system achieves 82% accuracy in identifying the semantic role of pre-segmented con- stituents. At the more difficult task of simultaneously segmenting constituents and identifying their semantic role, the system achieved 65% precision and 61% recall. Our study also allowed us to compare the usefulness of different features and feature- combination methods in the semantic role labeling task. We also explore the integration of role labeling with statistical syntactic parsing, and attempt to generalize to predicates unseen in the training data.
2414995 :: Tree-to-String Alignment Template for Statistical Machine Translation :: natural_language_and_speech_data.txt :: We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment be- tween a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and per- forming reordering at both low and high levels. The model is linguistically syntax- based because TATs are extracted auto- matically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to pro- duce a source parse tree and then ap- ply TATs to transform the tree into a tar- get string. Our experiments show that the TAT-based model significantly outper- forms Pharaoh, a state-of-the-art decoder for phrase-based models. :: We present a novel translation model based on tree-to-string alignment template (TAT) which describes the alignment be- tween a source parse tree and a target string. A TAT is capable of generating both terminals and non-terminals and per- forming reordering at both low and high levels. The model is linguistically syntax- based because TATs are extracted auto- matically from word-aligned, source side parsed parallel texts. To translate a source sentence, we first employ a parser to pro- duce a source parse tree and then ap- ply TATs to transform the tree into a tar- get string. Our experiments show that the TAT-based model significantly outper- forms Pharaoh, a state-of-the-art decoder for phrase-based models.
38081 :: A General Feature Space for Automatic Verb Classification :: natural_language_and_speech_data.txt :: We develop a general feature space for automatic classification of verbs into lexical semantic classes. Previous work was limited in scope by the need for manual selection of discriminating features, through a linguistic analysis of the target verb classes (Merlo and Stevenson, 2001). We instead analyze the classification structure at a higher level, using the possible defining characteristics of classes as the basis for our feature space. The general feature space achieves reductions in error rates of 42--69%, on a wider range of classes than investigated previously, with comparable performance to feature sets manually selected for the particular classification tasks. Our results show that the approach is generally applicable, and avoids the need for resource-intensive linguistic analysis for each new task. :: We develop a general feature space for automatic classification of verbs into lexical semantic classes. Previous work was limited in scope by the need for manual selection of discriminating features, through a linguistic analysis of the target verb classes (Merlo and Stevenson, 2001). We instead analyze the classification structure at a higher level, using the possible defining characteristics of classes as the basis for our feature space. The general feature space achieves reductions in error rates of 42--69%, on a wider range of classes than investigated previously, with comparable performance to feature sets manually selected for the particular classification tasks. Our results show that the approach is generally applicable, and avoids the need for resource-intensive linguistic analysis for each new task.
787230 :: Syntax-Directed Transduction :: common_data.txt :: A transduction is a mapping from one set of sequences to another. A syntax-directed transduction is a particular type of transduction which is defined on the grammar of a context-free language and which is meant to be a model of part of the translation process used in many compilers. The transduction is considered from an automata theory viewpoint as specifying the input-output relation of a machine. Special consideration is given to machines called translators which both transduce and recognize. In particular, some special conditions are investigated under which syntax-directed translations can be performed on (deterministic) pushdown machines. In addition, some time bounds for translations on Turing machines are derived. :: A transduction is a mapping from one set of sequences to another. A syntax-directed transduction is a particular type of transduction which is defined on the grammar of a context-free language and which is meant to be a model of part of the translation process used in many compilers. The transduction is considered from an automata theory viewpoint as specifying the input-output relation of a machine. Special consideration is given to machines called translators which both transduce and recognize. In particular, some special conditions are investigated under which syntax-directed translations can be performed on (deterministic) pushdown machines. In addition, some time bounds for translations on Turing machines are derived.
15813 :: Clustering Polysemic Subcategorization Frame Distributions Semantically :: natural_language_and_speech_data.txt :: Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data. We describe a new approach which involves clustering subcategorization frame (SCF) distributions using the Information Bottleneck and nearest neighbour methods. In contrast to previous work, we particularly focus on clustering polysemic verbs. A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data. :: Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data. We describe a new approach which involves clustering subcategorization frame (SCF) distributions using the Information Bottleneck and nearest neighbour methods. In contrast to previous work, we particularly focus on clustering polysemic verbs. A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data.
870581 :: From Grammar to Lexicon: Unsupervised Learning of Lexical Syntax :: natural_language_and_speech_data.txt :: Imagine a language that is completely unfamiliar; the only means of studying it are an ordinary grammar book and a very large corpus of text. No dictionary is available. How can easily recognized, surface grammatical facts be used to extract from a corpus as much syntactic information as possible about individual words? This paper describes an approach based on two principles. First, rely on local morpho-syntactic cues to structure rather than trying to parse entire sentences. Second, treat these cues as probabilistic rather than absolute indicators of syntactic structure. Apply inferential statistics to the data collected using the cues, rather than drawing a categorical conclusion from a single occurrence of a cue. The effectiveness of this approach for inferring the syntactic frames of verbs is supported by experiments on an English corpus using a program called Lerner. Lerner starts out with no knowledge of content words---it bootstraps from determiners, auxiliaries, modals, prepositions, pronouns, complementizers, coordinating conjunctions, and punctuation. :: Imagine a language that is completely unfamiliar; the only means of studying it are an ordinary grammar book and a very large corpus of text. No dictionary is available. How can easily recognized, surface grammatical facts be used to extract from a corpus as much syntactic information as possible about individual words? This paper describes an approach based on two principles. First, rely on local morpho-syntactic cues to structure rather than trying to parse entire sentences. Second, treat these cues as probabilistic rather than absolute indicators of syntactic structure. Apply inferential statistics to the data collected using the cues, rather than drawing a categorical conclusion from a single occurrence of a cue. The effectiveness of this approach for inferring the syntactic frames of verbs is supported by experiments on an English corpus using a program called Lerner. Lerner starts out with no knowledge of content words---it bootstraps from determiners, auxiliaries, modals, prepositions, pronouns, complementizers, coordinating conjunctions, and punctuation.
2363036 :: A High-Performance Semi-Supervised Learning Method for Text Chunking :: natural_language_and_speech_data.txt :: In machine learning, whether one can build a more accurate classifier by using unlabeled data (semi-supervised learning) is an important issue. Although a num- ber of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear. This paper presents a novel semi-supervised method that em- ploys a learning paradigm which we call structural learning. The idea is to find "what good classifiers are like" by learn- ing from thousands of automatically gen- erated auxiliary classification problems on unlabeled data. By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve perfor- mance on the target problem. The method produces performance higher than the pre- vious best results on CoNLL'00 syntac- tic chunking and CoNLL'03 named entity chunking (English and German). :: In machine learning, whether one can build a more accurate classifier by using unlabeled data (semi-supervised learning) is an important issue. Although a num- ber of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear. This paper presents a novel semi-supervised method that em- ploys a learning paradigm which we call structural learning. The idea is to find "what good classifiers are like" by learn- ing from thousands of automatically gen- erated auxiliary classification problems on unlabeled data. By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve perfor- mance on the target problem. The method produces performance higher than the pre- vious best results on CoNLL'00 syntac- tic chunking and CoNLL'03 named entity chunking (English and German).
800432 :: Content-Based Image Retrieval at the End of the Early Years :: multimedia_data.txt :: Presents a review of 200 references in content-based image retrieval. The paper starts with discussing the working conditions of content-based retrieval: patterns of use, types of pictures, the role of semantics, and the sensory gap. Subsequent sections discuss computational steps for image retrieval systems. Step one of the review is image processing for retrieval sorted by color, texture, and local geometry. Features for retrieval are discussed next, sorted by: accumulative and global features, salient points, object and shape features, signs, and structural combinations thereof. Similarity of pictures and objects in pictures is reviewed for each of the feature types, in close connection to the types and means of feedback the user of the systems is capable of giving by interaction. We briefly discuss aspects of system engineering: databases, system architecture, and evaluation. In the concluding section, we present our view on: the driving force of the field, the heritage from computer vision, the influence on computer vision, the role of similarity and of interaction, the need for databases, the problem of evaluation, and the role of the semantic gap. :: Presents a review of 200 references in content-based image retrieval. The paper starts with discussing the working conditions of content-based retrieval: patterns of use, types of pictures, the role of semantics, and the sensory gap. Subsequent sections discuss computational steps for image retrieval systems. Step one of the review is image processing for retrieval sorted by color, texture, and local geometry. Features for retrieval are discussed next, sorted by: accumulative and global features, salient points, object and shape features, signs, and structural combinations thereof. Similarity of pictures and objects in pictures is reviewed for each of the feature types, in close connection to the types and means of feedback the user of the systems is capable of giving by interaction. We briefly discuss aspects of system engineering: databases, system architecture, and evaluation. In the concluding section, we present our view on: the driving force of the field, the heritage from computer vision, the influence on computer vision, the role of similarity and of interaction, the need for databases, the problem of evaluation, and the role of the semantic gap.
2464493 :: Semantic role labeling of nominalized predicates in Chinese :: natural_language_and_speech_data.txt :: Recent work on semantic role labeling (SRL) has focused almost exclusively on the analysis of the predicate-argument structure of verbs, largely due to the lack of human-annotated resources for other types of predicates that can serve as train- ing and test data for the semantic role labeling systems. However, it is well- known that verbs are not the only type of predicates that can take arguments. Most notably, nouns that are nominalized forms of verbs and relational nouns gen- erally are also considered to have their own predicate-argument structure. In this paper we report results of SRL experi- ments on nominalized predicates in Chi- nese, using a newly completed corpus, the Chinese Nombank. We also dis- cuss the impact of using publicly avail- able manually annotated verb data to im- prove the SRL accuracy of nouns, exploit- ing a widely-held assumption that verbs and their nominalizations share the same predicate-argument structure. Finally, we discuss the results of applying reranking techniques to improve SRL accuracy for nominalized predicates, which showed in- significant improvement. :: Recent work on semantic role labeling (SRL) has focused almost exclusively on the analysis of the predicate-argument structure of verbs, largely due to the lack of human-annotated resources for other types of predicates that can serve as train- ing and test data for the semantic role labeling systems. However, it is well- known that verbs are not the only type of predicates that can take arguments. Most notably, nouns that are nominalized forms of verbs and relational nouns gen- erally are also considered to have their own predicate-argument structure. In this paper we report results of SRL experi- ments on nominalized predicates in Chi- nese, using a newly completed corpus, the Chinese Nombank. We also dis- cuss the impact of using publicly avail- able manually annotated verb data to im- prove the SRL accuracy of nouns, exploit- ing a widely-held assumption that verbs and their nominalizations share the same predicate-argument structure. Finally, we discuss the results of applying reranking techniques to improve SRL accuracy for nominalized predicates, which showed in- significant improvement.
22606 :: Generation that Exploits Corpus-Based Statistical Knowledge :: natural_language_and_speech_data.txt :: We describe novel aspects of a new natural language generator called Nitrogen. This generator has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts the burden of many linguistic decisions to the statistical post-processor. The generation algorithm is compositional, making it efficient, yet it also handles non-compositional aspects of language. Nitrogen's design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities. :: We describe novel aspects of a new natural language generator called Nitrogen. This generator has a highly flexible input representation that allows a spectrum of input from syntactic to semantic depth, and shifts the burden of many linguistic decisions to the statistical post-processor. The generation algorithm is compositional, making it efficient, yet it also handles non-compositional aspects of language. Nitrogen's design makes it robust and scalable, operating with lexicons and knowledge bases of one hundred thousand entities.
4572760 :: Situated Models of Meaning for Sports Video Retrieval :: natural_language_and_speech_data.txt :: Situated models of meaning ground words in the non-linguistic context, or situation, to which they refer. Applying such models to sports video re- trieval requires learning appropriate representa- tions for complex events. We propose a method that uses data mining to discover temporal pat- terns in video, and pair these patterns with associ- ated closed captioning text. This paired corpus is used to train a situated model of meaning that sig- nificantly improves video retrieval performance. :: Situated models of meaning ground words in the non-linguistic context, or situation, to which they refer. Applying such models to sports video re- trieval requires learning appropriate representa- tions for complex events. We propose a method that uses data mining to discover temporal pat- terns in video, and pair these patterns with associ- ated closed captioning text. This paired corpus is used to train a situated model of meaning that sig- nificantly improves video retrieval performance.
3727296 :: Semantic Role Labeling via Tree Kernel Joint Inference :: natural_language_and_speech_data.txt :: Recent work on Semantic Role Labeling (SRL) has shown that to achieve high accuracy a joint inference on the whole predicate argument structure should be ap- plied. In this paper, we used syntactic sub- trees that span potential argument struc- tures of the target predicate in tree ker- nel functions. This allows Support Vec- tor Machines to discern between correct and incorrect predicate structures and to re-rank them based on the joint probabil- ity of their arguments. Experiments on the PropBank data show that both classifica- tion and re-ranking based on tree kernels can improve SRL systems. :: Recent work on Semantic Role Labeling (SRL) has shown that to achieve high accuracy a joint inference on the whole predicate argument structure should be ap- plied. In this paper, we used syntactic sub- trees that span potential argument struc- tures of the target predicate in tree ker- nel functions. This allows Support Vec- tor Machines to discern between correct and incorrect predicate structures and to re-rank them based on the joint probabil- ity of their arguments. Experiments on the PropBank data show that both classifica- tion and re-ranking based on tree kernels can improve SRL systems.
4997588 :: Regular Tree Grammars as a Formalism for Scope Underspecification :: natural_language_and_speech_data.txt :: We propose the use of regular tree grammars (RTGs) as a formalism for the underspecified processing of scope ambiguities. By applying standard results on RTGs, we obtain a novel algorithm for eliminating equivalent readings and the first efficient algorithm for computing the best reading of a scope ambiguity. We also show how to derive RTGs from more tradi- tional underspecified descriptions. :: We propose the use of regular tree grammars (RTGs) as a formalism for the underspecified processing of scope ambiguities. By applying standard results on RTGs, we obtain a novel algorithm for eliminating equivalent readings and the first efficient algorithm for computing the best reading of a scope ambiguity. We also show how to derive RTGs from more tradi- tional underspecified descriptions.
1834583 :: Measuring Semantic Similarity by Latent Relational Analysis :: scientific_computing_data.txt :: This paper introduces Latent Relational Analysis (LRA), a method for measuring semantic similar- ity. LRA measures similarity in the semantic rela- tions between two pairs of words. When two pairs have a high degree of relational similarity, they a re analogous. For example, the pair cat:meow is analogous to the pair dog:bark. There is evidence from cognitive science that relational similarity i s fundamental to many cognitive and linguistic tasks (e.g., analogical reasoning). In the Vector Space Model (VSM) approach to measuring relational similarity, the similarity between two pairs is cal - culated by the cosine of the angle between the vec- tors that represent the two pairs. The elements in the vectors are based on the frequencies of manu- ally constructed patterns in a large corpus. LRA ex - tends the VSM approach in three ways: (1) patterns are derived automatically from the corpus, (2) Sin- gular Value Decomposition is used to smooth the frequency data, and (3) synonyms are used to re- formulate word pairs. This paper describes the LRA algorithm and experimentally compares LRA to VSM on two tasks, answering college-level mul- tiple-choice word analogy questions and classify- ing semantic relations in noun-modifier expres- sions. LRA achieves state-of-the-art results, reach - ing human-level performance on the analogy ques- tions and significantly exceeding VSM perform- ance on both tasks. :: This paper introduces Latent Relational Analysis (LRA), a method for measuring semantic similar- ity. LRA measures similarity in the semantic rela- tions between two pairs of words. When two pairs have a high degree of relational similarity, they a re analogous. For example, the pair cat:meow is analogous to the pair dog:bark. There is evidence from cognitive science that relational similarity i s fundamental to many cognitive and linguistic tasks (e.g., analogical reasoning). In the Vector Space Model (VSM) approach to measuring relational similarity, the similarity between two pairs is cal - culated by the cosine of the angle between the vec- tors that represent the two pairs. The elements in the vectors are based on the frequencies of manu- ally constructed patterns in a large corpus. LRA ex - tends the VSM approach in three ways: (1) patterns are derived automatically from the corpus, (2) Sin- gular Value Decomposition is used to smooth the frequency data, and (3) synonyms are used to re- formulate word pairs. This paper describes the LRA algorithm and experimentally compares LRA to VSM on two tasks, answering college-level mul- tiple-choice word analogy questions and classify- ing semantic relations in noun-modifier expres- sions. LRA achieves state-of-the-art results, reach - ing human-level performance on the analogy ques- tions and significantly exceeding VSM perform- ance on both tasks.
110183 :: The 1998 AI Planning Systems Competition :: artificial_intelligence_data.txt :: The 1998 Planning Competition at the AI Planning Systems Conference was the first of its kind. Its goal was to create planning domains that a wide variety of planning researchers could agree on, so as to make comparison among planners more meaningful, measure overall progress in the field, and set up a framework for long-term creation of a repository of problems in a standard notation. A rules committee for the competition was created in 1997, and had long discussions on how the contest should... :: The 1998 Planning Competition at the AI Planning Systems Conference was the first of its kind. Its goal was to create planning domains that a wide variety of planning researchers could agree on, so as to make comparison among planners more meaningful, measure overall progress in the field, and set up a framework for long-term creation of a repository of problems in a standard notation. A rules committee for the competition was created in 1997, and had long discussions on how the contest should...
4231109 :: Tailoring Word Alignments to Syntactic Machine Translation :: natural_language_and_speech_data.txt :: Extracting tree transducer rules for syntac- tic MT systems can be hindered by word alignment errors that violate syntactic corre- spondences. We propose a novel model for unsupervised word alignment which explic- itly takes into account target language con- stituent structure, while retaining the robust- ness and efficiency of the HMM alignment model. Our model's predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posterior- based methods of reconciling bidirectional alignments. :: Extracting tree transducer rules for syntac- tic MT systems can be hindered by word alignment errors that violate syntactic corre- spondences. We propose a novel model for unsupervised word alignment which explic- itly takes into account target language con- stituent structure, while retaining the robust- ness and efficiency of the HMM alignment model. Our model's predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posterior- based methods of reconciling bidirectional alignments.
4242681 :: Clustering for unsupervised relation identification :: common_data.txt :: Unsupervised Relation Identification is the task of automatically discovering interesting relations between entities in a large text corpora. Relations are identified by clustering the frequently co-occurring pairs of entities in such a way that pairs occurring in similar contexts end up belonging to the same clusters. In this paper we compare several clustering setups, some of them novel and others already tried. The setups include feature extraction and selection methods and clustering algorithms. In order to do the comparison, we develop a clustering evaluation metric, specifically adapted for the relation identification task. Our experiments demonstrate significant superiority of the single-linkage hierarchical clustering with the novel threshold selection technique over the other tested clustering algorithms. Also, the experiments indicate that for successful relation identification it is important to use rich complex features of two kinds: features that test both relation slots together ("relation features"), and features that test only one slot each ("entity features"). We have found that using both kinds of features with the best of the algorithms produces very high-precision results, significantly improving over the previous work. :: Unsupervised Relation Identification is the task of automatically discovering interesting relations between entities in a large text corpora. Relations are identified by clustering the frequently co-occurring pairs of entities in such a way that pairs occurring in similar contexts end up belonging to the same clusters. In this paper we compare several clustering setups, some of them novel and others already tried. The setups include feature extraction and selection methods and clustering algorithms. In order to do the comparison, we develop a clustering evaluation metric, specifically adapted for the relation identification task. Our experiments demonstrate significant superiority of the single-linkage hierarchical clustering with the novel threshold selection technique over the other tested clustering algorithms. Also, the experiments indicate that for successful relation identification it is important to use rich complex features of two kinds: features that test both relation slots together ("relation features"), and features that test only one slot each ("entity features"). We have found that using both kinds of features with the best of the algorithms produces very high-precision results, significantly improving over the previous work.
473216 :: A Robust Selection System Using Real-Time Multi-Modal User-Agent Interactions :: human_computer_interaction_data.txt :: This paper presents a real-time object selection system which can deal with gaze and speech inputs that include uncertainty. Although much research has focused on integration of multi-modal information, most of it assumes that each input is accurately symbolized in advance. In addition, real-time interaction with the user is an important and desirable feature which most systems have overlooked. Unlike those systems, our system is intended to satisfy these two requirements. In our system, target objects are modeled by agents which react to user’s action in real-time. The agent’s reactions are based on integration of multi-modal inputs. We use gaze input which enables real-time detection of focus-of-attention but has low accuracy, whereas speech input has high accuracy but nonreal-time feature. ISghly accurate selection with robustness is achieved by complementary effect through probabilistic integration of these two modalities. Our first experiment shows that it is possible to select target object successfully in most cases, even if either of the modalities includes great uncertainty. :: This paper presents a real-time object selection system which can deal with gaze and speech inputs that include uncertainty. Although much research has focused on integration of multi-modal information, most of it assumes that each input is accurately symbolized in advance. In addition, real-time interaction with the user is an important and desirable feature which most systems have overlooked. Unlike those systems, our system is intended to satisfy these two requirements. In our system, target objects are modeled by agents which react to user’s action in real-time. The agent’s reactions are based on integration of multi-modal inputs. We use gaze input which enables real-time detection of focus-of-attention but has low accuracy, whereas speech input has high accuracy but nonreal-time feature. ISghly accurate selection with robustness is achieved by complementary effect through probabilistic integration of these two modalities. Our first experiment shows that it is possible to select target object successfully in most cases, even if either of the modalities includes great uncertainty.
4231023 :: Generalizing semantic role annotations across syntactically similar verbs :: natural_language_and_speech_data.txt :: Large corpora of parsed sentences with semantic role labels (e.g. PropBank) pro- vide training data for use in the creation of high-performance automatic semantic role labeling systems. Despite the size of these corpora, individual verbs (or role- sets) often have only a handful of in- stances in these corpora, and only a fraction of English verbs have even a sin- gle annotation. In this paper, we describe an approach for dealing with this sparse data problem, enabling accurate semantic role labeling for novel verbs (rolesets) with only a single training example. Our approach involves the identification of syntactically similar verbs found in Prop- Bank, the alignment of arguments in their corresponding rolesets, and the use of their corresponding annotations in Prop- Bank as surrogate training data. :: Large corpora of parsed sentences with semantic role labels (e.g. PropBank) pro- vide training data for use in the creation of high-performance automatic semantic role labeling systems. Despite the size of these corpora, individual verbs (or role- sets) often have only a handful of in- stances in these corpora, and only a fraction of English verbs have even a sin- gle annotation. In this paper, we describe an approach for dealing with this sparse data problem, enabling accurate semantic role labeling for novel verbs (rolesets) with only a single training example. Our approach involves the identification of syntactically similar verbs found in Prop- Bank, the alignment of arguments in their corresponding rolesets, and the use of their corresponding annotations in Prop- Bank as surrogate training data.
5274910 :: Exploiting Feature Hierarchy for Transfer Learning in Named Entity Recognition :: natural_language_and_speech_data.txt :: We present a novel hierarchical prior struc- ture for supervised transfer learning in named entity recognition, motivated by the common structure of feature spaces for this task across natural language data sets. The problem of transfer learning, where information gained in one learning task is used to improve perfor- mance in another related task, is an important new area of research. In the subproblem of do- main adaptation, a model trained over a source domain is generalized to perform well on a re- lated target domain, where the two domains' data are distributed similarly, but not identi- cally. We introduce the concept of groups of closely-related domains, called genres, and show how inter-genre adaptation is related to domain adaptation. We also examine multi- task learning, where two domains may be re- lated, but where the concept to be learned in each case is distinct. We show that our prior conveys useful information across domains, genres and tasks, while remaining robust to spurious signals not related to the target do- main and concept. We further show that our model generalizes a class of similar hierarchi- cal priors, smoothed to varying degrees, and lay the groundwork for future exploration in this area. :: We present a novel hierarchical prior struc- ture for supervised transfer learning in named entity recognition, motivated by the common structure of feature spaces for this task across natural language data sets. The problem of transfer learning, where information gained in one learning task is used to improve perfor- mance in another related task, is an important new area of research. In the subproblem of do- main adaptation, a model trained over a source domain is generalized to perform well on a re- lated target domain, where the two domains' data are distributed similarly, but not identi- cally. We introduce the concept of groups of closely-related domains, called genres, and show how inter-genre adaptation is related to domain adaptation. We also examine multi- task learning, where two domains may be re- lated, but where the concept to be learned in each case is distinct. We show that our prior conveys useful information across domains, genres and tasks, while remaining robust to spurious signals not related to the target do- main and concept. We further show that our model generalizes a class of similar hierarchi- cal priors, smoothed to varying degrees, and lay the groundwork for future exploration in this area.
4905037 :: Grounded Language Modeling for Automatic Speech Recognition of Sports Video :: natural_language_and_speech_data.txt :: Grounded language models represent the rela- tionship between words and the non-linguistic context in which they are said. This paper de- scribes how they are learned from large cor- pora of unlabeled video, and are applied to the task of automatic speech recognition of sports video. Results show that grounded language models improve perplexity and word error rate over text based language models, and fur- ther, support video information retrieval better than human generated speech transcriptions. :: Grounded language models represent the rela- tionship between words and the non-linguistic context in which they are said. This paper de- scribes how they are learned from large cor- pora of unlabeled video, and are applied to the task of automatic speech recognition of sports video. Results show that grounded language models improve perplexity and word error rate over text based language models, and fur- ther, support video information retrieval better than human generated speech transcriptions.
4860637 :: Active Sample Selection for Named Entity Transliteration :: natural_language_and_speech_data.txt :: This paper introduces a new method for identifying named-entity (NE) transliterations within bilingual corpora. Current state-of-the- art approaches usually require annotated data and relevant linguistic knowledge which may not be available for all languages. We show how to effectively train an accurate transliter- ation classifier using very little data, obtained automatically. To perform this task, we intro- duce a new active sampling paradigm for guid- ing and adapting the sample selection process. We also investigate how to improve the clas- sifier by identifying repeated patterns in the training data. We evaluated our approach us- ing English, Russian and Hebrew corpora. :: This paper introduces a new method for identifying named-entity (NE) transliterations within bilingual corpora. Current state-of-the- art approaches usually require annotated data and relevant linguistic knowledge which may not be available for all languages. We show how to effectively train an accurate transliter- ation classifier using very little data, obtained automatically. To perform this task, we intro- duce a new active sampling paradigm for guid- ing and adapting the sample selection process. We also investigate how to improve the clas- sifier by identifying repeated patterns in the training data. We evaluated our approach us- ing English, Russian and Hebrew corpora.
4231182 :: Formalism-Independent Parser Evaluation with CCG and DepBank :: natural_language_and_speech_data.txt :: A key question facing the parsing commu- nity is how to compare parsers which use different grammar formalisms and produce different output. Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance. In this paper we evaluate a CCG parser on DepBank, and demonstrate the difficulties in converting the parser output into Dep- Bank grammatical relations. In addition we present a method for measuring the effec- tiveness of the conversion, which provides an upper bound on parsing accuracy. The CCG parser obtains an F-score of 81.9% on labelled dependencies, against an upper bound of 84.8%. We compare the CCG parser against the RASP parser, outperform- ing RASP by over 5% overall and on the ma- jority of dependency types. :: A key question facing the parsing commu- nity is how to compare parsers which use different grammar formalisms and produce different output. Evaluating a parser on the same resource used to create it can lead to non-comparable accuracy scores and an over-optimistic view of parser performance. In this paper we evaluate a CCG parser on DepBank, and demonstrate the difficulties in converting the parser output into Dep- Bank grammatical relations. In addition we present a method for measuring the effec- tiveness of the conversion, which provides an upper bound on parsing accuracy. The CCG parser obtains an F-score of 81.9% on labelled dependencies, against an upper bound of 84.8%. We compare the CCG parser against the RASP parser, outperform- ing RASP by over 5% overall and on the ma- jority of dependency types.
4231183 :: Domain Adaptation with Active Learning for Word Sense Disambiguation :: natural_language_and_speech_data.txt :: When a word sense disambiguation (WSD) system is trained on one domain but ap- plied to a different domain, a drop in ac- curacy is frequently observed. This high- lights the importance of domain adaptation for word sense disambiguation. In this pa- per, we first show that an active learning ap- proach can be successfully used to perform domain adaptation of WSD systems. Then, by using the predominant sense predicted by expectation-maximization (EM) and adopt- ing a count-merging technique, we improve the effectiveness of the original adaptation process achieved by the basic active learn- ing approach. :: When a word sense disambiguation (WSD) system is trained on one domain but ap- plied to a different domain, a drop in ac- curacy is frequently observed. This high- lights the importance of domain adaptation for word sense disambiguation. In this pa- per, we first show that an active learning ap- proach can be successfully used to perform domain adaptation of WSD systems. Then, by using the predominant sense predicted by expectation-maximization (EM) and adopt- ing a count-merging technique, we improve the effectiveness of the original adaptation process achieved by the basic active learn- ing approach.
166772 :: Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach :: common_data.txt :: In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. We tested our WSD program, named L. :: In this paper, we present a new approach for word sense disambiguation (WSD) using an exemplar-based learning algorithm. This approach integrates a diverse set of knowledge sources to disambiguate word sense, including part of speech of neighboring words, morphological form, the unordered set of surrounding words, local collocations, and verb-object syntactic relation. We tested our WSD program, named L.
2182013 :: Automatic Discovery of Part-Whole Relations :: natural_language_and_speech_data.txt :: An important problem in knowledge discovery from text is the automatic extraction of semantic relations. This paper presents a supervised, semantically intensive, domain independent ap- proach for the automatic detection of part-whole relations in text. First an algorithm is described that identifies lexico-syntactic patterns that encode part-whole relations. A difficulty is that these patterns also encode other semantic relations, and a learning method is necessary to discriminate whether or not a pattern contains a part-whole relation. A large set of training examples have been annotated and fed into a specialized learning system that learns classification rules. The rules are learned through an iterative semantic specialization (ISS) method applied to noun phrase constituents. Classification rules have been generated this way for different patterns such as genitives, noun compounds, and noun phrases containing prepositional phrases to extract part-whole relations from them. The applicability of these rules has been tested on a test corpus obtaining an overall average precision of 80.95% and recall of 75.91%. The results demonstrate the importance of word sense disambiguation for this task. They also demonstrate that different lexico-syntactic patterns encode different semantic information and should be treated separately in the sense that different clarification rules apply to different patterns. :: An important problem in knowledge discovery from text is the automatic extraction of semantic relations. This paper presents a supervised, semantically intensive, domain independent ap- proach for the automatic detection of part-whole relations in text. First an algorithm is described that identifies lexico-syntactic patterns that encode part-whole relations. A difficulty is that these patterns also encode other semantic relations, and a learning method is necessary to discriminate whether or not a pattern contains a part-whole relation. A large set of training examples have been annotated and fed into a specialized learning system that learns classification rules. The rules are learned through an iterative semantic specialization (ISS) method applied to noun phrase constituents. Classification rules have been generated this way for different patterns such as genitives, noun compounds, and noun phrases containing prepositional phrases to extract part-whole relations from them. The applicability of these rules has been tested on a test corpus obtaining an overall average precision of 80.95% and recall of 75.91%. The results demonstrate the importance of word sense disambiguation for this task. They also demonstrate that different lexico-syntactic patterns encode different semantic information and should be treated separately in the sense that different clarification rules apply to different patterns.
6340884 :: Assessing the Costs of Sampling Methods in Active Learning for Annotation :: natural_language_and_speech_data.txt :: Traditional Active Learning (AL) techniques assume that the annotation of each datum costs the same. This is not the case when anno- tating sequences; some sequences will take longer than others. We show that the AL tech- nique which performs best depends on how cost is measured. Applying an hourly cost model based on the results of an annotation user study, we approximate the amount of time necessary to annotate a given sentence. This model allows us to evaluate the effectiveness of AL sampling methods in terms of time spent in annotation. We acheive a 77% re- duction in hours from a random baseline to achieve 96.5% tag accuracy on the Penn Tree- bank. More significantly, we make the case for measuring cost in assessing AL methods. :: Traditional Active Learning (AL) techniques assume that the annotation of each datum costs the same. This is not the case when anno- tating sequences; some sequences will take longer than others. We show that the AL tech- nique which performs best depends on how cost is measured. Applying an hourly cost model based on the results of an annotation user study, we approximate the amount of time necessary to annotate a given sentence. This model allows us to evaluate the effectiveness of AL sampling methods in terms of time spent in annotation. We acheive a 77% re- duction in hours from a random baseline to achieve 96.5% tag accuracy on the Penn Tree- bank. More significantly, we make the case for measuring cost in assessing AL methods.
3221004 :: Improved HMM Alignment Models for Languages with Scarce Resources :: natural_language_and_speech_data.txt :: We introduce improvements to statistical word alignment based on the Hidden Markov Model. One improvement incorporates syntac- tic knowledge. Results on the workshop data show that alignment performance exceeds that of a state-of-the art system based on more com- plex models, resulting in over a 5.5% absolute reduction in error on Romanian-English. :: We introduce improvements to statistical word alignment based on the Hidden Markov Model. One improvement incorporates syntac- tic knowledge. Results on the workshop data show that alignment performance exceeds that of a state-of-the art system based on more com- plex models, resulting in over a 5.5% absolute reduction in error on Romanian-English.
5590651 :: Lexicalized Phonotactic Word Segmentation :: natural_language_and_speech_data.txt :: This paper presents a new unsupervised algo- rithm (WordEnds) for inferring word bound- aries from transcribed adult conversations. Phone ngrams before and after observed pauses are used to bootstrap a simple dis- criminative model of boundary marking. This fast algorithm delivers high performance even on morphologically complex words in English and Arabic, and promising results on accurate phonetic transcriptions with extensive pronun- ciation variation. Expanding training data be- yond the traditional miniature datasets pushes performance numbers well above those previ- ously reported. This suggests that WordEnds is a viable model of child language acquisition and might be useful in speech understanding. :: This paper presents a new unsupervised algo- rithm (WordEnds) for inferring word bound- aries from transcribed adult conversations. Phone ngrams before and after observed pauses are used to bootstrap a simple dis- criminative model of boundary marking. This fast algorithm delivers high performance even on morphologically complex words in English and Arabic, and promising results on accurate phonetic transcriptions with extensive pronun- ciation variation. Expanding training data be- yond the traditional miniature datasets pushes performance numbers well above those previ- ously reported. This suggests that WordEnds is a viable model of child language acquisition and might be useful in speech understanding.
5013555 :: Inducing Gazetteers for Named Entity Recognition by Large-Scale Clustering of Dependency Relations :: natural_language_and_speech_data.txt :: We propose using large-scale clustering of de- pendency relations between verbs and multi- word nouns (MNs) to construct a gazetteer for named entity recognition (NER). Since depen- dency relations capture the semantics of MNs well, the MN clusters constructed by using dependency relations should serve as a good gazetteer. However, the high level of computa- tional cost has prevented the use of clustering for constructing gazetteers. We parallelized a clustering algorithm based on expectation- maximization (EM) and thus enabled the con- struction of large-scale MN clusters. We demonstrated with the IREX dataset for the Japanese NER that using the constructed clus- ters as a gazetteer (cluster gazetteer) is a effec- tive way of improving the accuracy of NER. Moreover, we demonstrate that the combina- tion of the cluster gazetteer and a gazetteer ex- tracted from Wikipedia, which is also useful for NER, can further improve the accuracy in several cases. :: We propose using large-scale clustering of de- pendency relations between verbs and multi- word nouns (MNs) to construct a gazetteer for named entity recognition (NER). Since depen- dency relations capture the semantics of MNs well, the MN clusters constructed by using dependency relations should serve as a good gazetteer. However, the high level of computa- tional cost has prevented the use of clustering for constructing gazetteers. We parallelized a clustering algorithm based on expectation- maximization (EM) and thus enabled the con- struction of large-scale MN clusters. We demonstrated with the IREX dataset for the Japanese NER that using the constructed clus- ters as a gazetteer (cluster gazetteer) is a effec- tive way of improving the accuracy of NER. Moreover, we demonstrate that the combina- tion of the cluster gazetteer and a gazetteer ex- tracted from Wikipedia, which is also useful for NER, can further improve the accuracy in several cases.
2284234 :: Bayesian Multinomial Logistic Regression for Author Identication :: common_data.txt :: Motivated by high-dimensional applications in authorship atttribution, we describe a Bayesian multinomial logistic regression model together with an associated learning algorithm. :: Motivated by high-dimensional applications in authorship atttribution, we describe a Bayesian multinomial logistic regression model together with an associated learning algorithm.
1831717 :: Shot Detection and Motion Analysis for Automatic MPEG7 Annotation of Sports Videos :: multimedia_data.txt :: 5 :: 5
2034028 :: An empirical study of smoothing techniques for language modeling :: natural_language_and_speech_data.txt :: We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980); Katz (1987); Bell, Cleary and Witten (1990); Ney, Essen and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus (e.g. Brown vs. Wall Street Journal), count cutoffs, and n -gram order (bigram vs. trigram) affect the relative performance of these methods, which is measured through the cross-entropy of test data. We find that these factors can significantly affect the relative performance of models, with the most significant factor being training data size. Since no previous comparisons have examined these factors systematically, this is the first thorough characterization of the relative performance of various algorithms. In addition, we introduce methodologies for analyzing smoothing algorithm efficacy in detail, and using these techniques we motivate a novel variation of Kneser–Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented. :: We survey the most widely-used algorithms for smoothing models for language n -gram modeling. We then present an extensive empirical comparison of several of these smoothing techniques, including those described by Jelinek and Mercer (1980); Katz (1987); Bell, Cleary and Witten (1990); Ney, Essen and Kneser (1994), and Kneser and Ney (1995). We investigate how factors such as training data size, training corpus (e.g. Brown vs. Wall Street Journal), count cutoffs, and n -gram order (bigram vs. trigram) affect the relative performance of these methods, which is measured through the cross-entropy of test data. We find that these factors can significantly affect the relative performance of models, with the most significant factor being training data size. Since no previous comparisons have examined these factors systematically, this is the first thorough characterization of the relative performance of various algorithms. In addition, we introduce methodologies for analyzing smoothing algorithm efficacy in detail, and using these techniques we motivate a novel variation of Kneser–Ney smoothing that consistently outperforms all other algorithms evaluated. Finally, results showing that improved language model smoothing leads to improved speech recognition performance are presented.
908807 :: Probabilistic author-topic models for information discovery :: data_mining_data.txt :: We propose a new unsupervised learning technique for extracting information from large text collections. We model documents as if they were generated by a two-stage stochastic process. Each author is represented by a probability distribution over topics, and each topic is represented as a probability distribution over words for that topic. The words in a multi-author paper are assumed to be the result of a mixture of each authors' topic mixture. The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm. We apply the methodology to a large corpus of 160,000 abstracts and 85,000 authors from the well-known CiteSeer digital library, and learn a model with 300 topics. We discuss in detail the interpretation of the results discovered by the system including specific topic and author models, ranking of authors by topic and topics by author, significant trends in the computer science literature between 1990 and 2002, parsing of abstracts by topics and authors and detection of unusual papers by specific authors. An online query interface to the model is also discussed that allows interactive exploration of author-topic models for corpora such as CiteSeer. :: We propose a new unsupervised learning technique for extracting information from large text collections. We model documents as if they were generated by a two-stage stochastic process. Each author is represented by a probability distribution over topics, and each topic is represented as a probability distribution over words for that topic. The words in a multi-author paper are assumed to be the result of a mixture of each authors' topic mixture. The topic-word and author-topic distributions are learned from data in an unsupervised manner using a Markov chain Monte Carlo algorithm. We apply the methodology to a large corpus of 160,000 abstracts and 85,000 authors from the well-known CiteSeer digital library, and learn a model with 300 topics. We discuss in detail the interpretation of the results discovered by the system including specific topic and author models, ranking of authors by topic and topics by author, significant trends in the computer science literature between 1990 and 2002, parsing of abstracts by topics and authors and detection of unusual papers by specific authors. An online query interface to the model is also discussed that allows interactive exploration of author-topic models for corpora such as CiteSeer.
4563862 :: UCB: System Description for SemEval Task #4 :: natural_language_and_speech_data.txt :: The UC Berkeley team participated in the SemEval 2007 Task #4, with an approach that leverages the vast size of the Web in or- der to build lexically-specific features. The idea is to determine which verbs, preposi- tions, and conjunctions are used in sentences containing a target word pair, and to com- pare those to features extracted for other word pairs in order to determine which are most similar. By combining these Web fea- tures with words from the sentence context, our team was able to achieve the best results for systems of category C and third best for systems of category A. :: The UC Berkeley team participated in the SemEval 2007 Task #4, with an approach that leverages the vast size of the Web in or- der to build lexically-specific features. The idea is to determine which verbs, preposi- tions, and conjunctions are used in sentences containing a target word pair, and to com- pare those to features extracted for other word pairs in order to determine which are most similar. By combining these Web fea- tures with words from the sentence context, our team was able to achieve the best results for systems of category C and third best for systems of category A.
688622 :: Semantic construction in F-TAG :: natural_language_and_speech_data.txt :: 25 :: 25
1468702 :: Exploiting latent semantic information in statistical language modeling :: natural_language_and_speech_data.txt :: Statistical language models used in large-vocabulary speech recognition must properly encapsulate the various constraints, both local and global, present in the language. While local constraints are readily captured through n-gram modeling, global constraints, such as long-term semantic dependencies, have been more difficult to handle within a data-driven formalism. This paper focuses on the use of latent semantic analysis, a paradigm that automatically uncovers the salient semantic relationships between words and documents in a given corpus. In this approach, (discrete) words and documents are mapped onto a (continuous) semantic vector space, in which familiar clustering techniques can be applied. This leads to the specification of a powerful framework for automatic semantic classification, as well as the derivation of several language model families with various smoothing properties. Because of their large-span nature, these language models are well suited to complement conventional n-grams. An integrative formulation is proposed for harnessing this synergy, in which the latent semantic information is used to adjust the standard n-gram probability. Such hybrid language modeling compares favorably with the corresponding n-gram baseline: experiments conducted on the Wall Street Journal domain show a reduction in average word error rate of over 20%. This paper concludes with a discussion of intrinsic tradeoffs, such as the influence of training data selection on the resulting performance :: Statistical language models used in large-vocabulary speech recognition must properly encapsulate the various constraints, both local and global, present in the language. While local constraints are readily captured through n-gram modeling, global constraints, such as long-term semantic dependencies, have been more difficult to handle within a data-driven formalism. This paper focuses on the use of latent semantic analysis, a paradigm that automatically uncovers the salient semantic relationships between words and documents in a given corpus. In this approach, (discrete) words and documents are mapped onto a (continuous) semantic vector space, in which familiar clustering techniques can be applied. This leads to the specification of a powerful framework for automatic semantic classification, as well as the derivation of several language model families with various smoothing properties. Because of their large-span nature, these language models are well suited to complement conventional n-grams. An integrative formulation is proposed for harnessing this synergy, in which the latent semantic information is used to adjust the standard n-gram probability. Such hybrid language modeling compares favorably with the corresponding n-gram baseline: experiments conducted on the Wall Street Journal domain show a reduction in average word error rate of over 20%. This paper concludes with a discussion of intrinsic tradeoffs, such as the influence of training data selection on the resulting performance
4993738 :: ILK: Machine learning of semantic relations with shallow features and almost no data :: natural_language_and_speech_data.txt :: 1 :: 1
3715440 :: Semantic Role Labelling With Similarity-Based Generalisation Using EM-based Clustering :: natural_language_and_speech_data.txt :: We describe a system for semantic role assignment built as part of the Senseval III task, based on an off-the-shelf parser and Maxent and Memory-Based learners. We focus on generalisation using several similarity measures to increase the amount of train- ing data available and on the use of EM-based clus- tering to improve role assignment. Our final score is Precision=73.6%, Recall=59.4% (F=65.7). :: We describe a system for semantic role assignment built as part of the Senseval III task, based on an off-the-shelf parser and Maxent and Memory-Based learners. We focus on generalisation using several similarity measures to increase the amount of train- ing data available and on the use of EM-based clus- tering to improve role assignment. Our final score is Precision=73.6%, Recall=59.4% (F=65.7).
2464788 :: A Discriminative Framework for Bilingual Word Alignment :: natural_language_and_speech_data.txt :: Bilingual word alignment forms the foun- dation of most approaches to statistical machine translation. Current word align- ment methods are predominantly based on generative models. In this paper, we demonstrate a discriminative approach to training simple word alignment mod- els that are comparable in accuracy to the more complex generative models nor- mally used. These models have the the advantages that they are easy to add fea- tures to and they allow fast optimization of model parameters using small amounts of annotated data. :: Bilingual word alignment forms the foun- dation of most approaches to statistical machine translation. Current word align- ment methods are predominantly based on generative models. In this paper, we demonstrate a discriminative approach to training simple word alignment mod- els that are comparable in accuracy to the more complex generative models nor- mally used. These models have the the advantages that they are easy to add fea- tures to and they allow fast optimization of model parameters using small amounts of annotated data.
2182006 :: Sentence Fusion for Multidocument News Summarization :: natural_language_and_speech_data.txt :: A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading. In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents. Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence. Sentence fusion moves the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources. :: A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading. In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents. Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence. Sentence fusion moves the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources.
81067 :: Matching Words and Pictures :: common_data.txt :: We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann's hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data. :: We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann's hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data.
2394009 :: Dynamic programming for parsing and estimation of stochastic unification-based grammars :: natural_language_and_speech_data.txt :: 44 :: 44
6200749 :: Generating phonetic cognates to handle named entities in English-Chinese cross-language spoken document retrieval :: natural_language_and_speech_data.txt :: We have developed a technique for automatic transliteration of named entities for English-Chinese cross-language spoken document retrieval (CL-SDR). Our retrieval system integrates machine translation, speech recognition and information retrieval technologies. An English news story forms a textual query that is automatically translated into Chinese words, which are mapped into Mandarin syllables by pronunciation dictionary lookup. Mandarin radio news broadcasts form spoken documents that are indexed by word and syllable recognition. The information retrieval engine performs matching in both word and syllable scales. The English queries contain many named entities that tend to be out-of-vocabulary words for machine translation and speech recognition, and are omitted in retrieval. Names are often transliterated across languages and are generally important for retrieval. We present a technique that takes in a name spelling and automatically generates a phonetic cognate in terms of Chinese syllables to be used in retrieval. Experiments show consistent retrieval performance improvement by including the use of named entities in this way. :: We have developed a technique for automatic transliteration of named entities for English-Chinese cross-language spoken document retrieval (CL-SDR). Our retrieval system integrates machine translation, speech recognition and information retrieval technologies. An English news story forms a textual query that is automatically translated into Chinese words, which are mapped into Mandarin syllables by pronunciation dictionary lookup. Mandarin radio news broadcasts form spoken documents that are indexed by word and syllable recognition. The information retrieval engine performs matching in both word and syllable scales. The English queries contain many named entities that tend to be out-of-vocabulary words for machine translation and speech recognition, and are omitted in retrieval. Names are often transliterated across languages and are generally important for retrieval. We present a technique that takes in a name spelling and automatically generates a phonetic cognate in terms of Chinese syllables to be used in retrieval. Experiments show consistent retrieval performance improvement by including the use of named entities in this way.
179215 :: Indexing by Latent Semantic Analysis :: common_data.txt :: A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising :: A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising
514717 :: Syntax Directed Transduction :: algorithms_and_theory_data.txt :: A transduction is a mapping from one set of sequences to another. A syntax directed transduction is a particular type of transduction which is defined on the grammar of a context free language and which is meant to be a model of part of the translation process used in many compilers. The transduction is considered from an automata theory viewpoint as specifying the input-output relation of a machine. There is a close relationship between a subclass of these transductions, the simple syntax directed transductions, and push-down machines. This relationship is investigated in detail and conditions for such transductions to be performed on push-down machines are obtained. In addition, some time bounds for transductions on Turing machines are derived. :: A transduction is a mapping from one set of sequences to another. A syntax directed transduction is a particular type of transduction which is defined on the grammar of a context free language and which is meant to be a model of part of the translation process used in many compilers. The transduction is considered from an automata theory viewpoint as specifying the input-output relation of a machine. There is a close relationship between a subclass of these transductions, the simple syntax directed transductions, and push-down machines. This relationship is investigated in detail and conditions for such transductions to be performed on push-down machines are obtained. In addition, some time bounds for transductions on Turing machines are derived.
5126930 :: Phrase Table Training for Precision and Recall: What Makes a Good Phrase and a Good Phrase Pair? :: natural_language_and_speech_data.txt :: In this work, the problem of extracting phrase translation is formulated as an information re- trieval process implemented with a log-linear model aiming for a balanced precision and re- call. We present a generic phrase training al- gorithm which is parameterized with feature functions and can be optimized jointly with the translation engine to directly maximize the end-to-end system performance. Multiple data-driven feature functions are proposed to capture the quality and confidence of phrases and phrase pairs. Experimental results demon- strate consistent and significant improvement over the widely used method that is based on word alignment matrix only. :: In this work, the problem of extracting phrase translation is formulated as an information re- trieval process implemented with a log-linear model aiming for a balanced precision and re- call. We present a generic phrase training al- gorithm which is parameterized with feature functions and can be optimized jointly with the translation engine to directly maximize the end-to-end system performance. Multiple data-driven feature functions are proposed to capture the quality and confidence of phrases and phrase pairs. Experimental results demon- strate consistent and significant improvement over the widely used method that is based on word alignment matrix only.
1782520 :: WordNet::Similarity - Measuring the Relatedness of Concepts :: common_data.txt :: 297 :: 297
4994026 :: A Joint Model of Text and Aspect Ratings for Sentiment Summarization :: natural_language_and_speech_data.txt :: Online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects. We propose a statistical model which is able to discover corresponding topics in text and extract tex- tual evidence from reviews supporting each of these aspect ratings - a fundamental problem in aspect-based sentiment summarization (Hu and Liu, 2004a). Our model achieves high ac- curacy, without any explicitly labeled data ex- cept the user provided opinion ratings. The proposed approach is general and can be used for segmentation in other applications where sequential data is accompanied with corre- lated signals. :: Online reviews are often accompanied with numerical ratings provided by users for a set of service or product aspects. We propose a statistical model which is able to discover corresponding topics in text and extract tex- tual evidence from reviews supporting each of these aspect ratings - a fundamental problem in aspect-based sentiment summarization (Hu and Liu, 2004a). Our model achieves high ac- curacy, without any explicitly labeled data ex- cept the user provided opinion ratings. The proposed approach is general and can be used for segmentation in other applications where sequential data is accompanied with corre- lated signals.
