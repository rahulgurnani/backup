1345310 4997588 Redundancy elimination (Vestre, 1991; Chaves, 2003; Koller and Thater, 2006) is the problem of deriving from an USR U another USR U0, such that the readings of U0 are a proper subset of the readings of U, but every reading in U is semantically equivalent to some reading in U0. For instance, the ::: Redundancy elimination ( ) is the problem of deriving from an USR U another USR U ' , such that the readings of U ' are a proper subset of the read-ings of U  , but every reading in U is semantically equivalent to some reading in U ' ::: NN NN ( ) VBZ DT NN IN VVG IN DT NP NP DT NP NP POS , JJ IN/that DT NNS IN NP POS VBP DT JJ NN IN DT NNS IN NP , CC DT NN IN NP VBZ RB JJ TO DT NN IN NP POS ::: BackGround ::: 0.830041503113
17406 5742263 To extract such word clusters we used suffix arrays proposed in Yamamoto and Church (2001) and the pointwise mutual information measure, see Church and Hanks (1990) ::: To extract such word clusters we used suffix arrays proposed in Ya-mamoto and Church ( ) and the pointwise mutual information measure ::: TO VV JJ NN NNS PP VVD NN NNS VVN IN NP CC NP ( ) CC DT JJ JJ NN NN ::: Fundamental ::: 0.834057656228
10987639 4231117 Harbusch et al. (2006) present a generation workbench, which has the goal of producing not the most appropriate order, but all grammatical ones ::: Harbusch et al. ( ) present a generation workbench , which has the goal of producing not the most appropriate order , but all grammatical ones ::: NP NP NP ( ) VV DT NN NN , WDT VHZ DT NN IN VVG RB DT RBS JJ NN , CC DT JJ NNS ::: BackGround ::: 0.942809041582
16479 4231125 In order to ensure this determinism, NLG geared realisers generally rely on theories of grammar which systematically link form to function such as systemic functional grammar (SFG, (Matthiessen and Bateman, 1991)) and, to a lesser extent, Meaning Text Theory (MTT, (Melâ€™cuk, 1988)) ::: In order to ensure this determinism , NLG geared realisers generally rely on theories of grammar which systematically link form to function such as systemic functional grammar (SFG , ( )) and , to a lesser extent , Meaning Text Theory (MTT , ( )) ::: IN NN TO VV DT NN , NP VVN NNS RB VVP IN NNS IN NN WDT RB VVP NN TO VV JJ IN JJ JJ NN NN , ( NN CC , TO DT JJR NN , NP NP NP NP , ( NN ::: BackGround ::: 0.914755081704
16479 4231125 Similarly, KPML (Matthiessen and Bateman, 1991) assumes access to ideational, interpersonal and textual information which roughly corresponds to semantic, mood/voice, theme/rheme and focus/ground information ::: Similarly , KPML ( ) assumes access to ideational , interpersonal and textual information which roughly corresponds to semantic , mood/voice , theme/rheme and focus/ground information ::: RB , NP ( ) VVZ NN TO JJ , JJ CC JJ NN WDT RB VVZ TO JJ , NNS , NN CC NN NN ::: BackGround ::: 0.921786601848
473216 4231071 Carpenter, 1976; Griffin and Bock, 2000; Tenenhaus et al., 1995) and recent investigations on computational models for language acquisition and grounding (Siskind, 1995; Roy and Pentland, 2002; Yu and Ballard, 2004), we are particularly interested in two unique questions related to multimodal conversational systems: (1) In a multimodal conversation that involves more complex tasks (e.g., both user initiated tasks and system initiated ::: Motivated by psycholinguistic studies ( ) and recent investigations on computational models for language acquisition and grounding ( ) , we are particularly interested in two unique questions related to multimodal conversational systems: (1) In a multimodal conversation that involves more complex tasks (e.g. , both user initiated tasks and system initiated tasks) , is there a reliable temporal alignment between eye gaze and spoken references so that the coupled inputs can be used for automated vocabulary acquisition and interpretation? (2) If such an alignment exists , how can we model this alignment and automatically acquire and interpret the vocabularies? To address the first question , we conducted an empirical study to examine the temporal relationships between eye fixations and their corresponding spoken references ::: VVN IN JJ NNS ( ) CC JJ NNS IN JJ NNS IN NN NN CC NN ( ) , PP VBP RB JJ IN CD JJ NNS VVN TO JJ JJ NN NN IN DT JJ NN WDT VVZ JJR JJ NNS JJ , DT NN VVD NNS CC NN VVN NN , VBZ RB DT JJ JJ NN IN NN VVP CC VVN NNS RB IN/that DT VVN NNS MD VB VVN IN JJ NN NN CC JJ NN IN PDT DT NN VVZ , WRB MD PP VV DT NN CC RB VV CC VV DT NN TO VV DT JJ NN , PP VVD DT JJ NN TO VV DT JJ NNS IN NN NNS CC PP$ JJ VVN NNS ::: Fundamental ::: 0.669198092463
473216 4231071 Recent studies have shown that multisensory information (e.g., through vision and language processing) can be combined to effectively acquire words to their perceptually grounded objects in the environment (Siskind, 1995; Roy and Pentland, 2002; Yu and Ballard, 2004) ::: Recent studies have shown that multisensory information (e.g. , through vision and language processing) can be combined to effectively acquire words to their perceptually grounded objects in the environment ( ) ::: JJ NNS VHP VVN IN/that JJ NN NN , IN NN CC NN NN MD VB VVN TO RB VV NNS TO PP$ RB VVN NNS IN DT NN ( ) ::: BackGround ::: 0.846710709555
2182013 5658172 Many other works manually develop a set of heuristic features devised with some specific relationship in mind, like a WordNet-based meronymy feature (Bedmar et al., 2007) or size-of feature (Aramaki et al., 2006) ::: Many other works manually develop a set of heuristic features devised with some specific relationship in mind , like a WordNet-based meronymy feature ( ) or size-of feature ( ) ::: JJ JJ NNS RB VVP DT NN IN JJ NNS VVN IN DT JJ NN IN NN , IN DT JJ NN NN ( ) CC JJ NN ( ) ::: BackGround ::: 0.837930581596
2182013 5658172 Since (Hearst, 1992), numerous works have used patterns for discovery and identification of instances of semantic relationships (e.g., (Girju et al., 2006; Snow et al., 2006; Banko et al, 2007)) ::: Since ( ) , numerous works have used patterns for discovery and identification of instances of semantic relationships (e.g. , ( )) ::: IN ( ) , JJ NNS VHP VVN NNS IN NN CC NN IN NNS IN JJ NNS NN , ( NN ::: BackGround ::: 0.622699849077
2383413 4231071 they co-occur within the time window . This situation is very similar to the training process of translation models in statistical machine translation (Brown et al., 1993), where parallel corpus is used to find the mappings between words from different languages by exploiting their co-occurrence patterns ::: This situation is very similar to the training process of translation models in statistical machine translation ( ) , where parallel corpus is used to find the mappings between words from different languages by exploiting their co-occurrence patterns ::: DT NN VBZ RB JJ TO DT NN NN IN NN NNS IN JJ NN NN ( ) , WRB JJ NN VBZ VVN TO VV DT NNS IN NNS IN JJ NNS IN VVG PP$ NN NNS ::: Fundamental ::: 0.892691182267
2383413 4231071 This optimization problem can be solved by the EM algorithm (Brown et al., 1993) ::: J^Pr(Wj |o k) = 1 , Vk j =1 This optimization problem can be solved by the EM algorithm ( ) ::: NP NP NP SYM CD , NP NN NN DT NN NN MD VB VVN IN DT JJ NN ( ) ::: BackGround ::: 0.54554472559
141312 5616851 Although dependency relations have been widely used in automatic acquisition of lexical information, such as detection of polysemy (Lin, 1998) and WSD (McCarthy et al., 2004), their utility in AVC still remains untested ::: Although dependency relations have been widely used in automatic acquisition of lexical information , such as detection of polysemy ( ) and WSD ( ) , their utility in AVC still remains untested ::: IN NN NNS VHP VBN RB VVN IN JJ NN IN JJ NN , JJ IN NN IN NN ( ) CC NP ( ) , PP$ NN IN NP RB VVZ JJ ::: BackGround ::: 0.891555828242
141312 4231023 Our approach is analogous to previous work in extracting collocations from large text corpora using syntactic information (Lin, 1998) ::: Our approach is analogous to previous work in extracting collocations from large text corpora using syntactic information ( ) ::: PP$ NN VBZ JJ TO JJ NN IN VVG NNS IN JJ NN NNS VVG JJ NN ( ) ::: Fundamental ::: 0.899735410842
141312 4231023 This observation further supports the distributional hypothesis of word similarity and corresponding technologies for identifying synonyms by similarity of lexical-syntactic context (Lin, 1998) ::: This observation further supports the distributional hypothesis of word similarity and corresponding technologies for identifying synonyms by similarity of lexical-syntactic context ( ) ::: DT NN RBR VVZ DT JJ NN IN NN NN CC JJ NNS IN VVG NNS IN NN IN JJ NN ( ) ::: BackGround ::: 0.930949336251
309114 5658172 A leading method for utilizing context information for classification and extraction of relationships is that of patterns (Hearst, 1992; Pantel and Pennacchiotti, 2006) ::: A leading method for utilizing context information for classification and extraction of relationships is that of patterns ( ) ::: DT VVG NN IN VVG NN NN IN NN CC NN IN NNS VBZ IN/that IN NNS ( ) ::: BackGround ::: 0.862247981837
309114 5658172 Since (Hearst, 1992), numerous works have used patterns for discovery and identification of instances of semantic relationships (e.g., (Girju et al., 2006; Snow et al., 2006; Banko et al, 2007)) ::: Since ( ) , numerous works have used patterns for discovery and identification of instances of semantic relationships (e.g. , ( )) ::: IN ( ) , JJ NNS VHP VVN NNS IN NN CC NN IN NNS IN JJ NNS NN , ( NN ::: BackGround ::: 0.622699849077
309114 5013555 For example, we can use automatically extracted hyponymy relations (Hearst, 1992; Shinzato and Torisawa, 2004), or automatically induced MN clusters (Rooth et al., 1999; Torisawa, 2001) ::: For example , we can use automatically extracted hyponymy relations ( ) , or automatically induced MN clusters ( ) ::: IN NN , PP MD VV RB VVN JJ NNS ( ) , CC RB VVN JJ NNS ( ) ::: BackGround ::: 0.707106781187
845360 5126930 By combining word alignments in two directions using heuristics (Och and Ney, 2003), a single set of static word alignments is then formed ::: By combining word alignments in two directions using heuristics ( ) , a single set of static word alignments is then formed ::: IN VVG NN NNS IN CD NNS VVG NNS ( ) , DT JJ NN IN JJ NN NNS VBZ RB VVN ::: Fundamental ::: 0.890563556562
845360 5126930 Two different word alignment models are trained as the baseline, one is symmetric HMM word alignment model, the other is IBM Model-4 as implemented in the GIZA++ toolkit (Och and Ney, 2003) ::: Two different word alignment models are trained as the baseline , one is symmetric HMM word alignment model , the other is IBM Model-4 as implemented in the GIZA++ toolkit ( ) ::: CD JJ NN NN NNS VBP VVN IN DT NN , PP VBZ JJ NP NN NN NN , DT JJ VBZ NP NP RB VVD IN DT NP NN ( ) ::: Fundamental ::: 0.936776932043
845360 4231112 By combining word alignments in two directions using heuristics (Och and Ney, 2003), a single set of static word alignments is then formed ::: By combining word alignments in two directions using heuristics ( ) , a single set of static word alignments is then formed ::: IN VVG NN NNS IN CD NNS VVG NNS ( ) , DT JJ NN IN JJ NN NNS VBZ RB VVN ::: Fundamental ::: 0.890563556562
845360 4231112 We simply modify the GIZA++ toolkit (Och and Ney, 2003) by always weighting lexicon probabilities with soft constraints during iterative model training, and obtain 0.7% TER reduction on both sets and 0.4% BLEU improvement on the test set ::: We simply modify the GIZA++ toolkit ( ) by always weighting lexicon probabilities with soft constraints during iterative model training , and obtain 0.7% TER reduction on both sets and 0.4% BLEU improvement on the test set ::: PP RB VV DT NP NN ( ) IN RB NN NN NNS IN JJ NNS IN JJ NN NN , CC VV CD NN NN IN DT NNS CC CD NP NN IN DT NN NN ::: Fundamental ::: 0.943701430842
102893 4930784 Approaches include incorporating a subcategorization feature (Gildea  Jurafsky, 2002; Xue  Palmer, 2004), such as the one used in our baseline; and building a model which jointly classifies all arguments of a verb (Toutanova et al., 2005) ::: Approaches include incorporating a subcategorization feature ( ) , such as the one used in our baseline; and building a model which jointly classifies all arguments of a verb ( ) ::: NNS VVP VVG DT NN NN ( ) , JJ IN DT CD VVN IN PP$ NN CC VVG DT NN WDT RB VVZ DT NNS IN DT NN ( ) ::: BackGround ::: 0.852802865422
102893 4231023 Parse tree paths were used for semantic role labeling by Gildea and Jurafsky (2002) as descriptive features of the syntactic relationship between predicates and their arguments in the parse tree of a sentence ::: Parse tree paths were used for semantic role labeling by Gildea and Jurafsky ( ) as descriptive features of the syntactic relationship between predicates and their arguments in the parse tree of a sentence ::: JJ NN NNS VBD VVN IN JJ NN VVG IN NP CC NP ( ) IN JJ NNS IN DT JJ NN IN NNS CC PP$ NNS IN DT VVP NN IN DT NN ::: BackGround ::: 0.964485644341
102893 4231023 Our work suggests that feature generalization based on verb-similarity may compliment approaches to generalization based on role-similarity (Gildea and Jurafsky, 2002; Baldewein et al., 2004) ::: Our work suggests that feature generalization based on verb-similarity may compliment approaches to generalization based on role-similarity ( ) ::: PP$ NN VVZ IN/that NN NN VVN IN NN MD NN NNS TO NN VVN IN NN ( ) ::: BackGround ::: 0.854242196177
2145853 4231023 A recent release of the PropBank (Palmer et al., 2005) corpus of semantic role annotations of Treebank parses contained 112,917 labeled instances of 4,250 rolesets corresponding to 3,257 verbs, as illustrated by this example for the verb buy ::: A recent release of the PropBank ( ) corpus of semantic role annotations of Tree-bank parses contained 112 ,917 labeled instances of 4 ,250 rolesets corresponding to 3 ,257 verbs , as illustrated by this example for the verb buy ::: DT JJ NN IN DT NP ( ) NN IN JJ NN NNS IN NP VVZ VVN CD CD VVN NNS IN CD CD NNS JJ TO CD CD NNS , RB VVN IN DT NN IN DT NN NN ::: BackGround ::: 0.918397947963
2145853 4231174 This is known as multi-task learning in the machine learning literature (Caruana, 1997; Ben-David and Schuller, 2003; Evgeniou and Pontil, 2004; Micchelli and Pontil, 2005; Maurer, 2006) ::: This is known as multi-task learning in the machine learning literature ( ) ::: DT VBZ VVN IN NN VVG IN DT NN VVG NN ( ) ::: BackGround ::: 0.584348709791
2508377 4231174 In this paper, we apply Alternating Structure Optimization (ASO) (Ando and Zhang, 2005a) to the semantic role labeling task on NomBank ::: In this paper , we apply Alternating Structure Optimization (ASO) ( ) to the semantic role labeling task on NomBank ::: IN DT NN , PP VVP NP NP NP NN ( ) TO DT JJ NN VVG NN IN NP ::: Fundamental ::: 0.859726953621
2508377 4231174 ASO has been shown to be effective on the following natural language processing tasks: text categorization, named entity recognition, part-of-speech tagging, and word sense disambiguation (Ando and Zhang, 2005a; Ando and Zhang, 2005b; Ando, 2006) ::: ASO has been shown to be effective on the following natural language processing tasks: text categorization , named entity recognition , part-of-speech tagging , and word sense disambiguation ( ) ::: NP VHZ VBN VVN TO VB JJ IN DT VVG JJ NN VVG JJ NN NN , VVN NN NN , NN VVG , CC NN NN NN ( ) ::: BackGround ::: 0.766616532861
2508377 4231174 For a more complete description, see (Ando and Zhang, 2005a) ::: For a more complete description , see ( ) ::: IN DT RBR JJ NN , VVP ( ) ::: BackGround ::: 0.707106781187
2508377 4231174 An important observation in (Ando and Zhang, 2005a) is that the binary classification problems used to derive are not necessarily those problems we are aiming to solve ::: This relationship is modeled by + 6 Tvi (3) The parameters [{w l , vi} , 6] may then be found by joint empirical risk minimization over all the m problems , i.e. , their values should minimize the combined empirical risk: l=i  V   i=l J (4) An important observation in ( ) is that the binary classification problems used to derive 6 are not necessarily those problems we are aiming to solve ::: DT NN VBZ VVN RB SYM CD NP NN DT NNS NN NN , JJ , JJ MD RB VB VVN IN JJ JJ NN NN IN PDT DT NN NNS , FW , PP$ NNS MD VV DT JJ JJ NN NN NN NN NP NN DT JJ NN IN ( ) VBZ IN/that DT JJ NN NNS VVN TO VV CD VBP RB RB DT NNS PP VBP VVG TO VV ::: BackGround ::: 0.545485744593
2508377 4231174 Assuming there are k target problems and m auxiliary problems, it is shown in (Ando and Zhang, 2005a) that by performing one round of minimization, an approximate solution of can be obtained from (4) by the following algorithm: ::: Assuming there are k target problems and m auxiliary problems , it is shown in ( ) that by performing one round of minimization , an approximate solution of 6 can be obtained from (4) by the following algorithm: 1. For each of the m auxiliary problems , learn u l as described by (1) ::: VVG EX VBP NN NN NNS CC NN JJ NNS , PP VBZ VVN IN ( ) IN/that IN VVG CD NN IN NN , DT JJ NN IN CD MD VB VVN IN NN IN DT VVG NN NN DT IN DT NN JJ NNS , VV NN NN IN VVN IN NN ::: BackGround ::: 0.827832723166
2508377 4231174 This is a simplified version of the definition in (Ando and Zhang, 2005a), made possible because the same is used for all auxiliary problems ::: This is a simplified version of the definition in ( ) , made possible because the same A is used for all auxiliary problems ::: DT VBZ DT VVN NN IN DT NN IN ( ) , VVD JJ IN DT JJ NP VBZ VVN IN DT JJ NNS ::: BackGround ::: 0.876356092008
2508377 4231174 ASO has been demonstrated to be an effective semi-supervised learning algorithm (Ando and Zhang, 2005a; Ando and Zhang, 2005b; Ando, 2006) ::: ASO has been demonstrated to be an effective semi-supervised learning algorithm ( ) ::: NP VHZ VBN VVN TO VB DT JJ JJ NN NN ( ) ::: BackGround ::: 0.594088525786
2508377 4231174 A variety of auxiliary problems are tested in (Ando and Zhang, 2005a; Ando and Zhang, 2005b) in the semi-supervised settings, i.e., their auxiliary problems are generated from unlabeled data ::: A variety of auxiliary problems are tested in ( ) in the semi-supervised settings , i.e. , their auxiliary problems are generated from unlabeled data ::: DT NN IN JJ NNS VBP VVN IN ( ) IN DT JJ NNS , FW , PP$ JJ NNS VBP VVN IN JJ NNS ::: BackGround ::: 0.812141901446
144147 4231117 Since our learner treats all values as nominal, we discretized the values of dep and len with a C4.5 classier (Kohavi  Sahami, 1996) ::: Since our learner treats all values as nominal , we discretized the values of dep and len with a C4.5 classifier ( ) ::: IN PP$ NN VVZ DT NNS IN JJ , PP VVD DT NNS IN NNS CC NNS IN DT NP NN ( ) ::: Fundamental ::: 0.866921446863
2370785 5616851 Although there exist several manually-created verb lexicons or ontologies, including Levinâ€™s verb taxonomy, VerbNet, and FrameNet, automatic verb classification (AVC) is still necessary for extending existing lexicons (Korhonen and Briscoe, 2004), building and tuning lexical information specific to different domains (Korhonen et al., 2006), and bootstrapping verb lexicons for new languages (Tsang et al., 2002) ::: Although there exist several manually-created verb lexicons or ontologies , including Levin's verb taxonomy , VerbNet , and FrameNet , automatic verb classification (AVC) is still necessary for extending existing lexicons ( ) , building and tuning lexical information specific to different domains ( ) , and bootstrapping verb lexicons for new languages ( ) ::: IN EX VV JJ JJ NN NNS CC NNS , VVG NP NN NN , NP , CC NP , JJ NN NN NN VBZ RB JJ IN VVG JJ NNS ( ) , VVG CC VVG JJ NN JJ TO JJ NNS ( ) , CC VVG NN NNS IN JJ NNS ( ) ::: BackGround ::: 0.891695712069
2034028 4905037 The traditional text-only language models (which are also used below as baseline comparisons) are generated with the SRI language modeling toolkit (Stolcke, 2002) using Chen and Goodman's modified Kneser-Ney discounting and interpolation (Chen and Goodman, 1998) ::: The traditional text-only language models (which are also used below as baseline comparisons) are generated with the SRI language modeling toolkit ( ) using Chen and Goodman's modified Kneser-Ney discounting and interpolation ( ) ::: DT JJ JJ NN NNS NN VBP RB VVN IN IN NN NN VBP VVN IN DT NP NN NN NN ( ) VVG NP CC NP JJ NP NN CC NN ( ) ::: Fundamental ::: 0.928442061738
2034028 4231112 The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using all English sentences in the parallel training data ::: The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing ( ) using all English sentences in the parallel training data ::: DT NN NN VBZ DT JJ NN NN VVN IN NP NP VVG ( ) VVG DT JJ NNS IN DT JJ NN NNS ::: Fundamental ::: 0.898026510134
2394009 4930784 A relatively straight-forward extension of the inside-outside algorithm for chart-parses allows us to learn and perform inference in our compact representation (a similar algorithm is presented in (Geman  Johnson, 2002)) ::: A relatively straight-forward extension of the inside-outside algorithm for chart-parses allows us to learn and perform inference in our compact representation (a similar algorithm is presented in ( )) ::: DT RB JJ NN IN DT NN NN IN NNS VVZ PP TO VV CC VV NN IN PP$ JJ NN NN JJ NN VBZ VVN IN ( NN ::: Fundamental ::: 0.933699561848
31 5274910 Representing feature spaces with this kind of tree, besides often coinciding with the explicit language used by common natural language toolkits (Cohen, 2004), has the added benefit of allowing a model to easily back-off, or smooth, to decreasing levels of specificity ::: Representing feature spaces with this kind oftree , besides often coinciding with the explicit language used by common natural language toolkits ( ) , has the added benefit of allowing a model to easily back-off , or smooth , to decreasing levels of specificity ::: VVG NN NNS IN DT NN NN , IN RB VVG IN DT JJ NN VVN IN JJ JJ NN NNS ( ) , VHZ DT JJ NN IN VVG DT NN TO RB NN , CC JJ , TO VVG NNS IN NN ::: BackGround ::: 0.937903091855
31 5274910 We used a standard natural language toolkit (Cohen, 2004) to compute tens of thousands of binary features on each of these tokens, encoding such information as capitalization patterns and contextual information from surrounding words ::: We used a standard natural language toolkit ( ) to compute tens of thousands of binary features on each of these tokens , encoding such information as capitalization patterns and contextual information from surrounding words ::: PP VVD DT JJ JJ NN NN ( ) TO VV NNS IN NNS IN JJ NNS IN DT IN DT NNS , VVG JJ NN IN NN NNS CC JJ NN IN VVG NNS ::: Fundamental ::: 0.953462589246
648944 4231125 A Feature-based TAG (FTAG, (Vijay-Shanker and Joshi, 1988)) consists of a set of (auxiliary or initial) elementary trees and of two tree composition operations: substitution and adjunction ::: A Feature-based TAG (FTAG , ( )) consists of a set of (auxiliary or initial) elementary trees and of two tree composition operations: substitution and ad-junction ::: DT JJ NP NN , ( NN VVZ IN DT NN IN JJ CC JJ JJ NNS CC IN CD NN NN NN NN CC NN ::: BackGround ::: 0.876037590783
2414885 4231117 Finally, the articles are parsed with the CDG dependency parser (Foth  Menzel, 2006) ::: Finally , the articles are parsed with the CDG dependency parser ( ) ::: RB , DT NNS VBP VVN IN DT NP NN NN ( ) ::: Fundamental ::: 0.840168050417
4993738 5658172 Freely available tools like Weka (Witten and Frank, 1999) allow easy experimentation with common learning algorithms (Hendrickx et al., 2007) ::: Freely available tools like Weka ( ) allow easy experimentation with common learning algorithms ( ) ::: RB JJ NNS IN NP ( ) VV JJ NN IN JJ NN NNS ( ) ::: BackGround ::: 0.738548945876
2145504 4231115 First, we use the standard approach of greedily assigning each of the learned classes to the POS tag with which it has the greatest overlap, and then computing tagging accuracy (Smith and Eisner, 2005; Haghighi and Klein, 2006).8 Additionally, we ::: First , we use the standard approach of greedily assigning each of the learned classes to the POS tag with which it has the greatest overlap , and then computing tagging accuracy ( ) ::: RB , PP VVP DT JJ NN IN RB VVG DT IN DT VVN NNS TO DT NP NN IN WDT PP VHZ DT JJS VVP , CC RB VVG VVG NN ( ) ::: Fundamental ::: 0.878850170608
4639227 4231096 In addition to these basic types, several studies deal with the discovery and labeling of more specific relation sub-types, including inter-verb relations (Chklovski and Pantel, 2004) and nouncompound relationships (Moldovan et al, 2004) ::: In addition to these basic types , several studies deal with the discovery and labeling of more specific relation sub-types , including inter-verb relations ( ) and noun-compound relationships ( ) ::: IN NN TO DT JJ NNS , JJ NNS NN IN DT NN CC VVG IN RBR JJ NN NNS , VVG NN NNS ( ) CC JJ NNS ( ) ::: BackGround ::: 0.825136997007
4639227 4231096 It should be noted that some of these papers utilize language and domain-dependent preprocessing including syntactic parsing (Suchanek et al, 2006) and named entity tagging (Hasegawa et al, 2004), while others take advantage of handcrafted databases such as WordNet (Moldovan et al, 2004; Costello et al, 2006) and Wikipedia (Strube and Ponzetto, 2006) ::: It should be noted that some of these papers utilize language and domain-dependent preprocessing including syntactic parsing ( ) and named entity tagging ( ) , while others take advantage of handcrafted databases such as WordNet ( ) and Wikipedia ( ) ::: PP MD VB VVN IN/that DT IN DT NNS VV NN CC JJ NN VVG JJ VVG ( ) CC VVN NN VVG ( ) , IN NNS VVP NN IN VVN NNS JJ IN NP ( ) CC NP ( ) ::: BackGround ::: 0.687523872771
2385406 4231096 In addition to these basic types, several studies deal with the discovery and labeling of more specific relation sub-types, including inter-verb relations (Chklovski and Pantel, 2004) and nouncompound relationships (Moldovan et al, 2004) ::: In addition to these basic types , several studies deal with the discovery and labeling of more specific relation sub-types , including inter-verb relations ( ) and noun-compound relationships ( ) ::: IN NN TO DT JJ NNS , JJ NNS NN IN DT NN CC VVG IN RBR JJ NN NNS , VVG NN NNS ( ) CC JJ NNS ( ) ::: BackGround ::: 0.825136997007
845318 4231115 For both experiments, we used dependency trees extracted from the Penn Treebank (Marcus et al., 1993) using the head rules and dependency extractor from Yamada and Matsumoto (2003) ::: For both experiments , we used dependency trees extracted from the Penn Treebank ( ) using the head rules and dependency extractor from Yamada and Matsumoto ( ) ::: IN DT NNS , PP VVD NN NNS VVN IN DT NP NP ( ) VVG DT NN NNS CC NN NN IN NP CC NP ( ) ::: Fundamental ::: 0.90321064746
503349 6340884 Perhaps the best known are Query by Committee (QBC) (Seung et al., 1992) and uncertainty sampling (or Query by Uncertainty, QBU) (Thrun and Moeller, 1992) ::: Perhaps the best known are Query by Committee (QBC) ( ) and uncertainty sampling (or Query by Uncertainty, QBU) ( ) ::: RB DT RBS JJ VBP NP IN NP NP ( ) CC NN NN NN NP IN NN , NP ( ) ::: BackGround ::: 0.81148222492
1270402 4231117 The TnT tagger (Brants, 2000) and the TreeTagger (Schmid, 1997) are used for tagging and lemmatization ::: The TnT tagger ( ) and the TreeTagger ( ) are used for tagging and lemmatization ::: DT NP NN ( ) CC DT NP ( ) VBP VVN IN VVG CC NN ::: Fundamental ::: 0.836660026534
4683321 4231125 To associate semantic representations with natural language expressions, the FTAG is modified as proposed in (Gardent and Kallmeyer, 2003) ::: To associate semantic representations with natural language expressions , the FTAG is modified as proposed in ( ) ::: TO NN JJ NNS IN JJ NN NNS , DT NP VBZ VVN IN VVN IN ( ) ::: Fundamental ::: 0.845154254729
4683321 4231125 The proposal draws on ideas from (Koller and Striegnitz, 2002; Gardent and Kow, 2005) and aims to determine whether for a given input (a set of TAG elementary trees whose semantics equate the input semantics), syntactic requirements and resources cancel out ::: The proposal draws on ideas from ( ) and aims to determine whether for a given input (a set of TAG elementary trees whose semantics equate the input semantics) , syntactic requirements and resources cancel out ::: DT NN VVZ IN NNS IN ( ) CC VVZ TO VV IN IN DT VVN NN NN VVN IN NP JJ NNS WP$ NNS VV DT NN NN , JJ NNS CC NNS VV RP ::: Fundamental ::: 0.899820053982
179215 4231112 LSA has been successfully applied to information retrieval (Deerwester et al., 1990), statistical langauge modeling (Bellegarda, 2000) and etc ::: LSA has been successfully applied to information retrieval ( ) , statistical langauge modeling ( ) and etc ::: NP VHZ VBN RB VVN TO NN NN ( ) , JJ NN NN ( ) CC FW ::: BackGround ::: 0.786795792469
179215 4231112 Alternative constructions of the matrix are possible using raw counts or TF-IDF (Deerwester et al., 1990) ::: Alternative constructions of the matrix are possible using raw counts or TF-IDF ( ) ::: JJ NNS IN DT NN VBP RB VVG JJ NNS CC NP ( ) ::: BackGround ::: 0.827170191869
514717 4231077 Hiero (Chiang, 2005) is a hierarchical phrase-based model for statistical machine translation, based on weighted synchronous context-free grammar (CFG) (Lewis and Stearns, 1968) ::: Hiero ( ) is a hierarchical phrase-based model for statistical machine translation , based on weighted synchronous context-free grammar (CFG) ( ) ::: NP ( ) VBZ DT JJ JJ NN IN JJ NN NN , VVN IN JJ JJ JJ NN NN ( ) ::: BackGround ::: 0.850962943397
4416974 5452627 Other than for email summarization, other document summarization methods have adopted graphranking algorithms for summarization, e.g., (Wan et al., 2007), (Mihalcea and Tarau, 2004) and (Erkan and Radev, 2004) ::: Other than for email summarization , other document summarization methods have adopted graphranking algorithms for summarization , e.g. , ( ) , ( ) and ( ) ::: JJ IN IN NN NN , JJ NN NN NNS VHP VVN JJ NNS IN NN , FW , ( ) , ( ) CC ( ) ::: BackGround ::: 0.792593923901
166772 4231183 The DSO corpus (Ng and Lee, 1996) contains 192,800 annotated examples for 121 nouns and 70 verbs, drawn from BC and WSJ ::: The DSO corpus ( ) contains 192 ,800 annotated examples for 121 nouns and 70 verbs , drawn from BC and WSJ ::: DT NP NN ( ) VVZ CD CD VVN NNS IN CD NNS CC CD NNS , VVN IN NP CC NP ::: BackGround ::: 0.90144107192
38081 4231023 In future work, it would be particularly interesting to compare empiricallyderived verb clusters to verb classes derived from theoretical considerations (Levin, 1993), and to the automated verb classification techniques that use these classes (Joanis and Stevenson, 2003) ::: In future work , it would be particularly interesting to compare empirically-derived verb clusters to verb classes derived from theoretical considerations ( ) , and to the automated verb classification techniques that use these classes ( ) ::: IN JJ NN , PP MD VB RB JJ TO VV JJ NN NNS TO NN NNS VVN IN JJ NNS ( ) , CC TO DT JJ NN NN NNS WDT VVP DT NNS ( ) ::: BackGround ::: 0.895273785786
1468702 4231112 LSA has been successfully applied to information retrieval (Deerwester et al., 1990), statistical langauge modeling (Bellegarda, 2000) and etc ::: LSA has been successfully applied to information retrieval ( ) , statistical langauge modeling ( ) and etc ::: NP VHZ VBN RB VVN TO NN NN ( ) , JJ NN NN ( ) CC FW ::: BackGround ::: 0.786795792469
1468702 4231112 Following the notation in section 2.1, the ij-th entry of the matrix W is defined as in (Bellegarda, 2000) ::: Following the notation in section 2.1 , the ij-th entry of the matrix W is defined as in ( ) Wj = (1 - e Wz ) where Cj is the total number of words in the j-th sentence pair ::: VVG DT NN IN NN CD , DT NN NN IN DT NN NP VBZ VVN IN IN ( ) NP SYM NN : SYM NP ) WRB NP VBZ DT JJ NN IN NNS IN DT JJ NN NN ::: Fundamental ::: 0.831293884823
216871 4231077 First, we performed word alignment on the FBIS parallel corpus using GIZA++ (Och and Ney, 2000) in both directions ::: First , we performed word alignment on the FBIS parallel corpus using GIZA++ ( ) in both directions ::: RB , PP VVD NN NN IN DT NP JJ NN VVG NP ( ) IN DT NNS ::: Fundamental ::: 0.845154254729
1865646 5529044 the notion that all movement occurring during translation can be explained by permuting children in a parse tree (Fox, 2002) ::: Syntactic cohesion 1 is the notion that all movement occurring during translation can be explained by permuting children in a parse tree ( ) ::: JJ NN CD VBZ DT NN IN/that DT NN VVG IN NN MD VB VVN IN VVG NNS IN DT VVP NN ( ) ::: BackGround ::: 0.818181818182
1865646 5529044 Fox (2002) showed that cohesion is held in the vast majority of cases for English-French, while Cherry and Lin (2006) have shown it to be a strong feature for word alignment ::: Fox ( ) showed that cohesion is held in the vast majority of cases for English-French , while Cherry and Lin ( ) have shown it to be a strong feature for word alignment ::: NP ( ) VVD IN/that NN VBZ VVN IN DT JJ NN IN NNS IN NN , IN NP CC NP ( ) VHP VVN PP TO VB DT JJ NN IN NN NN ::: BackGround ::: 0.942809041582
1865646 5529044 Previous approaches to measuring the cohesion of a sentence pair have worked with a word alignment (Fox, 2002; Lin and Cherry, 2003) ::: Previous approaches to measuring the cohesion of a sentence pair have worked with a word alignment ( ) ::: JJ NNS TO VVG DT NN IN DT NN NN VHP VVN IN DT NN NN ( ) ::: BackGround ::: 0.832050294338
1865646 5529044 Fox (2002) demonstrated and counted cases where cohesion was not maintained in handaligned sentence-pairs, while Cherry and Lin (2006) ::: Fox ( ) demonstrated and counted cases where cohesion was not maintained in hand-aligned sentence-pairs , while Cherry and Lin ( ) showed that a soft cohesion constraint is superior to a hard constraint for word alignment ::: NP ( ) VVN CC VVN NNS WRB NN VBD RB VVN IN JJ NNS , IN NP CC NP ( ) VVD IN/that DT JJ NN NN VBZ JJ TO DT JJ NN IN NN NN ::: BackGround ::: 0.629940788349
4231148 4930784 Another area of related work in the semantic role labeling literature is that on tree kernels (Moschitti, 2004; Zhang et al., 2007) ::: Another area of related work in the semantic role labeling literature is that on tree kernels ( ) ::: DT NN IN JJ NN IN DT JJ NN VVG NN VBZ IN/that IN NN NNS ( ) ::: BackGround ::: 0.816496580928
2159636 4905037 We follow previous work in sports video processing (Gong et al., 2004) and define an event in a baseball video as any sequence of shots starting with a pitching-scene and continuing for four subsequent shots ::: We follow previous work in sports video processing ( ) and define an event in a baseball video as any sequence of shots starting with a pitching-scene and continuing for four subsequent shots ::: PP VVP JJ NN IN NNS JJ NN ( ) CC VV DT NN IN DT NN NN IN DT NN IN NNS VVG IN DT NN CC VVG IN CD JJ NNS ::: Fundamental ::: 0.935414346693
39303757 5590651 The other two English dictionary transcriptions were produced in a similar way from the Buckeye corpus (Pitt et al., 2005; Pitt et al., 2007) and Mississippi Stateâ€™s corrected version of the LDCâ€™s Switchboard transcripts (Godfrey and Holliman, 1994; Deshmukh et al., 1998) ::: The other two English dictionary transcriptions were produced in a similar way from the Buckeye corpus ( ) and Mississippi State's corrected version of the LDC's Switchboard transcripts ( ) ::: DT JJ CD JJ NN NNS VBD VVN IN DT JJ NN IN DT NP NN ( ) CC NP NP VVD NN IN DT NP NP NNS ( ) ::: Fundamental ::: 0.718070330817
1823 4860637 The idea of selectively sampling training samples has been wildly discussed in machine learning theory (Seung et al., 1992) and has been applied successfully to several NLP applications (McCallum and Nigam, 1998) ::: The idea of selectively sampling training samples has been wildly discussed in machine learning theory ( ) and has been applied successfully to several NLP applications ( ) ::: DT NN IN RB VVG NN NNS VHZ VBN RB VVN IN NN VVG NN ( ) CC VHZ VBN VVN RB TO JJ NP NNS ( ) ::: BackGround ::: 0.866540741767
2446635 5658172 Since (Hearst, 1992), numerous works have used patterns for discovery and identification of instances of semantic relationships (e.g., (Girju et al., 2006; Snow et al., 2006; Banko et al, 2007)) ::: Since ( ) , numerous works have used patterns for discovery and identification of instances of semantic relationships (e.g. , ( )) ::: IN ( ) , JJ NNS VHP VVN NNS IN NN CC NN IN NNS IN JJ NNS NN , ( NN ::: BackGround ::: 0.622699849077
3727296 4231023 Annotations similar to these have been used to create automated semantic role labeling systems (Pradhan et al., 2005; Moschitti et al., 2006) for use in natural language processing applications that require only shallow semantic parsing ::: Annotations similar to these have been used to create automated semantic role labeling systems ( ) for use in natural language processing applications that require only shallow semantic parsing ::: NNS JJ TO DT VHP VBN VVN TO VV JJ JJ NN VVG NNS ( ) IN NN IN JJ NN NN NNS WDT VVP RB JJ JJ VVG ::: BackGround ::: 0.829993306533
3727296 4231023 The overall performance of our semantic role labeling approach is not competitive with leading contemporary systems, which typically employ support vector machine learning algorithms with syntactic features (Pradhan et al., 2005) or syntactic tree kernels (Moschitti et al., 2006) ::: The overall performance of our semantic role labeling approach is not competitive with leading contemporary systems , which typically employ support vector machine learning algorithms with syntactic features ( ) or syntactic tree kernels ( ) ::: DT JJ NN IN PP$ JJ NN VVG NN VBZ RB JJ IN VVG JJ NNS , WDT RB VVP NN NN NN VVG NNS IN JJ NNS ( ) CC JJ NN NNS ( ) ::: Compare ::: 0.845154254729
385869 4231115 For both experiments, we used dependency trees extracted from the Penn Treebank (Marcus et al., 1993) using the head rules and dependency extractor from Yamada and Matsumoto (2003) ::: For both experiments , we used dependency trees extracted from the Penn Treebank ( ) using the head rules and dependency extractor from Yamada and Matsumoto ( ) ::: IN DT NNS , PP VVD NN NNS VVN IN DT NP NP ( ) VVG DT NN NNS CC NN NN IN NP CC NP ( ) ::: Fundamental ::: 0.90321064746
2414989 4231057 For example, incremental CFG parsing algorithms can be used with the CFGs produced by this transform, as can the Inside-Outside estimation algorithm (Lari and Young, 1990) and more exotic methods such as estimating adjoined hidden states (Matsuzaki et al., 2005; Petrov et al., 2006) ::: For example , incremental CFG parsing algorithms can be used with the CFGs produced by this transform , as can the Inside-Outside estimation algorithm ( ) and more exotic methods such as estimating adjoined hidden states ( ) ::: IN NN , JJ NP VVG NNS MD VB VVN IN DT NNS VVN IN DT VV , RB MD DT NP NN NN ( ) CC JJR JJ NNS JJ IN VVG VVN JJ NNS ( ) ::: BackGround ::: 0.83387639845
2284234 5616851 The software performs the socalled 1-of-k classification (Madigan et al., 2005) ::: The software performs the so-called 1-of-k classification ( ) ::: DT NN VVZ DT JJ NN NN ( ) ::: Fundamental ::: 0.653197264742
2409686 4231174 Noun predicates also appear in FrameNet semantic role labeling (Gildea and Jurafsky, 2002), and many FrameNet SRL systems are evaluated in Senseval-3 (Litkowski, 2004) ::: Noun predicates also appear in FrameNet semantic role labeling ( ) , and many FrameNet SRL systems are evaluated in Senseval-3 ( ) ::: NP VVZ RB VV IN NP JJ NN VVG ( ) , CC JJ NP NP NNS VBP VVN IN NP ( ) ::: BackGround ::: 0.871144810369
2374026 4930784 Approaches include incorporating a subcategorization feature (Gildea  Jurafsky, 2002; Xue  Palmer, 2004), such as the one used in our baseline; and building a model which jointly classifies all arguments of a verb (Toutanova et al., 2005) ::: Approaches include incorporating a subcategorization feature ( ) , such as the one used in our baseline; and building a model which jointly classifies all arguments of a verb ( ) ::: NNS VVP VVG DT NN NN ( ) , JJ IN DT CD VVN IN PP$ NN CC VVG DT NN WDT RB VVZ DT NNS IN DT NN ( ) ::: BackGround ::: 0.852802865422
2922658 4905037 The WEKA machine learning package is used to train a boosted decision tree to classify these frames into one of three categories: pitching-scene, field-scene, other (Witten and Frank, 2005) . Those shots whose key frames are classified as field-scenes are then subcategorized (using boosted decision trees) into one of the following categories: infield, outfield, wall, base, running, and misc ::: The WEKA machine learning package is used to train a boosted decision tree to classify these frames into one of three categories: pitching-scene , field-scene , other ( ) ::: DT NP NN VVG NN VBZ VVN TO VV DT VVN NN NN TO VV DT NNS IN CD IN CD NN NN , NN , JJ ( ) ::: Fundamental ::: 0.750972672942
1766544 4231096 Studying relationships between tagged named entities, (Hasegawa et al, 2004; Hassan et al, 2006) proposed unsupervised clustering methods that assign given (or semi-automatically extracted) sets of pairs into several clusters, where each cluster corresponds to one of a known relationship type ::: Studying relationships between tagged named entities , ( ) proposed unsupervised clustering methods that assign given (or semi-automatically extracted) sets of pairs into several clusters , where each cluster corresponds to one of a known relationship type ::: VVG NNS IN VVN VVN NNS , ( ) VVN JJ VVG NNS WDT VVP VVN NN RB JJ NNS IN NNS IN JJ NNS , WRB DT NN VVZ TO CD IN DT VVN NN NN ::: BackGround ::: 0.848528137424
1766544 4231096 It should be noted that some of these papers utilize language and domain-dependent preprocessing including syntactic parsing (Suchanek et al, 2006) and named entity tagging (Hasegawa et al, 2004), while others take advantage of handcrafted databases such as WordNet (Moldovan et al, 2004; Costello et al, 2006) and Wikipedia (Strube and Ponzetto, 2006) ::: It should be noted that some of these papers utilize language and domain-dependent preprocessing including syntactic parsing ( ) and named entity tagging ( ) , while others take advantage of handcrafted databases such as WordNet ( ) and Wikipedia ( ) ::: PP MD VB VVN IN/that DT IN DT NNS VV NN CC JJ NN VVG JJ VVG ( ) CC VVN NN VVG ( ) , IN NNS VVP NN IN VVN NNS JJ IN NP ( ) CC NP ( ) ::: BackGround ::: 0.687523872771
2363036 4231174 ASO has been shown to be effective on the following natural language processing tasks: text categorization, named entity recognition, part-of-speech tagging, and word sense disambiguation (Ando and Zhang, 2005a; Ando and Zhang, 2005b; Ando, 2006) ::: ASO has been shown to be effective on the following natural language processing tasks: text categorization , named entity recognition , part-of-speech tagging , and word sense disambiguation ( ) ::: NP VHZ VBN VVN TO VB JJ IN DT VVG JJ NN VVG JJ NN NN , VVN NN NN , NN VVG , CC NN NN NN ( ) ::: BackGround ::: 0.766616532861
2363036 4231174 ASO has been demonstrated to be an effective semi-supervised learning algorithm (Ando and Zhang, 2005a; Ando and Zhang, 2005b; Ando, 2006) ::: ASO has been demonstrated to be an effective semi-supervised learning algorithm ( ) ::: NP VHZ VBN VVN TO VB DT JJ JJ NN NN ( ) ::: BackGround ::: 0.594088525786
2363036 4231174 A variety of auxiliary problems are tested in (Ando and Zhang, 2005a; Ando and Zhang, 2005b) in the semi-supervised settings, i.e., their auxiliary problems are generated from unlabeled data ::: A variety of auxiliary problems are tested in ( ) in the semi-supervised settings , i.e. , their auxiliary problems are generated from unlabeled data ::: DT NN IN JJ NNS VBP VVN IN ( ) IN DT JJ NNS , FW , PP$ JJ NNS VBP VVN IN JJ NNS ::: BackGround ::: 0.812141901446
110183 4231207 PDDL (McDermott, 2000) is the standard input language for modern planning systems ::: PDDL ( ) is the standard input language for modern planning systems ::: NP ( ) VBZ DT JJ NN NN IN JJ NN NNS ::: BackGround ::: 0.845154254729
368362 4231096 Most related work deals with discovery of hypernymy (Hearst, 1992; Pantel et al, 2004), synonymy (Roark and Charniak, 1998; Widdows and Dorow, 2002; Davidov and Rappoport, 2006) and meronymy (Berland and Charniak, 1999) ::: Most related work deals with discovery of hypernymy ( ) , synonymy ( ) and meronymy ( ) ::: RBS VVN NN NNS IN NN IN NN ( ) , NN ( ) CC JJ ( ) ::: BackGround ::: 0.599042297873
443543 4231096 Most related work deals with discovery of hypernymy (Hearst, 1992; Pantel et al, 2004), synonymy (Roark and Charniak, 1998; Widdows and Dorow, 2002; Davidov and Rappoport, 2006) and meronymy (Berland and Charniak, 1999) ::: Most related work deals with discovery of hypernymy ( ) , synonymy ( ) and meronymy ( ) ::: RBS VVN NN NNS IN NN IN NN ( ) , NN ( ) CC JJ ( ) ::: BackGround ::: 0.599042297873
1968466 4231125 Reversible realisers. The realiser presented here differs in mainly two ways from existing reversible realisers such as (White, 2004)â€™s CCG system or the HPSG ERG based realiser (Carroll and Oepen, 2005) ::: The realiser presented here differs in mainly two ways from existing reversible realisers such as ( )'s CCG system or the HPSG ERG based realiser ( ) ::: DT NN VVD RB VVZ IN RB CD NNS IN VVG JJ NNS JJ IN ( NP NP NN CC DT NP NP VVN NN ( ) ::: Compare ::: 0.858984430017
2043904 5616851 We also lemmatize each word using the English lemmatizer as described in Minnen et al. (2000), and use lemmas as features instead of words ::: We also lemmatize each word using the English lemmatizer as described in Minnen et al. ( ) , and use lemmas as features instead of words ::: PP RB VVP DT NN VVG DT JJ NN IN VVN IN NP NP NP ( ) , CC VV NNS IN NNS RB IN NNS ::: Fundamental ::: 0.944911182523
631143 5185699 Query expansion (Voorhees, 1994; Mandala et al., 1999a; Fang and Zhai, 2006; Qiu and Frei, 1993; Bai et al., 2005; Cao et al., 2005) is a commonly used strategy to bridge the vocabulary gaps by expanding original queries with related terms ::: Query expansion ( ) is a commonly used strategy to bridge the vocabulary gaps by expanding original queries with related terms ::: NP NN ( ) VBZ DT RB VVN NN TO VV DT NN NNS IN VVG JJ NNS IN JJ NNS ::: BackGround ::: 0.567480306535
631143 5185699 However, previous studies failed to show any significant gain in retrieval performance when queries are expanded with terms selected from WordNet (Voorhees, 1994; Stairmand, 1997) ::: However , previous studies failed to show any significant gain in retrieval performance when queries are expanded with terms selected from WordNet ( ) ::: RB , JJ NNS VVD TO VV DT JJ NN IN NN NN WRB NNS VBP VVN IN NNS VVN IN NP ( ) ::: BackGround ::: 0.881917103688
631143 5185699 By incorporating this similarity function into the axiomatic retrieval models, we show that query expansion using the information from only WordNet can lead to significant improvement of retrieval performance, which has not been shown in the previous studies (Voorhees, 1994; Stairmand, 1997) ::: By incorporating this similarity function into the axiomatic retrieval models , we show that query expansion using the information from only WordNet can lead to significant improvement of retrieval performance , which has not been shown in the previous studies ( ) ::: IN VVG DT NN NN IN DT JJ NN NNS , PP VVP IN/that NN NN VVG DT NN IN JJ NP MD VV TO JJ NN IN NN NN , WDT VHZ RB VBN VVN IN DT JJ NNS ( ) ::: Compare ::: 0.940539943126
631143 5185699 We first compare the retrieval performance of query expansion with different similarity functions using short keyword (i.e., title-only) queries, because query expansion techniques are often more effective for shorter queries (Voorhees, 1994; Fang and Zhai, 2006) ::: We first compare the retrieval performance of query expansion with different similarity functions using short keyword (i.e. , title-only) queries , because query expansion techniques are often more effective for shorter queries ( ) ::: PP RB VVP DT NN NN IN NN NN IN JJ NN NNS VVG JJ NN NN , JJ NNS , IN NN NN NNS VBP RB RBR JJ IN JJR NNS ( ) ::: BackGround ::: 0.908893259146
2047122 4231183 For WSD, Fujii et al. (1998) used selective sampling for a Japanese language WSD system, Chen et al. (2006) used active learning for 5 verbs using coarse-grained evaluation, and H. T. Dang (2004) employed active learning for another set of 5 verbs ::: For WSD , Fujii et al. ( ) used selective sampling for a Japanese language WSD system , Chen et al. ( ) used active learning for 5 verbs using coarse-grained evaluation , and H ::: IN NP , NP NP NP ( ) VVN JJ NN IN DT JJ NN NN NN , NP NP NP ( ) VVN JJ NN IN CD NNS VVG JJ NN , CC NP ::: BackGround ::: 0.880324540928
4138526 4231196 Models of this type have previously been shown to yield very good g2p conversion results (Bisani and Ney, 2002; Galescu and Allen, 2001; Chen, 2003) ::: Models of this type have previously been shown to yield very good g2p conversion results ( ) ::: NNS IN DT NN VHP RB VBN VVN TO VV RB JJ NN NN NNS ( ) ::: BackGround ::: 0.719194952228
4138526 4231196 (Chen, 2003) also used a joint n-gram model ::: ( ) also used a joint n-gram model ::: ( ) RB VVD DT JJ NN NN ::: BackGround ::: 0.797724035217
908807 4905037 Recent work in automatic image annotation (Barnard et al., 2003; Blei and Jordan, 2003) and natural language processing (Steyvers et al., 2004), however, have demonstrated the advantages of using hierarchical Bayesian models for related tasks ::: Recent work in automatic image annotation ( ) and natural language processing ( ) , however , have demonstrated the advantages of using hierarchical Bayesian models for related tasks ::: JJ NN IN JJ NN NN ( ) CC JJ NN NN ( ) , RB , VHP VVN DT NNS IN VVG JJ NP NNS IN JJ NNS ::: BackGround ::: 0.746003846592
721528 5658172 The winning algorithms were LWL (Atkeson et al., 1997), SMO (Platt, 1999), and K* (Cleary and Trigg, 1995) (there were 7 tasks, and different algorithms could be selected for each task) ::: The winning algorithms were LWL ( ) , SMO ( ) , and K* ( ) (there were 7 tasks , and different algorithms could be selected for each task) ::: DT JJ NNS VBD NP ( ) , NP ( ) , CC NP ( ) NN VBD CD NNS , CC JJ NNS MD VB VVN IN DT NN ::: BackGround ::: 0.851102542781
15813 5616851 It is therefore unsurprising that much work on verb classification has adopted them as features (Schulte im Walde, 2000; Brew and Schulte im Walde, 2002; Korhonen et al., 2003) ::: It is therefore unsurprising that much work on verb classification has adopted them as features ( ) ::: PP VBZ RB VVG DT JJ NN IN NN NN VHZ VVN PP IN NNS ( ) ::: BackGround ::: 0.636714539967
2441172 4994026 These simple techniques are capable of modeling local topics without more expensive modeling of topic transitions used in (Griffiths et al., 2004; Wang and McCallum, 2005; Wallach, 2006; Gruber et al., 2007) ::: These simple techniques are capable of modeling local topics without more expensive modeling of topic transitions used in ( ) ::: DT JJ NNS VBP JJ IN VVG JJ NNS IN JJR JJ NN IN NN NNS VVN IN ( ) ::: BackGround ::: 0.723746864456
2122945 4231077 To perform translation, state-of-the-art MT systems use a statistical phrase-based approach (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by treating phrases as the basic units of translation ::: To perform translation , state-of-the-art MT systems use a statistical phrase-based approach ( ) by treating phrases as the basic units of translation ::: TO VV NN , JJ NP NNS VVP DT JJ JJ NN ( ) IN VVG NNS IN DT JJ NNS IN NN ::: BackGround ::: 0.807572853087
800432 6294835 The earliest approaches are closely related to image classification (Vailaya et al., 2001; Smeulders et al., 2000), where pictures are assigned a set of simple descriptions such as indoor, outdoor, landscape, people, animal ::: The earliest approaches are closely related to image classification ( ) , where pictures are assigned a set of simple descriptions such as indoor , outdoor , landscape , people , animal ::: DT JJS NNS VBP RB VVN TO NN NN ( ) , WRB NNS VBP VVN DT NN IN JJ NNS JJ IN JJ , JJ , NN , NNS , NN ::: BackGround ::: 0.811502671201
4727037 4930784 Another group of related work focuses on summarizing sentences through a series of deletions (Jing, 2000; Dorr et al., 2003; Galley  McKeown, 2007) ::: Another group of related work focuses on summarizing sentences through a series of deletions ( ) ::: DT NN IN JJ NN VVZ IN VVG NNS IN DT NN IN NNS ( ) ::: BackGround ::: 0.76980035892
4242681 5658172 Some other systems that avoided using the labels used WN as a supporting resource for their algorithms (Costello, 2007; Nakov and Hearst, 2007; Kim and Baldwin, 2007) ::: Some other systems that avoided using the labels used WN as a supporting resource for their algorithms ( ) ::: DT JJ NNS WDT VVD VVG DT NNS VVD NN IN DT VVG NN IN PP$ NNS ( ) ::: BackGround ::: 0.677834389405
4242681 5658172 Rosenfeld and Feldman (2007) discover relationship instances by clustering entities appearing in similar contexts ::: Rosenfeld and Feldman ( ) discover relationship instances by clustering entities appearing in similar contexts ::: NP CC NP ( ) VV NN NNS IN VVG NNS VVG IN JJ NNS ::: BackGround ::: 0.901387818866
259468 4231096 However, each of these assumes that the relations themselves are known in advance (implicitly or explicitly) so that the method can be provided with seed patterns (Agichtein and Gravano, 2000; Pantel et al, 2004), pattern-based rules (Etzioni et al, 2004), relation keywords (Sekine, 2006), or word pairs exemplifying relation instances (Pasca et al, 2006; Alfonseca et al, 2006; Rosenfeld and Feldman, 2006) ::: However , each of these assumes that the relations themselves are known in advance (implicitly or explicitly) so that the method can be provided with seed patterns ( ) , pattern-based rules ( ) , relation keywords ( ) , or word pairs exemplifying relation instances ( ) ::: RB , DT IN DT VVZ IN/that DT NNS PP VBP VVN IN NN NN CC JJ RB IN/that DT NN MD VB VVN IN NN NNS ( ) , JJ NNS ( ) , NN NNS ( ) , CC NN NNS VVG NN NNS ( ) ::: BackGround ::: 0.631054742868
1809513 4231125 The proposal draws on ideas from (Koller and Striegnitz, 2002; Gardent and Kow, 2005) and aims to determine whether for a given input (a set of TAG elementary trees whose semantics equate the input semantics), syntactic requirements and resources cancel out ::: The proposal draws on ideas from ( ) and aims to determine whether for a given input (a set of TAG elementary trees whose semantics equate the input semantics) , syntactic requirements and resources cancel out ::: DT NN VVZ IN NNS IN ( ) CC VVZ TO VV IN IN DT VVN NN NN VVN IN NP JJ NNS WP$ NNS VV DT NN NN , JJ NNS CC NNS VV RP ::: Fundamental ::: 0.899820053982
4572760 4905037 The method is based on the use of grounded language models to represent the relationship between words and the nonlinguistic context to which they refer (Fleischman and Roy, 2007) ::: The method is based on the use of grounded language models to repre-sent the relationship between words and the non-linguistic context to which they refer ( ) ::: DT NN VBZ VVN IN DT NN IN VVN NN NNS TO VV DT NN IN NNS CC DT JJ NN TO WDT PP VVP ( ) ::: Fundamental ::: 0.844741883758
4572760 4905037 Queries are generated artificially using a method similar to Berger and Lafferty (1999) and used in Fleischman and Roy (2007) ::: Queries are generated artificially using a method similar to Berger and Lafferty ( ) and used in Fleischman and Roy ( ) ::: NNS VBP VVN RB VVG DT NN JJ TO NP CC NP ( ) CC VVN IN NP CC NP ( ) ::: Fundamental ::: 0.925820099773
368811 4905037 We use the system of Bouthemy et al. (1999) which computes the camera motion using the parameters of a twodimensional affine model to fit every pair of sequential frames in a video ::: We use the system of Bouthemy et al. ( ) which computes the camera motion using the parameters of a two-dimensional affine model to fit every pair of sequential frames in a video ::: PP VVP DT NN IN NP NP NP ( ) WDT VVZ DT NN NN VVG DT NNS IN DT JJ JJ NN TO VV DT NN IN JJ NNS IN DT NN ::: Fundamental ::: 0.936382183835
1967060 5616851 WordNet). Although the problem of data sparsity is alleviated to certain extent (3), these features do not generally improve classification performance (Schulte im Walde, 2000; Joanis, 2002) ::: Although the problem of data sparsity is alleviated to certain extent (3) , these features do not generally improve classification performance ( ) ::: IN DT NN IN NN NN VBZ VVN TO JJ NN NN , DT NNS VVP RB RB VV NN NN ( ) ::: BackGround ::: 0.830454798537
5679 4231077 Hiero uses a general log-linear model (Och and Ney, 2002) where the weight of a derivation D for a particular source sentence and its translation is w(D) = Y ::: Hiero uses a general log-linear model ( ) where the weight of a derivation  D for a particular source sentence and its translation is where  ^  i is a feature function and  X i is the weight for feature  ^  i ::: NP VVZ DT JJ JJ NN ( ) WRB DT NN IN DT NN NP IN DT JJ NN NN CC PP$ NN VBZ WRB SYM NP VBZ DT NN NN CC NP NP VBZ DT NN IN NN SYM NP ::: BackGround ::: 0.753370803501
5679 4231112 These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method (Och and Ney, 2002) ::: These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method ( ) ::: DT NN NNS VBP VVN IN DT NN VVD TO VV JJ NN NN VVG RB JJ NN ( ) ::: Fundamental ::: 0.866025403784
165757 4231057 Since CFGs can be expressed as Horn-clause logic programs (Pereira and Shieber, 1987) and the Unfold-Fold transformation is provably correct for such programs (Sato, 1992; Pettorossi and Proeitti, 1992), it follows that its application to CFGs is provably correct as well ::: Since CFGs can be expressed as Horn-clause logic programs ( ) and the Unfold-Fold transformation is provably correct for such programs ( ) , it follows that its application to CFGs is provably correct as well ::: IN NP MD VB VVN IN NP NN NNS ( ) CC DT NN NN VBZ RB JJ IN JJ NNS ( ) , PP VVZ IN/that PP$ NN TO NP VBZ RB JJ IN RB ::: BackGround ::: 0.86903030742
10114551 4231196 Decision trees were one of the first data-based approaches to g2p and are still widely used (Kienappel and Kneser, 2001; Black et al., 1998) ::: Decision trees were one of the first data-based approaches to g2p and are still widely used ( ) ::: NN NNS VBD CD IN DT JJ JJ NNS TO NN CC VBP RB RB JJ ( ) ::: BackGround ::: 0.8106792284
1704923 4905037 Recognizing speech in broadcast video is a necessary precursor to many multimodal applications such as video search and summarization (Snoek and Worring, 2005;) ::: Recognizing speech in broadcast video is a necessary precursor to many multimodal applications such as video search and summarization ( ) ::: VVG NN IN NN NN VBZ DT JJ NN TO JJ JJ NNS JJ IN JJ NN CC NN ( ) ::: BackGround ::: 0.89148498832
1782520 4231023 An important area for future research will be to explore the correlation between our distance metric for syntactic similarity and various quantitative measures of semantic similarity (Pedersen, et al., 2004) ::: An important area for future research will be to explore the correlation between our distance metric for syntactic similarity and various quantitative measures of semantic similarity ( ) ::: DT JJ NN IN JJ NN MD VB TO VV DT NN IN PP$ NN JJ IN JJ NN CC JJ JJ NNS IN JJ NN ( ) ::: BackGround ::: 0.912870929175
2182006 4231117 E.g., in text-to-text generation (Barzilay  McKeown, 2005; Marsi  Krahmer, 2005; Wan et al., 2005), new sentences are fused from dependency structures of input sentences ::: E.g. , in text-to-text generation ( ) , new sentences are fused from dependency structures of input sentences ::: FW , IN NN NN ( ) , JJ NNS VBP VVN IN NN NNS IN NN NNS ::: BackGround ::: 0.733799385705
35930 5616851 Although there exist several manually-created verb lexicons or ontologies, including Levinâ€™s verb taxonomy, VerbNet, and FrameNet, automatic verb classification (AVC) is still necessary for extending existing lexicons (Korhonen and Briscoe, 2004), building and tuning lexical information specific to different domains (Korhonen et al., 2006), and bootstrapping verb lexicons for new languages (Tsang et al., 2002) ::: Although there exist several manually-created verb lexicons or ontologies , including Levin's verb taxonomy , VerbNet , and FrameNet , automatic verb classification (AVC) is still necessary for extending existing lexicons ( ) , building and tuning lexical information specific to different domains ( ) , and bootstrapping verb lexicons for new languages ( ) ::: IN EX VV JJ JJ NN NNS CC NNS , VVG NP NN NN , NP , CC NP , JJ NN NN NN VBZ RB JJ IN VVG JJ NNS ( ) , VVG CC VVG JJ NN JJ TO JJ NNS ( ) , CC VVG NN NNS IN JJ NNS ( ) ::: BackGround ::: 0.891695712069
3715440 4231023 Our work suggests that feature generalization based on verb-similarity may compliment approaches to generalization based on role-similarity (Gildea and Jurafsky, 2002; Baldewein et al., 2004) ::: Our work suggests that feature generalization based on verb-similarity may compliment approaches to generalization based on role-similarity ( ) ::: PP$ NN VVZ IN/that NN NN VVN IN NN MD NN NNS TO NN VVN IN NN ( ) ::: BackGround ::: 0.854242196177
845552 4231071 they co-occur within the time window . This situation is very similar to the training process of translation models in statistical machine translation (Brown et al., 1993), where parallel corpus is used to find the mappings between words from different languages by exploiting their co-occurrence patterns ::: This situation is very similar to the training process of translation models in statistical machine translation ( ) , where parallel corpus is used to find the mappings between words from different languages by exploiting their co-occurrence patterns ::: DT NN VBZ RB JJ TO DT NN NN IN NN NNS IN JJ NN NN ( ) , WRB JJ NN VBZ VVN TO VV DT NNS IN NNS IN JJ NNS IN VVG PP$ NN NNS ::: Fundamental ::: 0.892691182267
845552 4231071 This optimization problem can be solved by the EM algorithm (Brown et al., 1993) ::: J^Pr(Wj |o k) = 1 , Vk j =1 This optimization problem can be solved by the EM algorithm ( ) ::: NP NP NP SYM CD , NP NN NN DT NN NN MD VB VVN IN DT JJ NN ( ) ::: BackGround ::: 0.54554472559
2414995 4139591 An SCFG (Lewis and Stearns, 1968) is a context-free rewriting system for generating string pairs ::: An SCFG ( ) is a context-free rewriting system for generating string pairs ::: DT NP ( ) VBZ DT JJ VVG NN IN VVG NN NNS ::: BackGround ::: 0.816496580928
630783 5185699 However, previous studies failed to show any significant gain in retrieval performance when queries are expanded with terms selected from WordNet (Voorhees, 1994; Stairmand, 1997) ::: However , previous studies failed to show any significant gain in retrieval performance when queries are expanded with terms selected from WordNet ( ) ::: RB , JJ NNS VVD TO VV DT JJ NN IN NN NN WRB NNS VBP VVN IN NNS VVN IN NP ( ) ::: BackGround ::: 0.881917103688
630783 5185699 By incorporating this similarity function into the axiomatic retrieval models, we show that query expansion using the information from only WordNet can lead to significant improvement of retrieval performance, which has not been shown in the previous studies (Voorhees, 1994; Stairmand, 1997) ::: By incorporating this similarity function into the axiomatic retrieval models , we show that query expansion using the information from only WordNet can lead to significant improvement of retrieval performance , which has not been shown in the previous studies ( ) ::: IN VVG DT NN NN IN DT JJ NN NNS , PP VVP IN/that NN NN VVG DT NN IN JJ NP MD VV TO JJ NN IN NN NN , WDT VHZ RB VBN VVN IN DT JJ NNS ( ) ::: Compare ::: 0.940539943126
53284 4231052 BLEU score using the algorithm described in (Och, 2003) ::: BLEU score using the algorithm described in ( ) ::: NP NN VVG DT NN VVN IN ( ) ::: NULL ::: 0.797724035217
1831717 4905037 Shot detection and segmentation is a well studied problem; in this work we use the method of Tardini et al. (2005) ::: Shot detection and segmentation is a well studied problem; in this work we use the method of Tardini et al. ( ) ::: NN NN CC NN VBZ DT RB VVN NN IN DT NN PP VVP DT NN IN NP NP NP ( ) ::: Fundamental ::: 0.93250480824
115479 4231117 Unlike overgeneration approaches (Varges  Mellish, 2001, inter alia) which select the best of all possible outputs ours is more efcient, because we do not need to generate every permutation ::: Unlike overgeneration approaches ( ) which select the best of all possible outputs ours is more efficient , because we do not need to generate every permutation ::: IN NN NNS ( ) WDT VVP DT JJS IN DT JJ NNS PP VBZ RBR JJ , IN PP VVP RB VV TO VV DT NN ::: Compare ::: 0.843220911373
49119 4231096 In several studies (e.g., Widdows and Dorow, 2002; Pantel et al, 2004; Davidov and Rappoport, 2006) it has been shown that relatively unsupervised and language-independent methods could be used to generate many thousands of sets of words whose semantics is similar in some sense ::: In several studies ( ) it has been shown that relatively unsupervised and language-independent methods could be used to generate many thousands of sets of words whose semantics is similar in some sense ::: IN JJ NNS ( ) PP VHZ VBN VVN IN/that RB JJ CC JJ NNS MD VB VVN TO VV JJ NNS IN NNS IN NNS WP$ NNS VBZ JJ IN DT NN ::: BackGround ::: 0.825028647325
49119 4231096 Most related work deals with discovery of hypernymy (Hearst, 1992; Pantel et al, 2004), synonymy (Roark and Charniak, 1998; Widdows and Dorow, 2002; Davidov and Rappoport, 2006) and meronymy (Berland and Charniak, 1999) ::: Most related work deals with discovery of hypernymy ( ) , synonymy ( ) and meronymy ( ) ::: RBS VVN NN NNS IN NN IN NN ( ) , NN ( ) CC JJ ( ) ::: BackGround ::: 0.599042297873
1834583 5658172 Other resources used for relationship discovery include Wikipedia (Strube and Ponzetto, 2006), thesauri or synonym sets (Turney, 2005) and domainspecific semantic hierarchies like MeSH (Rosario and Hearst, 2001) ::: Other resources used for relationship discovery include Wikipedia ( ) , thesauri or synonym sets ( ) and domain-specific semantic hierarchies like MeSH ( ) ::: JJ NNS VVN IN NN NN VVP NP ( ) , NNS CC NN NNS ( ) CC JJ JJ NNS IN NP ( ) ::: BackGround ::: 0.726483157257
1834583 5658172 We have used the exact evaluation procedure described in (Turney, 2006), achieving a class f-score average of 60.1, as opposed to 54.6 in (Turney, 2005) and 51.2 in (Nastase et al., 2006) ::: We have used the exact evaluation procedure described in ( ) , achieving a class f-score average of 60.1 , as opposed to 54.6 in ( ) and 51.2 in ( ) ::: PP VHP VVN DT JJ NN NN VVN IN ( ) , VVG DT NN NN NN IN CD , RB VVD TO CD IN ( ) CC CD IN ( ) ::: Compare ::: 0.84162541153
4231182 5616851 SCF and DR: These more linguistically informed features are constructed based on the grammatical relations generated by the CC CCG parser (Clark and Curran, 2007) ::: SCF and DR: These more linguistically informed features are constructed based on the grammatical relations generated by the C&C CCG parser ( ) ::: NP CC NP DT RBR RB VVN NNS VBP VVN VVN IN DT JJ NNS VVN IN DT NP NP NN ( ) ::: Fundamental ::: 0.810140446728
2464777 5658172 Relationship classification is known to improve many practical tasks, e.g., textual entailment (Tatu and Moldovan, 2005) ::: Relationship classification is known to improve many practical tasks , e.g. , textual entailment ( ) ::: NN NN VBZ VVN TO VV JJ JJ NNS , FW , JJ NN ( ) ::: BackGround ::: 0.827170191869
631142 5185699 Voorhees (Voorhees, 1993) showed that using Word-Net for word sense disambiguation degrade the retrieval performance ::: Voorhees ( ) showed that using WordNet for word sense disambiguation degrade the retrieval performance ::: NP ( ) VVD IN/that VVG NN IN NN NN NN VVP DT NN NN ::: BackGround ::: 0.80622577483
2142135 4930784 Approaches include incorporating a subcategorization feature (Gildea  Jurafsky, 2002; Xue  Palmer, 2004), such as the one used in our baseline; and building a model which jointly classifies all arguments of a verb (Toutanova et al., 2005) ::: Approaches include incorporating a subcategorization feature ( ) , such as the one used in our baseline; and building a model which jointly classifies all arguments of a verb ( ) ::: NNS VVP VVG DT NN NN ( ) , JJ IN DT CD VVN IN PP$ NN CC VVG DT NN WDT RB VVZ DT NNS IN DT NN ( ) ::: BackGround ::: 0.852802865422
2510330 5616851  or sufficient for us to draw firm conclusions about its usage, the information about the class to which the verb type belongs can compensate for it, addressing the pervasive problem of data sparsity in a wide range of NLP tasks, such as automatic extraction of subcategorization frames (Korhonen, 2002), semantic role labeling (Swier and Stevenson, 2004; Gildea and Jurafsky, 2002), natural language generation for machine translation (Habash et ::: When the information about a verb type is not available or sufficient for us to draw firm conclusions about its usage , the information about the class to which the verb type belongs can compensate for it , addressing the pervasive problem of data sparsity in a wide range of NLP tasks , such as automatic extraction of subcategorization frames ( ) , semantic role labeling ( ) , natural language generation for machine translation ( ) , and deriving predominant verb senses from unlabeled data ( ) ::: WRB DT NN IN DT NN NN VBZ RB JJ CC JJ IN PP TO VV NN NNS IN PP$ NN , DT NN IN DT NN TO WDT DT NN NN VVZ MD VV IN PP , VVG DT JJ NN IN NN NN IN DT JJ NN IN NP NNS , JJ IN JJ NN IN NN NNS ( ) , JJ NN VVG ( ) , JJ NN NN IN NN NN ( ) , CC VVG JJ NN NNS IN JJ NNS ( ) ::: BackGround ::: 0.857671551467
2165038 5658172 Several different relationship hierarchies have been proposed (Nastase and Szpakowicz, 2003; Moldovan et al., 2004) ::: Several different relationship hierarchies have been proposed ( ) ::: JJ JJ NN NNS VHP VBN VVN ( ) ::: BackGround ::: 0.64168894792
2165038 5658172 Since the SemEval dataset is of a very specific nature, we have also applied our classification framework to the (Nastase and Szpakowicz, 2003) dataset, which contains 600 pairs labeled with 5 main relationship types ::: Since the SemEval dataset is of a very specific nature , we have also applied our classification framework to the ( ) dataset , which contains 600 pairs labeled with 5 main relationship types ::: IN DT JJ NN VBZ IN DT RB JJ NN , PP VHP RB VVN PP$ NN NN TO DT ( ) NN , WDT VVZ CD NNS VVN IN CD JJ NN NNS ::: Fundamental ::: 0.921954445729
1766593 5616851 Although dependency relations have been widely used in automatic acquisition of lexical information, such as detection of polysemy (Lin, 1998) and WSD (McCarthy et al., 2004), their utility in AVC still remains untested ::: Although dependency relations have been widely used in automatic acquisition of lexical information , such as detection of polysemy ( ) and WSD ( ) , their utility in AVC still remains untested ::: IN NN NNS VHP VBN RB VVN IN JJ NN IN JJ NN , JJ IN NN IN NN ( ) CC NP ( ) , PP$ NN IN NP RB VVZ JJ ::: BackGround ::: 0.891555828242
2073437 4905037 Although performance is often reasonable in controlled environments (such as studio news rooms), automatic speech recognition (ASR) systems have significant difficulty in noisier settings (such as those found in live sports broadcasts) (Wactlar et al., 1996) ::: Although performance is often reasonable in controlled environments (such as studio news rooms) , automatic speech recognition (ASR) systems have significant difficulty in noisier settings (such as those found in live sports broadcasts) ( ) ::: IN NN VBZ RB JJ IN JJ NNS NN IN NN NN NN , JJ NN NN NN NNS VHP JJ NN IN JJR NNS NN IN DT VVN IN JJ NNS NN ( ) ::: BackGround ::: 0.935414346693
2073437 4905037 Such video IR systems often use speech transcriptions to index segments of video in much the same way that words are used to index text documents (Wactlar et al., 1996) ::: Such video IR systems often use speech transcriptions to index segments of video in much the same way that words are used to index text documents ( ) ::: JJ JJ NN NNS RB VVP NN NNS TO NN NNS IN NN IN RB DT JJ NN IN/that NNS VBP VVN TO NN NN NNS ( ) ::: BackGround ::: 0.917662935482
5559010 4231117 E.g., in text-to-text generation (Barzilay  McKeown, 2005; Marsi  Krahmer, 2005; Wan et al., 2005), new sentences are fused from dependency structures of input sentences ::: E.g. , in text-to-text generation ( ) , new sentences are fused from dependency structures of input sentences ::: FW , IN NN NN ( ) , JJ NNS VBP VVN IN NN NNS IN NN NNS ::: BackGround ::: 0.733799385705
22606 4231117 We provide evidence that the task requires language-specic knowledge to achieve better results and point to the most difcult part of it. Similar to Langkilde  Knight (1998) we utilize statistical methods ::: Similar to Langkilde & Knight ( ) we utilize statistical methods ::: JJ TO NP CC NP ( ) PP VV JJ NNS ::: Fundamental ::: 0.54554472559
6200749 4231151 (Knight and Graehl, 1998; Meng et al., 2001; Al-Onaizan and Knight, 2002; Gao et al., 2004) â€” has mostly assumed that one has a training lexicon of transliteration pairs, from which one can learn a model, often a sourcechannel or MaxEnt-based model ::: 2 Previous Work Previous work — e.g. ( ) — has mostly assumed that one has a training lexicon of transliteration pairs , from which one can learn a model , often a source-channel or MaxEnt-based model ::: CD JJ NN JJ NN SENT FW ( ) SENT VHZ RB VVN IN/that PP VHZ DT NN NN IN NN NNS , IN WDT PP MD VV DT NN , RB DT NN CC JJ NN ::: BackGround ::: 0.662406258486
2414975 5616851 Although there exist several manually-created verb lexicons or ontologies, including Levinâ€™s verb taxonomy, VerbNet, and FrameNet, automatic verb classification (AVC) is still necessary for extending existing lexicons (Korhonen and Briscoe, 2004), building and tuning lexical information specific to different domains (Korhonen et al., 2006), and bootstrapping verb lexicons for new languages (Tsang et al., 2002) ::: Although there exist several manually-created verb lexicons or ontologies , including Levin's verb taxonomy , VerbNet , and FrameNet , automatic verb classification (AVC) is still necessary for extending existing lexicons ( ) , building and tuning lexical information specific to different domains ( ) , and bootstrapping verb lexicons for new languages ( ) ::: IN EX VV JJ JJ NN NNS CC NNS , VVG NP NN NN , NP , CC NP , JJ NN NN NN VBZ RB JJ IN VVG JJ NNS ( ) , VVG CC VVG JJ NN JJ TO JJ NNS ( ) , CC VVG NN NNS IN JJ NNS ( ) ::: BackGround ::: 0.891695712069
1834611 5658172 This corpus was extracted from the web starting from open directory links, comprising English web pages with varied topics and styles (Gabrilovich and Markovitch, 2005) ::: This corpus was extracted from the web starting from open directory links , comprising English web pages with varied topics and styles ( ) ::: DT NN VBD VVN IN DT NN VVG IN JJ NN NNS , VVG JJ NN NNS IN JJ NNS CC NNS ( ) ::: Fundamental ::: 0.90520381097
4563862 5658172 Some other systems that avoided using the labels used WN as a supporting resource for their algorithms (Costello, 2007; Nakov and Hearst, 2007; Kim and Baldwin, 2007) ::: Some other systems that avoided using the labels used WN as a supporting resource for their algorithms ( ) ::: DT JJ NNS WDT VVD VVG DT NNS VVD NN IN DT VVG NN IN PP$ NNS ( ) ::: BackGround ::: 0.677834389405
2384515 4231096 In several studies (e.g., Widdows and Dorow, 2002; Pantel et al, 2004; Davidov and Rappoport, 2006) it has been shown that relatively unsupervised and language-independent methods could be used to generate many thousands of sets of words whose semantics is similar in some sense ::: In several studies ( ) it has been shown that relatively unsupervised and language-independent methods could be used to generate many thousands of sets of words whose semantics is similar in some sense ::: IN JJ NNS ( ) PP VHZ VBN VVN IN/that RB JJ CC JJ NNS MD VB VVN TO VV JJ NNS IN NNS IN NNS WP$ NNS VBZ JJ IN DT NN ::: BackGround ::: 0.825028647325
2384515 4231096 Most related work deals with discovery of hypernymy (Hearst, 1992; Pantel et al, 2004), synonymy (Roark and Charniak, 1998; Widdows and Dorow, 2002; Davidov and Rappoport, 2006) and meronymy (Berland and Charniak, 1999) ::: Most related work deals with discovery of hypernymy ( ) , synonymy ( ) and meronymy ( ) ::: RBS VVN NN NNS IN NN IN NN ( ) , NN ( ) CC JJ ( ) ::: BackGround ::: 0.599042297873
2384515 4231096 Studying relationships between tagged named entities, (Hasegawa et al, 2004; Hassan et al, 2006) proposed unsupervised clustering methods that assign given (or semi-automatically extracted) sets of pairs into several clusters, where each cluster corresponds to one of a known relationship type ::: Studying relationships between tagged named entities , ( ) proposed unsupervised clustering methods that assign given (or semi-automatically extracted) sets of pairs into several clusters , where each cluster corresponds to one of a known relationship type ::: VVG NNS IN VVN VVN NNS , ( ) VVN JJ VVG NNS WDT VVP VVN NN RB JJ NNS IN NNS IN JJ NNS , WRB DT NN VVZ TO CD IN DT VVN NN NN ::: BackGround ::: 0.848528137424
2384515 4231096 However, each of these assumes that the relations themselves are known in advance (implicitly or explicitly) so that the method can be provided with seed patterns (Agichtein and Gravano, 2000; Pantel et al, 2004), pattern-based rules (Etzioni et al, 2004), relation keywords (Sekine, 2006), or word pairs exemplifying relation instances (Pasca et al, 2006; Alfonseca et al, 2006; Rosenfeld and Feldman, 2006) ::: However , each of these assumes that the relations themselves are known in advance (implicitly or explicitly) so that the method can be provided with seed patterns ( ) , pattern-based rules ( ) , relation keywords ( ) , or word pairs exemplifying relation instances ( ) ::: RB , DT IN DT VVZ IN/that DT NNS PP VBP VVN IN NN NN CC JJ RB IN/that DT NN MD VB VVN IN NN NNS ( ) , JJ NNS ( ) , NN NNS ( ) , CC NN NNS VVG NN NNS ( ) ::: BackGround ::: 0.631054742868
2464788 4231112 Ittycheriah and Roukos (2005); Moore (2005); Blunsom and Cohn (2006), have received great amount of study recently ::: Discriminative word alignment models , such as Ittycheriah and Roukos ( ); Moore ( ); Blunsom and Cohn ( ) , have received great amount of study recently ::: JJ NN NN NNS , JJ IN NP CC NP ( NP NP ( NP NP CC NP ( ) , VHP VVN JJ NN IN NN RB ::: BackGround ::: 0.711286759159
649061 4231117 The work of Uchimoto et al. (2000) is done on the free word order language Japanese ::: The work of Uchimoto et al. ( ) is done on the free word order language Japanese ::: DT NN IN NP NP NP ( ) VBZ VVN IN DT JJ NN NN NN JJ ::: BackGround ::: 0.912870929175
649061 4231117 plemented the algorithm of Uchimoto et al. (2000) ::: For the fourth baseline (UCHIMOTO) , we utilized a maximum entropy learner (OpenNLP 8) and reim-plemented the algorithm of Uchimoto et al. ( ) ::: IN DT JJ NN NN , PP VVD DT JJ NN NN NN JJ CC VVD DT NN IN NP NP NP ( ) ::: Fundamental ::: 0.516397779494
649061 4231117 Apart from acc and , we also adopt the metrics used by Uchimoto et al. (2000) and Ringger et al. (2004) ::: Apart from acc and t , we also adopt the metrics used by Uchimoto et al. ( ) and Ringger et al. ( ) ::: RB IN NN CC NN , PP RB VV DT NNS VVN IN NP NP NP ( ) CC NP NP NP ( ) ::: Fundamental ::: 0.907114735222
870581 5616851 Following studies on automatic SCF extraction (Brent, 1993), we apply a statistical test (Binomial Hypothesis Test) to the unfiltered-Levin-SCF to filter out noisy SCFs, and denote the resulting SCF set as filtered-Levin-SCF ::: Following studies on automatic SCF extraction ( ) , we apply a statistical test (Binomial Hypothesis Test) to the unfiltered-Levin-SCF to filter out noisy SCFs , and denote the resulting SCF set as filtered-Levin-SCF ::: VVG NNS IN JJ NP NN ( ) , PP VVP DT JJ NN JJ NP NP TO DT NN TO NN IN JJ NNS , CC VV DT VVG NP VVN IN NN ::: Fundamental ::: 0.963624111659
496230 4231207 While there have been previous systems that encode generation as planning (Cohen and Perrault, 1979; Appelt, 1985; Heeman and Hirst, 1995), our approach is distinguished from these systems by its focus on the grammatically specified contributions of each individual word (and the TAG tree it anchors) to syntax, semantics, and local pragmatics (Hobbs et al., 1993) ::: While there have been previous systems that encode generation as planning ( ) , our approach is distinguished from these systems by its focus on the grammatically specified contributions of each individual word (and the TAG tree it anchors) to syntax , semantics , and local pragmatics ( ) ::: IN EX VHP VBN JJ NNS WDT VVP NN IN NN ( ) , PP$ NN VBZ VVN IN DT NNS IN PP$ NN IN DT RB VVN NNS IN DT JJ NN NN DT NP NN PP JJ TO NN , NNS , CC JJ NNS ( ) ::: BackGround ::: 0.872502871778
5501233 4231174 ASO has been shown to be effective on the following natural language processing tasks: text categorization, named entity recognition, part-of-speech tagging, and word sense disambiguation (Ando and Zhang, 2005a; Ando and Zhang, 2005b; Ando, 2006) ::: ASO has been shown to be effective on the following natural language processing tasks: text categorization , named entity recognition , part-of-speech tagging , and word sense disambiguation ( ) ::: NP VHZ VBN VVN TO VB JJ IN DT VVG JJ NN VVG JJ NN NN , VVN NN NN , NN VVG , CC NN NN NN ( ) ::: BackGround ::: 0.766616532861
5501233 4231174 ASO has been demonstrated to be an effective semi-supervised learning algorithm (Ando and Zhang, 2005a; Ando and Zhang, 2005b; Ando, 2006) ::: ASO has been demonstrated to be an effective semi-supervised learning algorithm ( ) ::: NP VHZ VBN VVN TO VB DT JJ JJ NN NN ( ) ::: BackGround ::: 0.594088525786
5501233 4231174 More recently, for the word sense disambiguation (WSD) task, (Ando, 2006) experimented with both supervised and semi-supervised auxiliary problems, although the auxiliary problems she used are different from ours ::: More recently , for the word sense disambiguation (WSD) task , ( ) experimented with both supervised and semi-supervised auxiliary problems , although the auxiliary problems she used are different from ours ::: RBR RB , IN DT NN NN NN NN NN , ( ) VVN IN DT JJ CC JJ JJ NNS , IN DT JJ NNS PP VVD VBP JJ IN PP ::: BackGround ::: 0.948683298051
787230 4231077 Hiero (Chiang, 2005) is a hierarchical phrase-based model for statistical machine translation, based on weighted synchronous context-free grammar (CFG) (Lewis and Stearns, 1968) ::: Hiero ( ) is a hierarchical phrase-based model for statistical machine translation , based on weighted synchronous context-free grammar (CFG) ( ) ::: NP ( ) VBZ DT JJ JJ NN IN JJ NN NN , VVN IN JJ JJ JJ NN NN ( ) ::: BackGround ::: 0.850962943397
4144153 4231183 In applying active learning for domain adaptation, Zhang et al. (2003) presented work on sentence boundary detection using generalized Winnow, while Tur et al. (2004) performed language model adaptation of automatic speech recognition systems ::: In applying active learning for domain adaptation , Zhang et al. ( ) presented work on sentence boundary detection using generalized Winnow , while Tur et al. ( ) performed language model adaptation of automatic speech recognition systems ::: IN VVG JJ NN IN NN NN , NP NP NP ( ) VVN NN IN NN NN NN VVG VVN NP , IN NP NP NP ( ) VVN NN NN NN IN JJ NN NN NNS ::: BackGround ::: 0.951189731211
22851 4231023 In particular, our approach would be applicable to corpora with framespecific role labels, e.g. FrameNet (Baker et al., 1998) ::: In particular , our approach would be applicable to corpora with frame-specific role labels , e.g.FrameNet ( ) ::: IN JJ , PP$ NN MD VB JJ TO NNS IN JJ NN NNS , NN ( ) ::: BackGround ::: 0.775631534993
2464493 4231174 So far we are aware of only one English NomBank-based SRL system (Jiang and Ng, 2006), which uses the maximum entropy classifier, although similar efforts are reported on the Chinese NomBank by (Xue, 2006) and on FrameNet by (Pradhan et al., 2004) using a small set of hand-selected nominalizations ::: So far we are aware of only one English NomBank-based SRL system ( ) , which uses the maximum entropy classifier , although similar efforts are reported on the Chinese NomBank by ( ) and on FrameNet by ( ) using a small set of hand-selected nominalizations ::: RB RB PP VBP JJ IN RB CD NNS JJ NP NN ( ) , WDT VVZ DT JJ NN NN , IN JJ NNS VBP VVN IN DT JJ NN IN ( ) CC IN NP IN ( ) VVG DT JJ NN IN JJ NNS ::: BackGround ::: 0.89295850825
2464493 4231174 Second, we achieve accuracy higher than that reported in (Jiang and Ng, 2006) and advance the state of the art in SRL research ::: Second , we achieve accuracy higher than that reported in ( ) and advance the state of the art in SRL research ::: RB , PP VVP NN JJR IN DT VVN IN ( ) CC VV DT NN IN DT NN IN NP NN ::: Compare ::: 0.898807063528
2464493 4231174 Eighteen baseline features and six additional features are proposed in (Jiang and Ng, 2006) for Nom-Bank argument identification ::: Eighteen baseline features and six additional features are proposed in ( ) for NomBank argument identification ::: CD JJ NNS CC CD JJ NNS VBP VVN IN ( ) IN NP NN NN ::: Fundamental ::: 0.8
2464493 4231174 Unlike in (Jiang and Ng, 2006), we do not prune arguments dominated by other arguments or those that overlap with the predicate in the training data ::: Unlike in ( ) , we do not prune arguments dominated by other arguments or those that overlap with the predicate in the training data ::: IN IN ( ) , PP VVP RB VV NNS VVN IN JJ NNS CC DT WDT VVP IN DT NN IN DT NN NNS ::: Compare ::: 0.907485212973
2464493 4231174 The J&N column presents the result reported in (Jiang and Ng, 2006) using both baseline and additional features ::: The J&N column presents the result reported in ( ) using both baseline and additional features ::: DT NP NN VVZ DT NN VVN IN ( ) VVG DT NN CC JJ NNS ::: BackGround ::: 0.861411043293
2464493 4231174 To find a smaller set of effective features, we start with all the features considered in (Jiang and Ng, 2006), in (Xue and Palmer, 2004), and various combinations of them, for a total of 52 features ::: To find a smaller set of effective features , we start with all the features considered in ( ) , in ( ) , and various combinations of them , for a total of 52 features ::: TO VV DT JJR NN IN JJ NNS , PP VVP IN PDT DT NNS VVN IN ( ) , RB ( ) , CC JJ NNS IN PP , IN DT NN IN CD NNS ::: Fundamental ::: 0.895273785786
2464493 4231174 The J&N column presents the result reported in (Jiang and Ng, 2006) ::: The J&N column presents the result reported in ( ) ::: DT NP NN VVZ DT NN VVN IN ( ) ::: BackGround ::: 0.774596669241
2464493 4231174 This is the same configuration as reported in (Pradhan et al., 2005; Jiang and Ng, 2006) ::: This is the same configuration as reported in ( ) ::: DT VBZ DT JJ NN IN VVN IN ( ) ::: Fundamental ::: 0.666666666667
2464493 4231174 Our maximum entropy classifier consistently outperforms (Jiang and Ng, 2006), which also uses a maximum entropy classifier ::: Table 3: Fl scores of various classifiers on NomBank SRL Our maximum entropy classifier consistently outperforms ( ) , which also uses a maximum entropy classifier ::: NN CD NP NNS IN JJ NNS IN NP NP PP$ JJ NN NN RB VVZ ( ) , WDT RB VVZ DT JJ NN NN ::: Compare ::: 0.705642285073
2464493 4231174 Our results outperform those reported in (Jiang and Ng, 2006) ::: Our results outperform those reported in ( ) ::: PP$ NNS VVP DT VVN IN ( ) ::: Compare ::: 0.707106781187
4789796 5616851 Other methods for combining syntactic information with lexical information have also been attempted (Merlo and Stevenson, 2001; Joanis et al., 2007) ::: Other methods for combining syntactic information with lexical information have also been attempted ( ) ::: JJ NNS IN VVG JJ NN IN JJ NN VHP RB VBN VVN ( ) ::: BackGround ::: 0.774596669241
4789796 5616851 Joanis et al. (2007) demonstrates that the general feature space they devise achieves a rate of error reduction ranging from 48% to 88% over a chance baseline accuracy, across classification tasks of varying difficulty ::: Joanis et al. ( ) demonstrates that the general feature space they devise achieves a rate of error reduction ranging from 48% to 88% over a chance baseline accuracy , across classification tasks of varying difficulty ::: NP NP NP ( ) VVZ IN/that DT JJ NN NN PP VVP VVZ DT NN IN NN NN VVG IN CD TO CD IN DT NN NN NN , IN NN NNS IN VVG NN ::: BackGround ::: 0.961769203084
4789796 5616851 However, some of the functions words, prepositions in particular, are known to carry great amount of syntactic information that is related to lexical meanings of verbs (Schulte im Walde, 2003; Brew and Schulte im Walde, 2002; Joanis et al., 2007) ::: However , some of the functions words , prepositions in particular , are known to carry great amount of syntactic information that is related to lexical meanings of verbs ( ) ::: RB , DT IN DT NNS NNS , NNS IN JJ , VBP VVN TO VV JJ NN IN JJ NN WDT VBZ VVN TO JJ NNS IN NNS ( ) ::: BackGround ::: 0.779193722474
4789796 5616851 Joanis et al. (2007), which consists of 224 features ::: JOANIS07: We use the feature set proposed in Joanis et al. ( ) , which consists of 224 features ::: NP PP VVP DT NN NN VVN IN NP NP NP ( ) , WDT VVZ IN CD NNS ::: Fundamental ::: 0.603022689156
4789796 5616851 For example, Schulte im Walde (2000) uses 153 verbs in 30 classes, and Joanis et al. (2007) takes on 835 verbs and 15 verb classes ::: For example , Schulte im Walde ( ) uses 153 verbs in 30 classes , and Joanis et al. ( ) takes on 835 verbs and 15 verb classes ::: IN NN , NP NP NP ( ) VVZ CD NNS IN CD NNS , CC NP NP NP ( ) VVZ IN CD NNS CC CD NN NNS ::: BackGround ::: 0.937436866561
2290211 4231174 So far we are aware of only one English NomBank-based SRL system (Jiang and Ng, 2006), which uses the maximum entropy classifier, although similar efforts are reported on the Chinese NomBank by (Xue, 2006) 208 and on FrameNet by (Pradhan et al., 2004) using a small set of hand-selected nominalizations ::: So far we are aware of only one English NomBank-based SRL system ( ) , which uses the maximum entropy classifier , although similar efforts are reported on the Chinese NomBank by ( ) and on FrameNet by ( ) using a small set of hand-selected nominalizations ::: RB RB PP VBP JJ IN RB CD NNS JJ NP NN ( ) , WDT VVZ DT JJ NN NN , IN JJ NNS VBP VVN IN DT JJ NN IN ( ) CC IN NP IN ( ) VVG DT JJ NN IN JJ NNS ::: BackGround ::: 0.886557289459
3221004 4231109 However, few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them (Cherry and Lin, 2006; DaumÂ´e III and Marcu, 2005; Lopez and Resnik, 2005) ::: However , few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them ( ) ::: RB , JJ IN DT NNS VHP RB VVN DT NN IN NN NNS CC DT JJ NNS WDT VVP PP ( ) ::: BackGround ::: 0.739140494986
1278889 5153649 Specifically, they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 ::: Specifically , they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Con-rath ( ) measure ::: RB , PP VVD TO JJ NNS CC VBD RB JJ IN VVN IN DT NP CC NP ( ) NN ::: Fundamental ::: 0.846327266056
3331459 4231117 Ringger et al. (2004) aim at regenerating the order of constituents as well as the order within them for German and French technical manuals ::: Ringger et al. ( ) aim at regenerating the order of constituents as well as the order within them for German and French technical manuals ::: NP NP NP ( ) NN IN VVG DT NN IN NNS RB RB IN DT NN IN PP IN JJ CC JJ JJ NNS ::: BackGround ::: 0.951971638233
3331459 4231117 Similar to Ringger et al. (2004), we nd the order with the highest probability conditioned on syntactic and semantic categories ::: Similar to Ringger et al. ( ) , we find the order with the highest probability conditioned on syntactic and semantic categories ::: JJ TO NP NP NP ( ) , PP VVP DT NN IN DT JJS NN VVN IN JJ CC JJ NNS ::: Fundamental ::: 0.890870806375
3331459 4231117 Apart from acc and , we also adopt the metrics used by Uchimoto et al. (2000) and Ringger et al. (2004) ::: Apart from acc and t , we also adopt the metrics used by Uchimoto et al. ( ) and Ringger et al. ( ) ::: RB IN NN CC NN , PP RB VV DT NNS VVN IN NP NP NP ( ) CC NP NP NP ( ) ::: Fundamental ::: 0.907114735222
3331459 4231117 According to the inv metric, our results are considerably worse than those reported by Ringger et al. (2004) ::: According to the inv metric , our results are considerably worse than those reported by Ringger et al. ( ) ::: VVG TO DT NN JJ , PP$ NNS VBP RB JJR IN DT VVN IN NP NP NP ( ) ::: Compare ::: 0.921954445729
4231145 5122856 We carried out automatic evaluation of our summaries using ROUGE (Lin, 2004) toolkit, which has been widely adopted by DUC for automatic summarization evaluation ::: We carried out automatic evaluation of our summaries using ROUGE ( ) toolkit, which has been widely adopted by DUC for automatic summarization evaluation ::: PP VVD RP JJ NN IN PP$ NNS VVG NP ( ) NN , WDT VHZ VBN RB VVN IN NP IN JJ NN NN ::: Fundamental ::: 0.930949336251
346030 4231023 To prepare this corpus for analysis, we extracted the body text from each of the 4.1 million entries in the corpus and applied a maximum-entropy algorithm to identify sentence boundaries (Reynar and Ratnaparkhi, 1997) ::: To prepare this corpus for analysis , we extracted the body text from each of the 4.1 million entries in the corpus and applied a maximum-entropy algorithm to identify sentence boundaries ( ) ::: TO VV DT NN IN NN , PP VVD DT NN NN IN DT IN DT CD CD NNS IN DT NN CC VVD DT NN NN TO VV NN NNS ( ) ::: Fundamental ::: 0.935692702405
695121 5658172 Freely available tools like Weka (Witten and Frank, 1999) allow easy experimentation with common learning algorithms (Hendrickx et al., 2007) ::: Freely available tools like Weka ( ) allow easy experimentation with common learning algorithms ( ) ::: RB JJ NNS IN NP ( ) VV JJ NN IN JJ NN NNS ( ) ::: BackGround ::: 0.738548945876
285244 4905037 Because these transcriptions are not necessarily time synched with the audio, we use the method described in Hauptmann and Witbrock (1998) to align the closed captioning to the announcersâ€™ speech ::: Because these transcriptions are not necessarily time synched with the audio , we use the method described in Hauptmann and Witbrock ( ) to align the closed captioning to the announcers' speech ::: IN DT NNS VBP RB RB NN VVN IN DT NN , PP VVP DT NN VVN IN NP CC NP ( ) TO VV DT JJ VVG TO DT NP NN ::: Fundamental ::: 0.966841563389
2165315 5658172 A leading method for utilizing context information for classification and extraction of relationships is that of patterns (Hearst, 1992; Pantel and Pennacchiotti, 2006) ::: A leading method for utilizing context information for classification and extraction of relationships is that of patterns ( ) ::: DT VVG NN IN VVG NN NN IN NN CC NN IN NNS VBZ IN/that IN NNS ( ) ::: BackGround ::: 0.862247981837
2165315 5658172 Other resources used for relationship discovery include Wikipedia (Strube and Ponzetto, 2006), thesauri or synonym sets (Turney, 2005) and domainspecific semantic hierarchies like MeSH (Rosario and Hearst, 2001) ::: Other resources used for relationship discovery include Wikipedia ( ) , thesauri or synonym sets ( ) and domain-specific semantic hierarchies like MeSH ( ) ::: JJ NNS VVN IN NN NN VVP NP ( ) , NNS CC NN NNS ( ) CC JJ JJ NNS IN NP ( ) ::: BackGround ::: 0.726483157257
2165315 5658172 Strategies were developed for discovery of multiple patterns for some specified lexical relationship (Pantel and Pennacchiotti, 2006) and for unsupervised pattern ranking (Turney, 2006) ::: Strategies were developed for discovery of multiple patterns for some specified lexical relationship ( ) and for unsuper-vised pattern ranking ( ) ::: NNS VBD VVN IN NN IN JJ NNS IN DT JJ JJ NN ( ) CC IN JJ NN NN ( ) ::: BackGround ::: 0.8
2165315 4231096 In several studies (e.g., Widdows and Dorow, 2002; Pantel et al, 2004; Davidov and Rappoport, 2006) it has been shown that relatively unsupervised and language-independent methods could be used to generate many thousands of sets of words whose semantics is similar in some sense ::: In several studies ( ) it has been shown that relatively unsupervised and language-independent methods could be used to generate many thousands of sets of words whose semantics is similar in some sense ::: IN JJ NNS ( ) PP VHZ VBN VVN IN/that RB JJ CC JJ NNS MD VB VVN TO VV JJ NNS IN NNS IN NNS WP$ NNS VBZ JJ IN DT NN ::: BackGround ::: 0.825028647325
2165315 4231096 Most related work deals with discovery of hypernymy (Hearst, 1992; Pantel et al, 2004), synonymy (Roark and Charniak, 1998; Widdows and Dorow, 2002; Davidov and Rappoport, 2006) and meronymy (Berland and Charniak, 1999) ::: Most related work deals with discovery of hypernymy ( ) , synonymy ( ) and meronymy ( ) ::: RBS VVN NN NNS IN NN IN NN ( ) , NN ( ) CC JJ ( ) ::: BackGround ::: 0.599042297873
2165315 4231096 In some recent work (Strube and Ponzetto, 2006), it has been shown that related pairs can be generated without pre-specifying the nature of the relation sought ::: In some recent work ( ) , it has been shown that related pairs can be generated without pre-specifying the nature of the relation sought ::: IN DT JJ NN ( ) , PP VHZ VBN VVN IN/that JJ NNS MD VB VVN IN VVG DT NN IN DT NN VVN ::: BackGround ::: 0.898026510134
2165315 4231096 It should be noted that some of these papers utilize language and domain-dependent preprocessing including syntactic parsing (Suchanek et al, 2006) and named entity tagging (Hasegawa et al, 2004), while others take advantage of handcrafted databases such as WordNet (Moldovan et al, 2004; Costello et al, 2006) and Wikipedia (Strube and Ponzetto, 2006) ::: It should be noted that some of these papers utilize language and domain-dependent preprocessing including syntactic parsing ( ) and named entity tagging ( ) , while others take advantage of handcrafted databases such as WordNet ( ) and Wikipedia ( ) ::: PP MD VB VVN IN/that DT IN DT NNS VV NN CC JJ NN VVG JJ VVG ( ) CC VVN NN VVG ( ) , IN NNS VVP NN IN VVN NNS JJ IN NP ( ) CC NP ( ) ::: BackGround ::: 0.687523872771
2165315 4231096 We do this as follows, essentially implementing a simplified version of the method of Davidov and Rappoport (2006) ::: We do this as follows , essentially implementing a simplified version of the method of Davidov and Rappoport ( ) ::: PP VVP DT RB VVZ , RB VVG DT VVN NN IN DT NN IN NP CC NP ( ) ::: Fundamental ::: 0.929320377285
2165315 4231096 It was shown in (Davidov and Rappoport, 2006) that pairs of words that often appear together in such symmetric patterns tend to belong to the same class (that is, they share some notable aspect of their semantics) ::: It was shown in ( ) that pairs of words that often appear together in such symmetric patterns tend to belong to the same class (that is , they share some notable aspect of their semantics) ::: PP VBD VVN IN ( ) DT NNS IN NNS WDT RB VVP RB IN JJ JJ NNS VVP TO VV TO DT JJ NN NN VBZ , PP VVP DT JJ NN IN PP$ NN ::: BackGround ::: 0.939336436628
2165315 4231096 Note that our method differs from that of Davidov and Rappoport (2006) in that here we provide an initial seed pair, representing our target concept, while there the goal is grouping of as many words as possible into concept classes ::: Note that our method differs from that of Davidov and Rappoport ( ) in that here we provide an initial seed pair , representing our target concept , while there the goal is grouping of as many words as possible into concept classes ::: NN IN/that PP$ NN VVZ IN DT IN NP CC NP ( ) IN IN/that RB PP VVP DT JJ NN NN , VVG PP$ NN NN , IN RB DT NN VBZ VVG IN IN JJ NNS IN JJ IN NN NNS ::: Compare ::: 0.972845605134
1383708 4231183 Among the few currently available manually sense-annotated corpora for WSD, the SEMCOR (SC) corpus (Miller et al., 1994) is the most widely used ::: Among the few currently available manually sense-annotated corpora for WSD , the SEMCOR (SC) corpus ( ) is the most widely used ::: IN DT JJ RB JJ RB JJ NNS IN NP , DT NP NN NN ( ) VBZ DT RBS RB VVN ::: BackGround ::: 0.901387818866
5442340 5658172 In (Davidov and Rappoport, 2008) we present an approach to extract pattern clusters from an untagged corpus ::: In ( ) we present an approach to extract pattern clusters from an untagged corpus ::: IN ( ) PP VVP DT NN TO VV NN NNS IN DT JJ NN ::: BackGround ::: 0.845154254729
5442340 5658172 In (Davidov and Rappoport, 2008) we describe the algorithm at length, discuss its behavior and parameters in detail, and evaluate its intrinsic quality ::: In ( ) we describe the algorithm at length , discuss its behavior and parameters in detail , and evaluate its intrinsic quality ::: IN ( ) PP VVP DT NN IN NN , VV PP$ NN CC NNS IN NN , CC VV PP$ JJ NN ::: BackGround ::: 0.907442510801
81067 4905037 Recent work in automatic image annotation (Barnard et al., 2003; Blei and Jordan, 2003) and natural language processing (Steyvers et al., 2004), however, have demonstrated the advantages of using hierarchical Bayesian models for related tasks ::: Recent work in automatic image annotation ( ) and natural language processing ( ) , however , have demonstrated the advantages of using hierarchical Bayesian models for related tasks ::: JJ NN IN JJ NN NN ( ) CC JJ NN NN ( ) , RB , VHP VVN DT NNS IN VVG JJ NP NNS IN JJ NNS ::: BackGround ::: 0.746003846592
2371752 4930784 Table 2 shows results of these three systems on the Conll-2005 task, plus the top-performing system (Punyakanok et al., 2005) for reference ::: Table 2 shows results of these three systems on the Conll-2005 task , plus the top-performing system ( ) for reference ::: NN CD VVZ NNS IN DT CD NNS IN DT NP NN , CC DT JJ NN ( ) IN NN ::: Fundamental ::: 0.895273785786
4640573 5658172 Common choices include variations of SVM (Girju et al., 2004; Nastase et al., 2006), decision trees and memory-based learners ::: Common choices include variations of SVM ( ) , decision trees and memory-based learners ::: JJ NNS VVP NNS IN NP ( ) , NN NNS CC JJ NNS ::: BackGround ::: 0.679366220487
688622 4231125 Thus for instance, both REALPRO (Lavoie and Rambow, 1997) and SURGE (Elhadad and Robin, 1999) assume that the input associates semantic literals with low level syntactic and lexical information mostly leaving the realiser to just handle inflection, word order, insertion of grammatical words and agreement ::: Thus for instance , both REALPRO ( ) and Surge ( ) assume that the input associates semantic literals with low level syntactic and lexical information mostly leaving the realiser to just handle inflection , word order , insertion of grammatical words and agreement ::: RB IN NN , CC NP ( ) CC NN ( ) VV IN/that DT NN NNS JJ NNS IN JJ NN JJ CC JJ NN RB VVG DT NN TO RB VV NN , NN NN , NN IN JJ NNS CC NN ::: BackGround ::: 0.897303170097
