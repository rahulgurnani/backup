Following the notation in section 2.1 , the ij-th entry of the matrix W is defined as in CITATION Wj = (1 - e Wz ) where Cj is the total number of words in the j-th sentence pair
For the simple bag-of-word bilingual LSA as described in Section 2.2.1 , after SVD on the sparse matrix using the toolkit SVDPACK CITATION , all source and target words are projected into a low-dimensional (R = 88) LSA-space
Discriminative word alignment models , such as Ittycheriah and Roukos CITATION; Moore CITATION; Blunsom and Cohn CITATION , have received great amount of study recently
For instance , the 1 most relaxed IBM Model-1 , which assumes that any source word can be generated by any target word equally regardless of distance , can be improved by demanding a Markov process of alignments as in HMM-based models CITATION , or implementing a distribution of number of target words linked to a source word as in IBM fertility-based models CITATION
It can be applied to complicated models such IBM Model-4 CITATION
The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing CITATION using all English sentences in the parallel training data
LSA has been successfully applied to information retrieval CITATION , statistical langauge modeling CITATION and etc
Alternative constructions of the matrix are possible using raw counts or TF-IDF CITATION
It has been shown that human knowledge , in the form of a small amount of manually annotated parallel data to be used to seed or guide model training , can significantly improve word alignment F-measure and translation performance CITATION
Our decoder is a phrase-based multi-stack imple-mentation of the log-linear model similar to Pharaoh CITATION
Since Arabic is a morphologically rich language where affixes are attached to stem words to indicate gender , tense , case and etc , in order to reduce vocabulary size and address out-of-vocabulary words , we split Arabic words into affix and root according to a rule-based segmentation scheme CITATION with the help from the Buckwalter analyzer CITATION output
Basic models in two translation directions are trained simultaneously where statistics of two directions are shared to learn symmetric translation lexicon and word alignments with high precision motivated by CITATION and CITATION
In CITATION , bilingual semantic maps are constructed to guide word alignment
As formulated in the competitive linking algorithm CITATION , the problem of word alignment can be regarded as a process of word linkage disambiguation , that is , choosing correct associations among all competing hypothesis
The example demos that due to reasonable constraints placed in word alignment training , the link to "_tK" is corrected and consequently we have accurate word translation for the Arabic singleton 7 Heuristics based on co-occurrence analysis , such as point-wise mutual information or Dice coefficients  , have been shown to be indicative for word alignments CITATION
These feature weights are tuned on the dev set to achieve optimal translation performance using downhill simplex method CITATION
By combining word alignments in two directions using heuristics CITATION , a single set of static word alignments is then formed
We simply modify the GIZA++ toolkit CITATION by always weighting lexicon probabilities with soft constraints during iterative model training , and obtain 0.7% TER reduction on both sets and 0.4% BLEU improvement on the test set
We measure translation performance by the BLEU score CITATION and Translation Error Rate (TER) CITATION with one reference for each hypothesis
Toutanova et al. CITATION augmented bilingual sentence pairs with part-of-speech tags as linguistic constraints for HMM-based word alignments
While word alignments can help identifying semantic relations CITATION , we proceed in the reverse direction
We shall take HMM-based word alignment model CITATION as an example and follow the notation of CITATION
Our baseline word alignment model is the word-to-word Hidden Markov Model CITATION
Many state-of-the-art SMT systems do not use trees and base the ordering decisions on surface phrases CITATION
An important advantage of our model is that it is global , and does not decompose the task of ordering a target sentence into a series of local decisions , as in the recently proposed order models for Machine Transition CITATION
Alternatively , order is modelled in terms of movement of automatically induced hierarchical structure of sentences CITATION
These N-best lists are generated using approximate search and simpler models , as in the re-ranking approach of CITATION
tings , even for a bi-gram language model CITATION
9 The advantages of modeling how a target language syntax tree moves with respect to a source language syntax tree are that (i) we can capture the fact that constituents move as a whole and generally respect the phrasal cohesion constraints CITATION , and (ii) we can model broad syntactic reordering phenomena , such as subject-verb-object constructions translating into subject-object-verb ones , as is generally the case for English and Japanese
Previous work has shown that it is useful to model target language order in terms of movement of syntactic constituents in constituency trees CITATION or dependency trees CITATION , which are obtained using a parser trained to determine linguistic constituency
Our results show that combining features derived from the source and target dependency trees , distortion surface order-based features (like the distortion used in Pharaoh CITATION) and language model-like features results in a model which significantly outperforms models using only some of the information sources
Pharaoh DISP: Displacement as used in Pharaoh CITATION
These models are combined as feature functions in a (log)linear model for predicting a target sentence given a source sentence , in the framework proposed by CITATION
The target dependency trees are obtained through projection of the source dependency trees , using the word alignment (we use GIZA++ CITATION) , ensuring better parallelism of the source and target structures
The sentences were annotated with alignment (using GIZA++ CITATION) and syntactic dependency structures of the source and target , obtained as described in Section 2
Our model is discriminatively trained to select the best order (according to the BLEU measure) CITATION of an unordered target dependency tree from the space of possible orders
Our algorithm for obtaining target dependency trees by projection of the source trees via the word alignment is the one used in the MT system of CITATION
It follows the order model defined in CITATION
Our baseline SMT system is the system of Quirk et al. CITATION
The projection algorithm of CITATION defines heuristics for each of these problems
1 Previous studies have shown that if both the source and target dependency trees represent linguistic constituency , the alignment between subtrees in the two languages is very complex CITATION
A host of discriminative methods have been introduced CITATION
2 We also investigated extraction-specific metrics: the frequency of interior nodes - a measure of how often the alignments violate the constituent structure of English parses - and a variant of the CPER metric of Ayan and Dorr CITATION
* From Ayan and Dorr CITATION , grow-diag-final heuristic
5 Similarly , we compared our Chinese results to the GIZA++ results in Ayan and Dorr CITATION
Additionally , we evaluated our model with the transducer analog to the consistent phrase error rate (CPER) metric of Ayan and Dorr CITATION
Like the classic IBM models CITATION , our model will introduce a latent alignment vector a = {a 1  ,... ,a J } that specifies the position of an aligned target word for each source word
However , few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them CITATION
Syntactic methods are an increasingly promising approach to statistical machine translation , being both algorithmically appealing CITATION and empirically successful CITATION
Daume III and Marcu CITATION employs a syntax-aware distortion model for aligning summaries to documents , but condition upon the roots of the constituents that are jumped over during a transition , instead ofthose that are visited during a walk through the tree
Our transductive learning algorithm , Algorithm 1 , is inspired by the Yarowsky algorithm CITATION
Under certain precise conditions , as described in CITATION , we can analyze Algorithm 1 as minimizing the entropy of the distribution over translations of U
We used the following scoring functions in our experiments: Length-normalized Score: Each translated sentence pair (t , s) is scored according to the model probability p (t | s) normalized by the length |t| of the target sentence: Score(t , s)  = p (t | s) 1*1 (3) Confidence Estimation: The confidence estimation which we implemented follows the approaches suggested in CITATION: The confidence score of a target sentence t is calculated as a log-linear combination of phrase posterior probabilities , Levenshtein-based word posterior probabilities , and a target language model score
One language pair creates data for another language pair and can be naturally used in a CITATION-style co-training algorithm
These lists are rescored with the following models: (a) the different models used in the decoder which are described above , (b) two different features based on IBM Model l CITATION , (c) posterior probabilities for words , phrases , n-grams , and sentence length CITATION , all calculated over the N-best list and using the sentence probabilities which the baseline system assigns to the translation hypotheses
In CITATION , a generative model for word alignment is trained using unsupervised learning on parallel text
In CITATION co-training is applied to MT
Along similar lines , CITATION combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences
BLEU score using the algorithm described in CITATION
The models (or features) which are employed by the decoder are: (a) one or several phrase table(s) , which model the translation direction p (s 1 1) , (b) one or several n-gram language model(s) trained with the SRILM toolkit CITATION; in the experiments reported here , we used 4-gram models on the NIST data , and a trigram model on EuroParl , (c) a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase , and (d) a word penalty
for a detailed description see CITATION
For details , see CITATION
It overlaps with the original phrase tables , but also contains many new phrase pairs CITATION
Self-training for SMT was proposed in CITATION
Recently , Cabezas and Resnik CITATION experimented with incorporating WSD translations into Pharaoh , a state-of-the-art phrase-based MT system CITATION
The relatively small improvement reported by Cabezas and Resnik CITATION without a statistical significance test appears to be inconclusive
Note that comparing with the MT systems used in CITATION and CITATION , the Hiero system we are using represents a much stronger baseline MT system upon which the WSD system must improve
Carpuat and Wu CITATION integrated the translation predictions from a Chinese WSD system CITATION into a Chinese-English word-based statistical MT system using the ISI ReWrite decoder CITATION
Note that the experiments in CITATION did not use a state-of-the-art MT system , while the experiments in CITATION were not done using a full-fledged MT system and the evaluation was not on how well each source sentence was translated as a whole
We obtain accuracy that compares favorably to the best participating system in the task CITATION
For our experiments , we use the SVM implementation of CITATION as it is able to work on multi-class problems to output the classification probability for each class
Capitalizing on the strength of the phrase-based approach , Chiang CITATION introduced a hierarchical phrase-based statistical MT system , Hiero , which achieves significantly better translation performance than Pharaoh CITATION , which is a state-of-the-art phrase-based statistical MT system
In this paper , we successfully integrate a state-of-the-art WSD system into the state-of-the-art hierarchical phrase-based MT system , Hiero CITATION
Hiero CITATION is a hierarchical phrase-based model for statistical machine translation , based on weighted synchronous context-free grammar (CFG) CITATION
Similar to CITATION , we trained the Hiero system on the FBIS corpus , used the NIST MT 2002 evaluation test set as our development set to tune the feature weights , and the NIST MT 2003 evaluation test set as our test data
Following CITATION , we used the version 11a NIST BLEU script with its default settings to calculate the BLEU scores CITATION based on case-insensitive n-gram matching , where n is up to 4
A n-gram language model adds a dependence on (n 1) neighboring target-side words CITATION , making decoding much more difficult but still polynomial; in this paper , we add features that depend on the neighboring source-side words , which does not affect decoding complexity at all because the source string is fixed
The improvement of 0.57 is statistically significant at p  < 0.05 using the sign-test as described by Collins et al. CITATION , with 374 (+1) , 318 ( 1) and 227 (0)
To perform translation , state-of-the-art MT systems use a statistical phrase-based approach CITATION by treating phrases as the basic units of translation
The word alignments of both directions are then combined into a single set of alignments using the "diag-and" method of Koehn et al. CITATION
Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results CITATION
Our implemented WSD classifier uses the knowledge sources of local collocations , parts-of-speech (POS) , and surrounding words , following the successful approach of CITATION
First , we performed word alignment on the FBIS parallel corpus using GIZA++ CITATION in both directions
Hiero uses a general log-linear model CITATION where the weight of a derivation  D for a particular source sentence and its translation is where  ^  i is a feature function and  X i is the weight for feature  ^  i
Using the MT 2002 test set , we ran the minimum-error rate training (MERT) CITATION with the decoder to tune the weights for each feature
the English portion of the FBIS corpus and the Xinhua portion of the Gigaword corpus , we trained a tri-gram language model using the SRI Language Modelling Toolkit CITATION
WSD approaches can be classified as (a) knowledge-based approaches , which make use of linguistic knowledge , manually coded or extracted from lexical resources CITATION; (b) corpus-based approaches , which make use of shallow knowledge automatically acquired from corpus and statistical or machine learning algorithms to induce disambiguation models CITATION; and (c) hybrid approaches , which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge CITATION
Although it has been argued that WSD does not yield better translation quality than a machine translation system alone , it has been recently shown that a WSD module that is developed following specific multilingual requirements can significantly improve the performance of a machine translation system CITATION
Finally , MC-WSD CITATION is a multi-class averaged perceptron classifier using syntactic and narrow context features , with one component trained on the data provided by Senseval and other trained on WordNet glosses
It is an interesting approach to learning which has been considered promising for several applications in natural language processing and has been explored for a few of them , namely POS-tagging , grammar acquisition and semantic parsing CITATION
For example , Dang and Palmer CITATION also use a rich set of features with a traditional learning algorithm (maximum entropy)
Linguistic knowledge is available in electronic resources suitable for practical use , such as WordNet CITATION , dictionaries and parsers
There is not always a direct relation between the possible senses for a word in a (monolingual) lexicon and its translations to a particular language , so this represents a different task to WSD against a (monolingual) lexicon CITATION
CLaC1 CITATION uses a Naive Bayes algorithm with a dynamically adjusted context window around the target word
The sense with the highest count of overlapping words in its dictionary definition and in the sentence containing the target verb (excluding stop words)   CITATION ,   represented by has_overlapping(sentence , translation) : has_overlapping(snt 1 , voltar)
Verbs and possible senses in our corpus Both corpora were lemmatized and part-of-speech (POS) tagged using Minipar CITATION and Mxpost CITATION , respectivelly
These approaches have shown good results; particularly those using supervised learning CITATION
WSD systems have generally been more successful in the disambiguation of nouns than other grammatical categories CITATION
Syntalex-3 CITATION is based on an ensemble of bagged decision trees with narrow context part-of-speech features and bigrams
This is achieved using Inductive Logic Programming (ILP) CITATION , which has not yet been applied to WSD
Inductive Logic Programming CITATION employs techniques from Machine Learning and Logic Programming to build first-order theories from examples and background knowledge , which are also represented by first-order clauses
2. A more specific clause (the bottom clause) is built using inverse entailment CITATION , generally consisting of the representation of all the knowledge about that example
This corpus was automatically annotated with the translation of the verb using a tagging system based on parallel corpus , statistical information and translation dictionaries CITATION , followed by a manual revision
All the knowledge sources were made available to be used by the inference engine , since previous experiments showed that they are all relevant CITATION
We use the Aleph ILP system CITATION , which provides a complete inference engine and can be customized in various ways
In the hybrid approaches that have been explored so far , deep knowledge , like selectional preferences , is either pre-processed into a vector representation to accommodate machine learning algorithms , or used in previous steps to filter out possible senses e.g. CITATION
Roark and Bacchiani CITATION showed that weighted count-merging is a special case of maximum a posteriori (MAP) estimation , and successfully used it for probabilistic context-free grammar domain adaptation CITATION and language model adaptation CITATION
We have recently shown that this algorithm is effective in estimating the sense priors of a set of nouns CITATION
However , in CITATION , we showed that in a supervised setting where one has access to some annotated training data , the EM-based method in section 5 estimates the sense priors more effectively than the method described in CITATION
A similar work is the recent research by Chen et al. CITATION , where active learning was used successfully to reduce the annotation effort for WSD of 5 English verbs using coarse-grained evaluation
This is slightly higher than the 5.8 senses per verb in CITATION , where the experiments were conducted using coarse-grained evaluation
For WSD , Fujii et al. CITATION used selective sampling for a Japanese language WSD system , Chen et al. CITATION used active learning for 5 verbs using coarse-grained evaluation , and H
Dang CITATION employed active learning for another set of 5 verbs
To investigate this , Escudero et al. CITATION and Martinez and Agirre CITATION conducted experiments using the DSO corpus , which contains sentences from two different corpora , namely Brown Corpus (BC) and Wall Street Journal (WSJ)
Escudero et al. CITATION pointed out that one of the reasons for the drop in accuracy is the difference in sense priors (i.e. , the proportions of the different senses of a word) between BC and WSJ
Following the setup of CITATION , we similarly made use of the DSO corpus to perform our experiments on domain adaptation
As mentioned in section 1 , research in CITATION noted an improvement in accuracy when they adjusted the BC and WSJ datasets such that the proportions of the different senses of each word were the same between BC and WSJ
Escudero et al. CITATION used the DSO corpus to highlight the importance of the issue of domain dependence of WSD systems , but did not propose methods such as active learning or count-merging to address the specific problem of how to perform domain adaptation for WSD
This is similar to the approach taken in CITATION where they focus on determining the predominant sense of words in corpora drawn from finance versus sports domains
Research by McCarthy et al. CITATION and Koeling et al. CITATION pointed out that a change of predominant sense is often indicative of a change in domain
These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work CITATION
To reduce the effort required to adapt a WSD system to a new domain , we employ an active learning strategy CITATION to select examples to annotate from the new domain of interest
With active learning CITATION , we use uncertainty sampling as shown r    WSD system trained on D T b    word sense prediction for d using r  p    confidence of prediction b if p < p  min then Figure 1: Active learning in Figure 1
The WordNet Domains resource CITATION assigns domain labels to synsets in WordNet
Among the few currently available manually sense-annotated corpora for WSD , the SEMCOR (SC) corpus CITATION is the most widely used
The DSO corpus CITATION contains 192 ,800 annotated examples for 121 nouns and 70 verbs , drawn from BC and WSJ
In this section , we describe an EM-based algorithm that was introduced by Saerens et al. CITATION , which can be used to estimate the sense priors , or a priori probabilities of the different senses in a new dataset
Most of this section is based on CITATION
In applying active learning for domain adaptation , Zhang et al. CITATION presented work on sentence boundary detection using generalized Winnow , while Tur et al. CITATION performed language model adaptation of automatic speech recognition systems
In most contexts , the similarity between chocolate , say , and a narcotic like heroin will mea-gerly reflect the simple ontological fact that both are kinds of substances; certainly , taxonomic measures of similarity as discussed in Budanitsky and Hirst CITATION will capture little more than this commonality
The function (%sim arg 0 CAT) reflects the perceived similarity between the putative member arg 0 and a synset CAT in WordNet , using one of the standard formulations described in Budanitsky and Hirst CITATION
Whissell CITATION reduces the notion of affect to a single numeric dimension , to produce a dictionary of affect that associates a numeric value in the range 1.0 (most unpleasant) to 3.0 CITATION
We have described an approach that can be seen as a functional equivalent to the CPA (Corpus Pattern Analysis) approach of Pustejovsky et al. CITATION , in which our goal is not that of automated induction of word senses in context (as it is in CPA) but the automated induction of flexible , context-sensitive category structures
Since the line between literal and metaphoric uses of a category is often impossible to draw , the best one can do is to accept metaphor as a gradable phenomenon CITATION
The most revealing variations are syntagmatic in nature , which is to say , they look beyond individual word forms to larger patterns of contiguous usage CITATION
Dice's coefficient CITATION is used to implement this measure
As noted by De Leenheer and de Moor CITATION , ontologies are lexical representations of concepts , so we can expect the effects of context on language use to closely reflect the effects of context on ontolog-Linguistic variation across contexts is often symp- ical structure
While simile is a mechanism for highlighting inter-concept similarity , metaphor is at heart a mechanism of category inclusion CITATION
Glucksberg CITATION notes that the same category , used figuratively , can exhibit different qualities in different metaphors
In this section , we describe how we use Markov chain Monte Carlo methods to perform inference in the statistical models described in the previous section; Andrieu et al. CITATION provide an excellent introduction to MCMC techniques
These are short statements that restrict the space of languages in a concrete way (for instance "object-verb ordering implies adjective-noun ordering"); Croft CITATION , Hawkins CITATION and Song CITATION provide excellent introductions to linguistic typology
This is a well-documented issue (see , eg. , CITATION) stemming from the fact that any set of languages is not sampled uniformly from the space of all probable languages
The closest work is represented by the books Possible and Probable Languages CITATION and Language Classification by Numbers CITATION , but the focus of these books is on automatically discovering phylogenetic trees for languages based on Indo-European cognate sets CITATION
Those that reference Hawkins (eg. , #11) are based on implications described by Hawkins CITATION; those that reference Lehmann are references to the principles decided by Lehmann CITATION in Ch 4 & 8
They have also been used computationally to aid in the learning of unsupervised part of speech taggers CITATION
For instance our #7 is implication #18 from Greenberg , reproduced by Song CITATION
We examined sentences using a phrase structure parser CITATION and an HPSG parser CITATION
Since the number of parameters in NLM is still large , several smoothing methods are used CITATION to produce more accurate probabilities , and to assign nonzero probabilities to any word string
We would like to see more refined online learning methods with kernels CITATION that we could apply in these areas
Therefore we make use of an online learning algorithm proposed by CITATION , which has a much smaller computational cost
Blei , 2003; Wang et al. , 2005) , our result may encourage the study ofthe combination offeatures forlanguage modeling
We used a Viterbi decoding CITATION for the partition
Discriminative language models (DLMs) have been proposed to classify sentences directly as correct or incorrect CITATION , and these models can handle both non-local and overlapping information
For fast kernel computation , the Polynomial Kernel Inverted method (PKI)) is proposed CITATION , which is an extension of Inverted Index in Information Retrieval
The class model was originally proposed by CITATION
However , by considering only those counts that actually change , the algorithm can be made to scale somewhere between linearly and quadratically to the number of classes CITATION
Recently , Whole Sentence Maximum Entropy Models CITATION (WSMEs) have been introduced
In our experiments , we did not examine the result of using other sampling methods , For example , it would be possible to sample sentences from a whole sentence maximum entropy model CITATION and this is a topic for future research
A contrastive estimation method CITATION is similar to ours with regard to constructing pseudo-negative examples
If the kernel-trick CITATION is applied to online margin-based learning , a subset of the observed examples , called the active set , needs to be stored
It should be noted that models based on finite state transducers have been shown to be adequate for describing fusion as wellCITATION , and further work should evaluate these types of models in ASR of languages with higher indexes of fusion
The final approach applies a manually constructed rule-based morphological taggerCITATION
For training the LMs , a subset of 43 million words from the Estonian Segakorpus was usedCITATION , preprocessed with a morphological analyzerCITATION
In CITATION a WER of 44.5% was obtained with word-based trigrams and a WER of 37.2% with items similar to ones from "grammar" using the same speech corpus as in this work
It should be noted that every OOV causes roughly two errors in recognition , and vocabulary decomposition approaches such as the ones evaluated here give some benefits to word error rate (WER) even in recognizing languages such as EnglishCITATION
This is similar to what was introduced as "flat hybrid model"CITATION , and it tries to model OOV-words as sequences of words and fragments
The results for "hybrid" are in in the range suggested by earlier workCITATION
The morph approach was developed for the needs of Finnish speech recognition , which is a high synthesis , moderate fusion and very low orthographic irregularity language , whereas the hybrid approach in CITATION was developed for English , which has low synthesis , moderate fusion , and very high orthographic irregularity
VarigramsCITATION are used in this work , and to make LMs trained with each approach comparable , the varigrams have been grown to roughly sizes of 5 million counts
ing approach , growing varigram modelsCITATION were used with no limits as to the order of n-grams , but limiting the number of counts to 4.8 and 5 million counts
For example , in English with language models (LM) of 60k words trained from the Gigaword Corpus V.2CITATION , and testing on a very similar Voice of America -portion of TDT4 speech corporaCITATION , this gives a OOV rate of 1.5%
Models of this type have previously been shown to yield very good g2p conversion results CITATION
It has been argued that using morphological information is important for languages where morphology has an important influence on pronunciation , syllabiication and word stress such as German , Dutch , Swedish or , to a smaller extent , also English CITATION
Decision trees were one of the first data-based approaches to g2p and are still widely used CITATION
Best results were obtained when using a variant of Modified Kneser-Ney Smoothing 2 CITATION
CITATION also used a joint n-gram model
We compared four different state-of-the-art unsu-pervised systems for morphological decomposition (cf. CITATION)
In very recent work , CITATION developed an unsupervised algorithm (f-meas: 68%; an extension of RePortS) whose segmentations improve g2p when using a the decision tree (PER: 3.45%)
The German corpus used in these experiments is CELEX CITATION
Among the unsupervised systems , best results 7 on the g2p task with morphological annotation were obtained with the RePortS system CITATION
The same algorithms have previously been shown to help a speech recognition task CITATION
The joint n-gram model performs significantly better than the decision tree (essentially based on CITATION) , and achieves scores comparable to the Pronunciation by Analogy (PbA) algorithm CITATION
This is much faster than the times for Pronunciation by Analogy (PbA) CITATION on the same corpus
Examples of such approaches using Hidden Markov Models are CITATION (who applied the HMM to the related task of phoneme-to-grapheme conversion) , CITATION and CITATION
For German , CITATION show that information about stress assignment and the position of a syllable within a word improve g2p conversion
Vowel length and quality has been argued to also depend on morphological structure CITATION
The two rule-based systems we evaluated , the ETI 4 morphological system and SMOR 5 CITATION , are both high-quality systems with large lexica that have been developed over several years
We used the syllabifier described in CITATION , which works similar to the joint n-gram model used for g2p conversion
A possible reason for the observed dichotomy in the behavior of the vowel and consonant inventories with respect to redundancy can be as follows: while the organization of the vowel inventories is known to be governed by a single force - the maximal perceptual contrast CITATION) , consonant inventories are shaped by a complex interplay of several forces CITATION
It has been postulated earlier by functional phonologists that such regularities are the consequences of certain general principles like maximal perceptual contrast CITATION , which is desirable between the phonemes of a language for proper perception of each individ-ual phoneme in a noisy environment , ease of articulation CITATION , which requires that the sound systems of all languages are formed of certain universal (and highly frequent) sounds , and ease of learnabilityCITATION , which is necessary for a speaker to learn the sounds of a language with minimum effort
Such an observation is significant since whether or not these principles are similar/different for the two inventories had been a question giving rise to perennial debate among the past researchers CITATION
On the other hand , in spite of several attempts CITATION the organization of the consonant inventories lacks a satisfactory explanation
Various attempts have been made in the past to explain the aforementioned trends through linguistic insights CITATION mainly establishing their statistical significance
For instance , in biological systems we find redundancy in the codons CITATION , in the genes CITATION and as well in the proteins CITATION
In fact , the organization of the vowel inventories (especially those with a smaller size) across languages has been satisfactorily explained in terms of the single principle of maximal perceptual contrast CITATION
This redundancy is present mainly to reduce the risk of the complete loss of information that might occur due to accidental errors CITATION
Many typological studies CITATION of segmental inventories have been carried out in past on the UCLA Phonological Segment Inventory Database (UPSID) CITATION
In order to explain these trends , feature economy was proposed as the organizing principle of the consonant inventories CITATION
Inspired by the aforementioned studies and the concepts of information theory CITATION we try to quantitatively capture the amount of redundancy found across the consonant Table 1: The table shows four plosives
For this purpose , we present an information theoretic definition of redundancy , which is calculated based on the set of features 1 CITATION that are used to express the consonants
However , one of the earliest observations about the consonant inventories has been that consonants tend to occur in pairs that exhibit strong correlation in terms of their features CITATION
2 Previous Work Previous work   e.g. CITATION   has mostly assumed that one has a training lexicon of transliteration pairs , from which one can learn a model , often a source-channel or MaxEnt-based model
A linear classifier is trained using the Winnow algorithm from the SNoW toolkit CITATION
Using comparable corpora , the named-entities for persons and locations were extracted from the English text; in this paper , the English named-entities were extracted using the named-entity recognizer described in Li et al. CITATION , based on the SNoW machine learning toolkit CITATION
This is quite small compared to previous approaches such as Knight and Graehl CITATION or Gao et al. CITATION
Gildea and Jurafsky CITATION counted the number of features whose values are different , and used them as a substitution cost
Halle and Clements CITATION's distinctive features are used in order to model the substitution/ insertion/deletion costs for the string-alignment algorithm and linear classifier
All pronunciations are based on the WorldBet transliteration system CITATION , an ascii-only version of the IPA
a. For all the training data , the pairs of pronunciations are aligned using standard string alignment algorithm based on Kruskal CITATION
For the set of features X and set of weights W , the linear classifier is defined as [1] CITATION X  = { X [ , X2 , ..
In this paper , the phonetic transliteration is performed using the following steps: 1)  Generation of the pronunciation for English words and target words: a. Pronunciations for English words are obtained using the Festival text-to-speech system CITATION
Based on the pronunciation error data of learners of English as a second language as reported in CITATION , we propose the use of what we will term pseudofeatures
Examples of the top-3 candidates in the transliteration of English-Korean To evaluate the proposed transliteration methods quantitatively , the Mean Reciprocal Rank (MRR) , a measure commonly used in information retrieval when there is precisely one correct answer CITATION was measured , following Tao and Zhai CITATION
In our work , we adopt the method proposed in CITATION and apply it to the problem of transliteration
The substitution/insertion/deletion cost for the string alignment algorithm is based on the baseline cost from CITATION
The pseudo features in this study are same as in Tao et al. CITATION
MRRs of the phonetic transliteration The baseline was computed using the phonetic transliteration method proposed in Tao et al. CITATION
By treating a letter/character as a word and a group of letters/characters as a phrase or token unit in SMT , one can easily apply the traditional SMT models , such as the IBM generative model CITATION or the phrase-based translation model CITATION to transliteration
In G2P studies , Font Llitjos and Black CITATION showed how knowledge of language of origin may improve conversion accuracy
Phonetic transliteration can be considered as an extension to the traditional grapheme-to-phoneme (G2P) conversion CITATION , which has been a much-researched topic in the field of speech processing
Many of the loanwords exist in today's Chinese through semantic transliteration , which has been well received CITATION by the people because of many advantages
Unfortunately semantic transliteration , which is considered as a good tradition in translation practice CITATION , has not been adequately addressed computationally in the literature
5.S Semantic Transliteration The performance was measured using the Mean Reciprocal Rank (MRR) metric CITATION , a measure that is commonly used in information retrieval , assuming there is precisely one correct answer
In computational linguistic literature , much effort has been devoted to phonetic transliteration , such as English-Arabic , English-Chinese CITATION , English-Japanese CITATION and English-Korean
In the extraction of transliterations , data-driven methods are adopted to extract actual transliteration pairs from a corpus , in an effort to construct a large , up-to-date transliteration lexicon CITATION
The Latin-scripted personal names are always assumed to homogeneously follow the English phonic rules in automatic transliteration CITATION
This model is conceptually similar to the joint source-channel model CITATION where the target token t i depends on not only its source token s i but also the history t i-1 and s i -1
Some recent work CITATION has attempted to introduce preference into a probabilistic framework for selection of Chinese characters in phonetic transliteration
In transliteration modeling , transliteration rules are trained from a large , bilingual transliteration lexicon CITATION , with the objective of translating unknown words on the fly in an open , general domain
As discussed elsewhere CITATION , out of several thousand common Chinese characters , a subset of a few hundred characters tends to be used overwhelmingly for transliterating English names to Chinese , e.g.only 731 Chinese characters are adopted in the E-C corpus
As a Chinese transliteration can arouse to certain connotations , the choice of Chinese characters becomes a topic of interest CITATION
Table 4: Lexicon statistics For Arabic , as a full-size Arabic lexicon was not available to us , we used the Buckwalter morphological analyzer CITATION to derive a lexicon
For example , CITATION showed that factored language models , which consider morphological features and use an optimized backoff policy , yield lower perplexity
A recent work CITATION experimented with English-to-Turkish translation with limited success , suggesting that inflection generation given morphological features may give positive results
More recently , CITATION achieved improvements in Czech-English MT , optimizing a Table 1: Morphological features used for Russian and Arabic set of possible source transformations , incorporating morphology
Ideally , the best word analysis should be provided as a result of contextual disambiguation (e.g. , CITATION); we leave this for future work
Another work CITATION showed improvements by splitting compounds in German
Translating from a morphology-poor to a morphology-rich language is especially challenging since detailed morphological information needs to be decoded from a language that does not encode this information or does so only implicitly CITATION
Koehn CITATION includes a survey of statistical MT systems in both directions for the Europarl corpus , and points out the challenges of this task
For example , it has been shown CITATION that determiner segmentation and deletion in Arabic sentences in an Arabic-to-English translation system improves sentence alignment , thus leading to improved overall translation quality
For Arabic , we apply the following heuristic: use the most frequent analysis estimated from the gold standard labels in the Arabic Treebank CITATION; if a word does not appear in the treebank , we choose the first analysis returned by the Buckwalter analyzer
Our learning framework uses a Maximum Entropy Markov model CITATION
CITATION demonstrated that a similar level of alignment quality can be achieved with smaller corpora applying morpho-syntactic source restructuring , using hierarchical lexicon models , in translating from German into English
The sentence pairs were word-aligned using GIZA++ CITATION and submitted to a treelet-based MT system CITATION , which uses the word dependency structure of the source language and projects word dependency structure to the target language , creating the structure shown in Figure 1 above
CITATION experimented successfully with translating from inflectional languages into English making use of POS tags , word stems and suffixes in the source language
The framework suggested here is most closely related to CITATION , which uses a probabilistic model to generate Japanese case markers for English-to-Japanese MT
The algorithm is similar to the one described in CITATION
It also differs from now traditional uses of comparable corpora for detecting translation equivalents CITATION or extracting terminology CITATION , which allows a one-to-one correspondence irrespective of the context
In the spirit of CITATION , it is intended as a translator's amenuensis "under the tight control of a human translator ..
It has been aligned on the sentence level by JAPA CITATION , and further on the word level by GIZA++ CITATION
Thus the present system is unlike SMT CITATION , where lexical selection is effected by a translation model based on aligned , parallel corpora , but the novel techniques it has developed are exploitable in the SMT paradigm
Similarity is measured as the cosine between collocation vectors , whose dimensionality is reduced by SVD using the implementation by Rapp CITATION
We have generalised the method used in our previous study CITATION for extracting equivalents for continuous multiword expressions (MWEs)
Recent efforts in statistical machine translation (MT) have seen promising improvements in output quality , especially the phrase-based models CITATION and syntax-based models CITATION
By adapting the k-best parsing Algorithm 2 of Huang and Chiang CITATION , it achieves significant speed-up over full-integration on Chiang's Hiero system
We also devise a faster variant of cube pruning , called cube growing , which uses a lazy version of k-best parsing CITATION that tries to reduce k to the minimum needed at each node to obtain the desired number of hypotheses at the root
In a nutshell , cube pruning works on the  LM forest , keeping at most k +LM items at each node , and uses the k-best parsing Algorithm 2 of Huang and Chiang CITATION to speed up the computation
This situation is very similar to k-best parsing and we can adapt the Algorithm 2 of Huang and Chiang CITATION here to explore this grid in a best-first order
This new method , called cube growing , is a lazy version of cube pruning just as Algorithm 3 of Huang and Chiang CITATION , is a lazy version of Algorithm 2 (see Table 1)
The different target sides then constitute a third dimension of the grid , forming a cube of possible combinations CITATION
The data set is same as in Section 5.1 , except that we also parsed the English-side using a variant of the Collins CITATION parser , and then extracted 24.7M tree-to-string rules using the algorithm of CITATION
These forest rescoring algorithms have potential applications to other computationally intensive tasks involving combinations of different models , for example , head-lexicalized parsing CITATION; joint parsing and semantic role labeling CITATION; or tagging and parsing with nonlocal features
In tree-to-string (also called syntax-directed) decoding CITATION , the source string is first parsed into a tree , which is then recursively converted into a target string according to transfer rules in a synchronous grammar CITATION
We generalize cube pruning and adapt it to two systems very different from Hiero: a phrase-based system similar to Pharaoh CITATION and a tree-to-string system CITATION
We test our methods on two large-scale English-to-Chinese translation systems: a phrase-based system and our tree-to-string system CITATION
Our data preparation follows Huang et al. CITATION: the training data is a parallel corpus of28.3M words on the English side , and a trigram language model is trained on the Chinese side
For cube growing , we use a non-duplicate k-best method CITATION to get 100-best unique translations according to   LM to estimate the lower-bound heuristics
Since our tree-to-string rules may have many variables , we first binarize each hyperedge in the forest on the target projection CITATION
Thus we envision forest rescoring as being of general applicability for reducing complicated search spaces , as an alternative to simulated annealing methods CITATION
Part of the complexity arises from the expressive power of the translation model: for example , a phrase- or word-based model with full reordering has exponential complexity CITATION
We will use the following example from Chinese to English for both systems described in this section: yU  Shalong juxing le huitan with Sharon hold   [past] meeting 'held a meeting with Sharon' A typical phrase-based decoder generates partial target-language outputs in left-to-right order in the form of hypotheses CITATION
We implemented Cubit , a Python clone of the Pharaoh decoder CITATION , 3 and adapted cube pruning to it as follows
We set the decoder phrase-table limit to 100 as suggested in CITATION and the distortion limit to 4
An SCFG CITATION is a context-free rewriting system for generating string pairs
To integrate with a bigram language model , we can use the dynamic-programming algorithms of Och and Ney CITATION and Wu CITATION for phrase-based and SCFG-based systems , respectively , which we may think of as doing a iner-grained version of the deductions above
The language model also , if fully integrated into the decoder , introduces an expensive overhead for maintaining target-language boundary words for dynamic programming CITATION
Similarly , the decoding problem with SCFGs can also be cast as a deductive (parsing) system CITATION
However , the hope is that by choosing the right value ofi , these estimates will be accurate enough to affect the search quality only slightly , which is analogous to "almost admissible" heuristics in A* search CITATION
A few exceptions are the hierarchical (possibly syntax-based) trans-duction models CITATION and the string transduction models CITATION
The SFST approach described here is similar to the one described in CITATION which has subsequently been adopted by CITATION
In preliminary experiments , we have associated the target lexical items with supertag information CITATION
We separate the most popular classification techniques into two broad categories: also called Maxent as it finds the distribution with maximum entropy that properly estimates the average of each feature over the training data CITATION
Most of the previous work on statistical machine translation , as exemplified in CITATION , employs word-alignment algorithm (such as GIZA++ CITATION) that provides local associations between source and target words
The BOW approach is different from the parsing based approaches CITATION where the translation model tightly couples the syntactic and lexical items of the two languages
The excellent results recently obtained with the SEARN algorithm CITATION also suggest that binary classifiers , when properly trained and combined , seem to be capable ofmatching more complex structured output approaches
A new L1-regularized Maxent algorithms was proposed for density estimation CITATION and we adapted it to classification
From the bilanguage corpus B , we train an n-gram language model using standard tools CITATION
The use of supertags in phrase-based SMT system has been shown to improve results CITATION
here: all state hypotheses of a whole sentence are kept in memory) , it is necessary to either use heuristic forward pruning or constrain permutations to be within a local window of adjustable size (also see CITATION)
Although Conditional Random Fields (CRF) CITATION train an exponential model at the sequence level , in translation tasks such as ours the computational requirements of training such models are prohibitively expensive
We found this algorithm to converge faster than the current state-of-the-art in Maxent training , which is L2-regularized L-BFGS CITATION 1
Discriminative training has been used mainly for translation model combination CITATION and with the exception of CITATION , has not been used to directly train parameters of a translation model
For the work reported in this paper , we have used the GIZA++ tool CITATION which implements a string-alignment algorithm
Each output label t is projected into a bit string with components b j (t) where probability of each component is estimated independently: In practice , despite the approximation , the 1-vs-other scheme has been shown to perform as well as the multiclass scheme CITATION
For the Hansard corpus we used the same training and test split as in CITATION: 1.4 million training sentence pairs and 5432 test sentences
In search of a balance between structural flexibility and computational complexity , several authors have proposed constraints to identify classes of non-projec-tive dependency structures that are computationally well-behaved CITATION
This result generalizes previous work on the relation between ltag and dependency representations CITATION
? The encoding of dependency structures as order-annotated trees allows us to reformulate two constraints on non-projectivity originally defined on fully specified dependency structures CITATION in terms of syntactic properties of the order annotations that they induce: Gap-degree The gap-degree of a dependency structure is the maximum over the number of discontinuities in any yield of that structure
This enables us to generalize a previous result on the class of dependency structures generated by lexicalized tags CITATION to the class of generated dependency languages , LTAL
Lately , they have also been used in many computational tasks , such as relation extraction CITATION , parsing CITATION , and machine translation CITATION
Unfortunately , most formal results on non-projectivity are discouraging: While grammar-driven dependency parsers that are restricted to projective structures can be as efficient as parsers for lexicalized context-free grammar CITATION , parsing is prohibitively expensive when unrestricted forms of non-projectivity are permitted CITATION
We also show that adding the well-nestedness condition corresponds to the restriction of lcfrs to Coupled Context-Free Grammars CITATION , and that regular sets of well-nested structures with a gap-degree of at most 1 are exactly the class of sets of derivations of Lexicalized Tree Adjoining Grammar (ltag)
This restriction is central to the formalism of Coupled-Context-Free Grammar (ccfg) CITATION
REGD w (k) = LCCFL(k + 1) As a special case , Coupled-Context-Free Grammars with fan-out 2 are equivalent to Tree Adjoining Grammars (tags) CITATION
This gives rise to a notion of regular dependency languages , and allows us to establish a formal relation between the structural constraints and mildly context-sensitive grammar formalisms CITATION: We show that regular dependency languages correspond to the sets of derivations of lexicalized Linear Context-Free Rewriting Systems (lcfrs) CITATION , and that the gap-degree measure is the structural correspondent of the concept of 'fan-out' in this formalism CITATION
Such a comparison may be empirically more adequate than one based on traditional notions of generative capacity CITATION
Both constraints have been shown to be in very good fit with data from dependency treebanks CITATION
A dependency structure is projective , if each of its yields forms an interval with respect to the precedence order CITATION
Data-driven dependency parsing with non-projective structures is quadratic when all attachment decisions are assumed to be independent of one another CITATION , but becomes intractable when this assumption is abandoned CITATION
The number of components in the order-annotation , and hence , the gap-degree of the resulting dependency language , corresponds to the fan-out of the function: the highest number of components among the arguments of the function CITATION
Linear Context-Free Rewriting Systems Gap-restricted dependency languages are closely related to Linear Context-Free Rewriting Systems (lcfrs) CITATION , a class of formal systems that generalizes several mildly context-sensitive grammar formalisms
The Unfold-Fold transformation is a calculus for transforming functional and logic programs into equivalent but (hopefully) faster programs CITATION
Standard methods for converting weighted CFGs to equivalent PCFGs can be used if required CITATION
Second , Eisner-Satta O(n 3) PBDG parsing algorithms are extremely fast CITATION
It is straight-forward to extend the split-head CFG to encode the additional state information required by the head automata of Eisner and Satta CITATION; this corresponds to splitting the non-terminals L u and uR
The O(n 3) split-head grammar is closely related to the O(n 3) PBDG parsing algorithm given by Eisner and Satta CITATION
Goodman CITATION observed that the Viterbi parse is in general not the optimal parse for evaluation metrics such as f-score that are based on the number of correct constituents in a parse
For example , incremental CFG parsing algorithms can be used with the CFGs produced by this transform , as can the Inside-Outside estimation algorithm CITATION and more exotic methods such as estimating adjoined hidden states CITATION
The closest related work we are aware of is McAllester CITATION , which also describes a reduction of PBDGs to efficiently-parsable CFGs and directly inspired this work
This paper investigates the relationship between Context-Free Grammar (CFG) parsing and the Eis-ner/Satta PBDG parsing algorithms , including their extension to second-order PBDG parsing CITATION
First , because they capture bilexical head-to-head dependencies they are capable of producing extremely high-quality parses: state-of-the-art dis-criminatively trained PBDG parsers rival the accuracy of the very best statistical parsers available today CITATION
The steps involved in CKY parsing with this grammar correspond closely to those of the McDonald CITATION second-order PBDG parsing algorithm
These weights are estimated by an online procedure as in McDonald CITATION , and are not intended to define a probability distribution
We provided one grammar which captures horizontal second-order dependencies CITATION , and another which captures vertical second-order head-to-head-to-head dependencies
Since CFGs can be expressed as Horn-clause logic programs CITATION and the Unfold-Fold transformation is provably correct for such programs CITATION , it follows that its application to CFGs is provably correct as well
Specifically , we show how to use an off-line preprocessing step , the Unfold-Fold transformation , to transform a PBDG into an equivalent CFG that can be parsed in O(n 3) time using a version of the CKY algorithm with suitable indexing CITATION , and extend this transformation so that it captures second-order PBDG dependencies as well
By a slight generalization of a result by Aoto CITATION , this typing r h N' : a must be negatively non-duplicated in the sense that each atomic type has at most one negative occurrence in it
By Aoto and Ono's CITATION generalization of the Coherence Theorem CITATION , it follows that every /-term P such that r' h P : a for some r' c r must be Sr-equal to N' (and consequently to N)
The reduction to Datalog makes it possible to apply to parsing and generation sophisticated evaluation techniques for Datalog queries; in particular , an application of generalized supplementary magic-sets rewriting CITATION automatically yields Earley-style algorithms for both parsing and generation
(In the case of an IO macro grammar , the result is an IO context-free tree grammar CITATION.) String copying becomes tree copying , and the resulting grammar can be represented by an almost linear CFLG and hence by a Datalog program
With regard to parsing and recognition of input strings , polynomial-time algorithms and the LOGCFL upper bound on the computational complexity are already known for the grammar formalisms covered by our results CITATION; nevertheless , we believe that our reduction to Datalog offers valuable insights
In this paper , we show that a similar reduction to Datalog is possible for more powerful grammar formalisms with "context-free" derivations , such as (multi-component) tree-adjoining grammars CITATION , IO macro grammars CITATION , and (parallel) multiple context-free grammars CITATION
By the main result of Gottlob et al. CITATION , the related search problem of finding one derivation tree for the input /-term is in functional LOGCFL , i.e. , the class of functions that can be computed by a logspace-bounded Turing machine with a LOGCFL oracle
Our method essentially relies on the encoding of different formalisms in terms of abstract categorial grammars CITATION
What we have called a context-free / -term grammar is nothing but an alternative notation for an abstract categorial grammar CITATION whose abstract vocabulary is second-order , with the restriction to linear /-terms removed
A string-generating grammar coupled with Montague semantics may be represented by a synchronous CFLG , a pair of CFLGs with matching rule sets CITATION
linear ACGs are known to be expressive enough to encode well-known mildly context-sensitive grammar formalisms in a straightforward way , including TAGs and multiple context-free grammars CITATION
For example , the linear CFLG in Figure 8 is an encoding of the TAG in Figure 3 , where a(S) = o >o and a(A) = (o  > o)  > o   > o CITATION
We can eliminate e-rules from an almost linear CFLG by the same method that Kanazawa and Yoshinaka CITATION used for linear grammars , noting that for any r and a , there are only finitely many almost linear /-terms M such that r h M : a.If a grammar has no e-rule , any derivation tree for the input /-term N that has a /-term P at its root node corresponds to a Datalog derivation tree whose number of leaves is equal to the number of occurrences of constants in P , which cannot exceed the number of occurrences of constants in N
For such P and D , it is known that {(D , q) | D eD , P U D derives q } is in the complexity class LOGCFL CITATION
3 In the linear case , Salvati CITATION has shown the recognition/parsing complexity to be PTIME , and exhibited an algorithm similar to Earley parsing for TAGs
The result of the generalized supplementary magic-sets rewriting of Beeri and Ramakrish-nan CITATION applied to the Datalog program representing a CFG essentially coincides with the deduction system CITATION or uninstantiated parsing system CITATION for Earley parsing
By naive (or seminaive) bottom-up evaluation CITATION , the answer to such a query can be computed in polynomial time in the size of the database for any Datalog program
We illustrate this approach with the program in Figure 4 , following the presentation of Ullman CITATION
But there are also other factors involved - for example , the tendency to put "given" discourse elements before "new" ones , which has been shown to play a role independent of length CITATION
First , how close is dependency length in English to that of this optimal DLA? Secondly , how similar is the optimal DLA to English in terms of the actual rules that arise? Finding linear arrangements of graphs that minimize total edge length is a classic problem , NP-complete for general graphs but with an O(n 16) algorithm for trees CITATION
Statistical parsers make use of features that capture dependency length (e.g.an adjacency feature in Collins CITATION , more explicit length features in McDonald et al. CITATION and Eisner and Smith CITATION) and thus learn to favor parses with shorter dependencies
We take sentences from the Wall Street Journal section of the Penn Treebank , extract the dependency trees using the head-word rules of Collins CITATION , consider them to be unordered dependency trees , and linearize them to minimize dependency length
Exactly this pattern has been observed by Dryer CITATION in natural languages
Frazier CITATION suggests that this might serve the function of keeping heads and dependents close together
This has been offered as an explanation for numerous psycholinguistic phenomena , such as the greater processing difficulty of object relative clauses versus subject relative clauses CITATION
Hawkins CITATION has shown that this principle is reflected in grammatical rules across many languages
One might suppose that such syntactic choices in English are guided at least partly by dependency length minimization , and indeed there is evidence for this; for example , people tend to put the shorter of two PPs closer to the verb CITATION
The problem of finding the optimum weighted DLA for a set of input trees can be shown to be NP-complete by reducing from the problem of finding a graph's minimum Feedback Arc Set , one of the 21 classic problems of Karp CITATION
This setting is reminiscent of the problem of optimizing feature weights for reranking of candidate machine translation outputs , and we employ an optimization technique similar to that used by Och CITATION for machine translation
In particular , our approach would be applicable to corpora with frame-specific role labels , e.g.FrameNet CITATION
Our work suggests that feature generalization based on verb-similarity may compliment approaches to generalization based on role-similarity CITATION
For this task we utilized the August 2005 release of the Charniak parser with the default speed/accuracy settings CITATION , which required roughly 360 hours of processor time on a 2.5 GHz PowerPC G5
To automatically identify all verb inflections , we utilized the English DELA electronic dictionary CITATION , which contained all but 21 of the PropBank verbs (for which we provided the inflections ourselves) , with old-English verb inflections removed
Parse tree paths were used for semantic role labeling by Gildea and Jurafsky CITATION as descriptive features of the syntactic relationship between predicates and their arguments in the parse tree of a sentence
In future work , it would be particularly interesting to compare empirically-derived verb clusters to verb classes derived from theoretical considerations CITATION , and to the automated verb classification techniques that use these classes CITATION
Our approach is analogous to previous work in extracting collocations from large text corpora using syntactic information CITATION
This observation further supports the distributional hypothesis of word similarity and corresponding technologies for identifying synonyms by similarity of lexical-syntactic context CITATION
In our work , we utilized the GigaWord corpus of English newswire text CITATION , consisting of nearly 12 gigabytes of textual data
Annotations similar to these have been used to create automated semantic role labeling systems CITATION for use in natural language processing applications that require only shallow semantic parsing
The overall performance of our semantic role labeling approach is not competitive with leading contemporary systems , which typically employ support vector machine learning algorithms with syntactic features CITATION or syntactic tree kernels CITATION
A recent release of the PropBank CITATION corpus of semantic role annotations of Tree-bank parses contained 112 ,917 labeled instances of 4 ,250 rolesets corresponding to 3 ,257 verbs , as illustrated by this example for the verb buy
An important area for future research will be to explore the correlation between our distance metric for syntactic similarity and various quantitative measures of semantic similarity CITATION
To prepare this corpus for analysis , we extracted the body text from each of the 4.1 million entries in the corpus and applied a maximum-entropy algorithm to identify sentence boundaries CITATION
Feature-based Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky CITATION , who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet CITATION
SVM CITATION is selected as our classifier and the one vs
In the context of it , more and more kernels for restricted syntaxes or specific domains CITATION are proposed and explored in the NLP domain
In this paper , we apply Alternating Structure Optimization (ASO) CITATION to the semantic role labeling task on NomBank
ASO has been shown to be effective on the following natural language processing tasks: text categorization , named entity recognition , part-of-speech tagging , and word sense disambiguation CITATION
For a more complete description , see CITATION
In this work , we use a modification of Huber's robust loss function , similar to that used in CITATION: L(p , y)  4py if py <  1 (1   py) 2 if   1 py< 1 (2) 0 if py > 1 We fix the regularization parameter A to 10 -4 , similar to that used in CITATION
This relationship is modeled by + 6 Tvi (3) The parameters [{w l , vi} , 6] may then be found by joint empirical risk minimization over all the m problems , i.e. , their values should minimize the combined empirical risk: l=i  V   i=l J (4) An important observation in CITATION is that the binary classification problems used to derive 6 are not necessarily those problems we are aiming to solve
Assuming there are k target problems and m auxiliary problems , it is shown in CITATION that by performing one round of minimization , an approximate solution of 6 can be obtained from (4) by the following algorithm: 1. For each of the m auxiliary problems , learn u l as described by (1)
This is a simplified version of the definition in CITATION , made possible because the same A is used for all auxiliary problems
ASO has been demonstrated to be an effective semi-supervised learning algorithm CITATION
A variety of auxiliary problems are tested in CITATION in the semi-supervised settings , i.e. , their auxiliary problems are generated from unlabeled data
More recently , for the word sense disambiguation (WSD) task , CITATION experimented with both supervised and semi-supervised auxiliary problems , although the auxiliary problems she used are different from ours
In recent years , the availability of large human-labeled corpora such as PropBank CITATION and FrameNet CITATION has made possible a statistical approach of identifying and classifying the arguments of verbs in natural language texts
This is known as multi-task learning in the machine learning literature CITATION
A large number of SRL systems have been evaluated and compared on the standard data set in the CoNLL shared tasks CITATION , and many systems have performed reasonably well
In addition to the target outputs , CITATION discusses configurations where both used inputs and unused inputs (due to excessive noise) are utilized as additional outputs
First , we train the various classifiers on sections 2 to 21 using gold argument labels and automatic parse trees produced by Charniak's re-ranking parser CITATION , and test them on section 23 with automatic parse trees
Noun predicates also appear in FrameNet semantic role labeling CITATION , and many FrameNet SRL systems are evaluated in Senseval-3 CITATION
So far we are aware of only one English NomBank-based SRL system CITATION , which uses the maximum entropy classifier , although similar efforts are reported on the Chinese NomBank by CITATION and on FrameNet by CITATION using a small set of hand-selected nominalizations
Second , we achieve accuracy higher than that reported in CITATION and advance the state of the art in SRL research
Eighteen baseline features and six additional features are proposed in CITATION for NomBank argument identification
Unlike in CITATION , we do not prune arguments dominated by other arguments or those that overlap with the predicate in the training data
The J&N column presents the result reported in CITATION using both baseline and additional features
A diverse set of 28 features is used in CITATION for argument classification
To find a smaller set of effective features , we start with all the features considered in CITATION , in CITATION , and various combinations of them , for a total of 52 features
The J&N column presents the result reported in CITATION
This is the same configuration as reported in CITATION
Table 3: Fl scores of various classifiers on NomBank SRL Our maximum entropy classifier consistently outperforms CITATION , which also uses a maximum entropy classifier
Our results outperform those reported in CITATION
With the recent release of NomBank CITATION , it becomes possible to apply machine learning techniques to the task
Accordingly , we do not maximize the probability of the entire labeled parse tree as in CITATION
Some approaches have used WordNet for the generalization step CITATION , others EM-based clustering CITATION
The argument positions for which we compute selec-tional preferences will be semantic roles in the FrameNet CITATION paradigm , and the predicates we consider will be semantic classes of words rather than individual words (which means that different preferences will be learned for different senses of a predicate word)
We use FrameNet CITATION , a semantic lexicon for English that groups words in semantic classes called frames and lists semantic roles for each frame
Brockmann and Lapata CITATION perform a comparison of WordNet-based models
The sim function can equally well be in-c)stantiated with a WordNet-based metric (for an overview see Budanitsky and Hirst CITATION) , but we restrict our experiments to corpus-based metrics (a) in the interest of greatest possible sim cosine(w , w')   =    ,-      p      , =    sim Dice(w ,w')   =  ip/  Miiip)  a V  '    7 ^/E r p  f(w ,r p ) 2YE r p  f (w' ,r p )  2   Dic ^ V  '    7  \  R(w) \  +  \  R(w' ) \ sim  Lj n(w ,w') = ^   p    n  - tt    i  v  sim i accarH(w ,w') =   p  ) \i  ,   a slm nindie(w ,w /)   =    r  p sim Hindie(w , w' ,r p) where sim Hindle(w , w' , r p)  =   ^   abs(max(1 (w ,r p ) ,1 (w' ,r p)))   if I(w ,r p) <  0 and I (w' ,rp) <  0 Table 1: Similarity measures used resource-independence and (b) in order to be able to shape the similarity metric by the choice of generalization corpus
In SRL , the two most pressing issues today are (1) the development of strong semantic features to complement the current mostly syntactically-based systems , and (2) the problem of the domain dependence CITATION
The preference that r p has for a given synset co , the selectional association between the two , is then defined as the contribution of c 0 to r p's selectional preference strength: A(rp ,C 0) = P (C 0|r p)log  ^gf* S (rp Further WordNet-based approaches to selec-tional preference induction include Clark and Weir CITATION , and Abe and Li CITATION
To determine headwords of the semantic roles , the corpus was parsed using the Collins CITATION parser
5x2cv CITATION
They have been used for example for syntactic disambiguation CITATION , word sense disambiguation (WSD) CITATION and semantic role labeling (SRL) CITATION
While EM-based models have been shown to work better in SRL tasks CITATION , this has been attributed to the difference in coverage
We will be using the similarity metrics shown in Table 1: Cosine , the Dice and Jaccard coefficients , and Hindle's CITATION and Lin's CITATION mutual information-based metrics
Selectional restrictions and selectional preferences that predicates impose on their arguments have long been used in semantic theories , (see e.g. CITATION)
It was parsed using Minipar CITATION , which is considerably faster than the Collins parser but failed to parse about a third of all sentences
In this paper we propose a new , simple model for selectional preference induction that uses corpus-based semantic similarity metrics , such as Cosine or Lin's CITATION mutual information-based metric , for the generalization step
The corpus-based induction of selectional preferences was first proposed by Resnik CITATION
The induction of selectional preferences from corpus data was pioneered by Resnik CITATION
Rooth et al. CITATION generalize over seen headwords using EM-based clustering rather than WordNet
Experimental design Like Rooth et al. CITATION we evaluate selectional preference induction approaches in a pseudo-disambiguation task
The intuition that "hard to learn" examples are suspect corpus errors is not new , and appears also in Abney et al. CITATION , who consider the "heaviest" samples in the final distribution of the AdaBoost algorithm to be the hardest to classify and thus likely corpus errors
The HEB  Err version of the corpus is obtained by projecting the chunk boundaries on the sequence of PoS and morphology tags obtained by the automatic PoS tagger of Adler & Elhadad CITATION
We tested this hypothesis by training the Error-Driven Pruning (EDP) method of CITATION with an extended set of features
In CITATION , we established that the task is not trivially transferable to Hebrew , but reported that SVM based chunking CITATION performs well
In CITATION we argued that it is not applicable to Hebrew , mainly because of the prevalence of the Hebrew's construct state (smixut)
For the Hebrew experiments , we use the corpora of CITATION
These are the same settings as in CITATION
Refining the SimpleNP Definition: The hard cases analysis identified examples that challenge the SimpleNP definition proposed in Goldberg et al. CITATION
Kudo and Matsumoto CITATION used SVM as a classification engine and achieved an F-Score of 93.79 on the shared task NPs
Further details can be found in Kudo and Matsumoto CITATION
Following Ramshaw and Marcus CITATION , the current dominant approach is formulating chunking as a classification task , in which each word is classified as the (B)eginning , (I)nside or (O)outside of a chunk
NP chunks in the shared task data are BaseNPs , which are non-recursive NPs , a definition first proposed by Ramshaw and Marcus CITATION
For the English experiments , we use the now-standard training and test sets that were introduced in CITATION 2
This method is similar to the corpus error detection method presented by Nakagawa and Matsumoto CITATION
It is a well studied problem in English , and was the focus of CoNLL2000's Shared Task CITATION
We applied this definition to the Hebrew Tree Bank CITATION , and constructed a moderate size corpus (about 5 ,000 sentences) for Hebrew SimpleNP chunking
SVM CITATION is a supervised binary classifier
However , each of these assumes that the relations themselves are known in advance (implicitly or explicitly) so that the method can be provided with seed patterns CITATION , pattern-based rules CITATION , relation keywords CITATION , or word pairs exemplifying relation instances CITATION
Most related work deals with discovery of hypernymy CITATION , synonymy CITATION and meronymy CITATION
In addition to these basic types , several studies deal with the discovery and labeling of more specific relation sub-types , including inter-verb relations CITATION and noun-compound relationships CITATION
It should be noted that some of these papers utilize language and domain-dependent preprocessing including syntactic parsing CITATION and named entity tagging CITATION , while others take advantage of handcrafted databases such as WordNet CITATION and Wikipedia CITATION
In several studies CITATION it has been shown that relatively unsupervised and language-independent methods could be used to generate many thousands of sets of words whose semantics is similar in some sense
We do this as follows , essentially implementing a simplified version of the method of Davidov and Rappoport CITATION
Note that our method differs from that of Davidov and Rappoport CITATION in that here we provide an initial seed pair , representing our target concept , while there the goal is grouping of as many words as possible into concept classes
It was shown in CITATION that pairs of words that often appear together in such symmetric patterns tend to belong to the same class (that is , they share some notable aspect of their semantics)
Studying relationships between tagged named entities , CITATION proposed unsupervised clustering methods that assign given (or semi-automatically extracted) sets of pairs into several clusters , where each cluster corresponds to one of a known relationship type
A lot of this research is based on the initial insight CITATION that certain lexical patterns ('X is a country') can be exploited to automatically generate hyponyms of a specified word
In some recent work CITATION , it has been shown that related pairs can be generated without pre-specifying the nature of the relation sought
Finally , CITATION provided a pattern distance measure which allows a fully unsupervised measurement of relational similarity between two pairs of words; however , relationship types were not discovered explicitly
The bracketing guidelines CITATION also mention the considerable difficulty of identifying the correct scope for nominal modifiers
We use Bikel's implementation CITATION of Collins' parser CITATION in order to carry out these experiments , using the non-deficient Collins settings
We draw our counts from a corpus of n-gram counts calculated over 1 trillion words from the web CITATION
We use the Briscoe and Carroll CITATION version of DepBank , a 560 sentence subset used to evaluate the rasp parser
We map the brackets to dependencies by finding the head of the np , using the Collins CITATION head finding rules , and then creating a dependency between each other child's head and this head
We dis-cretised the non-binary features using an implementation of Fayyad and Irani's CITATION algorithm , and classify using MegaM 2
For instance , CCGbank CITATION was created by semi-automatically converting the Treebank phrase structure to Combinatory Categorial Grammar (ccg) CITATION derivations
An additional grammar rule is needed just to get a parse , but it is still not correct (Hockenmaier , 2003 , p
We check the correctness of the corpus by measuring inter-annotator agreement , by reannotating the first section , and by comparing against the sub -NP structure in DepBank CITATION
We used the PARC700 Dependency Bank CITATION which consists of 700 Section 23 sentences annotated with labelled dependencies
Our annotation guidelines 1 are based on those developed for annotating full sub -np structure in the biomedical domain CITATION
Lapata and Keller CITATION derive estimates from web counts , and only compare at a lexical level , achieving 78.7% accuracy
Finally , we test the utility of the extended Treebank for training statistical models on two tasks: NP bracketing CITATION and full parsing CITATION
Lauer CITATION has demonstrated superior performance of the dependency model using a test set of 244 (216 unique) noun compounds drawn from Grolier's encyclopedia
We implement a similar system to Table 4: Comparison of NP bracketing corpora Table 5: Lexical overlap Lauer CITATION , described in Section 3 , and report on results from our own data and Lauer's original set
The np bracketing task has often been posed in terms of choosing between the left or right branching structure of three word noun compounds: (a)  (world (oil prices)) - Right-branching (b)  ((crude oil) prices) - Left-branching Most approaches to the problem use unsupervised methods , based on competing association strength between two of the words in the compound (Marcus , 1980 , p
The Penn Treebank CITATION is perhaps the most influential resource in Natural Language Processing (NLP)
According to Marcus et al. CITATION , asking annota-tors to markup base -np structure significantly reduced annotation speed , and for this reason base- nps were left flat
For the original bracketing of the Treebank , anno-tators performed at 375-475 words per hour after a Table 1: Agreement between annotators few weeks , and increased to about 1000 words per hour after gaining more experience CITATION
Nakov and Hearst CITATION also use web counts , but incorporate additional counts from several variations on simple bigram queries , including queries for the pairs of words concatenated or joined by a hyphen
With our new data set , we began running experiments similar to those carried out in the literature CITATION
Many approaches to identifying base noun phrases have been explored as part of chunking CITATION , but determining sub -np structure is rarely addressed
The bracketing tool often suggests a bracketing using rules based mostly on named entity tags , which are drawn from the bbn corpus CITATION
The most common form of parser evaluation is to apply the parseval metrics to phrase-structure parsers based on the penn Treebank , and the highest reported scores are now over 90% CITATION
In this paper we evaluate a ccg parser CITATION on the Briscoe and Carroll version of DepBank CITATION
Briscoe and Carroll CITATION reannotated this resource using their grs scheme , and used it to evaluate the rasp parser
Parsers have been developed for a variety of grammar formalisms , for example hpsg CITATION , lfg CITATION , tag CITATION , ccg CITATION , and variants of phrase-structure grammar CITATION , including the phrase-structure grammar implicit in the Penn Treebank CITATION
And third , we provide the first evaluation of a wide-coverage ccg parser outside of CCGbank , obtaining impressive results on DepBank and outperforming the rasp parser CITATION by over 5% overall and on the majority of dependency types
For the gold standard we chose the version of Dep-Bank reannotated by Briscoe and Carroll CITATION , consisting of 700 sentences from Section 23 of the Penn Treebank
The results in Table 4 were obtained by parsing the sentences from CCGbank corresponding to those in the 560-sentence test set used by Briscoe et al. CITATION
the macro-averaged scores are the mean of the individual scores for each relation CITATION
Can the ccg parser be compared with parsers other than rasp? Briscoe and Carroll CITATION give a rough comparison of rasp with the Parc lfg parser on the different versions of DepBank , obtaining similar results overall , but they acknowledge that the results are not strictly comparable because of the different annotation schemes used
Briscoe et al. CITATION split the 700 sentences in DepBank into a test and development set , but the latter only consists of 140 sentences which was not enough to reliably create the transformation
All the results were obtained using the RASP evaluation scripts , with the results for the rasp parser taken from Briscoe et al. CITATION
Preiss CITATION compares the parsers of Collins CITATION and Charniak CITATION , the gr finder of Buchholz et al. CITATION , and the rasp parser , using the Carroll et al. CITATION gold-standard
It has been argued that the parseval metrics are too forgiving and that phrase structure is not the ideal representation for a gold standard CITATION
Carroll et al. CITATION describe such a suite , consisting of sentences taken from the Susanne corpus , annotated with Grammatical Relations (grs) which specify the syntactic relation between a head and dependent
We chose not to use the corpus based on the Susanne corpus CITATION because the grs are less like the ccg dependencies; the corpus is not based on the Penn Treebank , making comparison more difficult because of tokenisation differences , for example; and the latest results for rasp are on DepBank
parser evaluation has improved on the original parseval measures CITATION , but the challenge remains to develop a representation and evaluation suite which can be easily applied to a wide variety of parsers and formalisms
Clark and Curran CITATION describes the ccg parser used for the evaluation
Previous evaluations of ccg parsers have used the predicate-argument dependencies from CCGbank as a test set CITATION , with impressive results of over 84% F-score on labelled dependencies
Kaplan et al. CITATION compare the Collins CITATION parser with the Parc lfg parser by mapping lfg F-structures and Penn Treebank parses into DepBank dependencies , claiming that the lfg parser is considerably more accurate with only a slight reduction in speed
The ccg parser results are based on automatically assigned pos tags , using the Curran and Clark CITATION tagger
An example of this is from CCGbank CITATION , where all modifiers in noun-noun compound constructions modify the final noun (because the penn Treebank , from which CCGbank is derived , does not contain the necessary information to obtain the correct bracketing)
The grammar used by the parser is extracted from CCGbank , a ccg version of the Penn Treebank CITATION
Such conversions have been performed for other parsers , including parsers producing phrase structure output CITATION
Kaplan et al. CITATION clearly invested considerable time and expertise in mapping the output of the Collins parser into the DepBank dependencies , but they also note that "This conversion was relatively straightforward for LFG structures ..
In the case of Kaplan et al. CITATION , the testing procedure would include running their conversion process on Section 23 of the Penn Treebank and evaluating the output against DepBank
A similar resource   the Parc Dependency Bank (DepBank) CITATION   has been created using sentences from the Penn Treebank
The b&c scheme is similar to the original DepBank scheme CITATION , but overall contains less grammatical detail; Briscoe and Carroll CITATION describes the differences
Different parsers produce different output , for ex-ample phrase structure trees CITATION , dependency trees CITATION , grammatical relations CITATION , and formalism-specific dependencies CITATION
The grammar consists of 425 lexical categories   expressing subcategorisation information   plus a small number of combinatory rules which combine the categories CITATION
A more interesting statement would be that it makes learning easier , along the lines of the result of CITATION   note , however , that their results are for the "semi-supervised" domain adaptation problem and so do not apply directly
A part-of-speech tagging problem on PubMed abstracts introduced by Blitzer et al. CITATION
The first model , which we shall refer to as the Prior model , was first introduced by Chelba and Acero CITATION
This is a recapitalization task introduced by Chelba and Acero CITATION and also used by Daume III and Marcu CITATION
For the CNN-Recap task , we use identical feature to those used by both Chelba and Acero CITATION and Daume III and Marcu CITATION: the current , previous and next word , and 1-3 letter prefixes and suffixes
Many of these are presented and evaluated by Daume III and Marcu CITATION
Daume III and Marcu CITATION provide empirical evidence on four datasets that the Prior model outperforms the baseline approaches
More recently , Daume III and Marcu CITATION presented an algorithm for domain adaptation for maximum entropy classifiers
We additionally ran the MegaM model CITATION on these data (though not in the multi-conditional case; for this , we considered the single source as the union of all sources)
In all cases , we use the S earn algorithm for solving the sequence labeling problem CITATION with an underlying averaged perceptron classifier; implementation due to CITATION
Second , it is arguable that a measure like F 1 is inappropriate for chunking tasks CITATION
Following CITATION , we call the first the source domain , and the second the target domain
Recently there have been some studies addressing domain adaptation from different perspectives CITATION
The POS data set and the CTS data set have previously been used for testing other adaptation methods CITATION , though the setup there is different from ours
Blitzer et al. CITATION propose a domain adaptation method that uses the unlabeled target instances to infer a good feature representation , which can be regarded as weighting the features
Chelba and Acero CITATION use the parameters of the maximum entropy model learned from the source domain as the means of a Gaussian prior when training a new model on the target data
The setup is very similar to Daume III and Marcu CITATION
els the different distributions in the source and the target domains is by Daume III and Marcu CITATION
Florian et al. CITATION first train a NE tagger on the source domain , and then use the tagger's predictions as features for training and testing on the target domain
This way of setting 7 corresponds to the entropy minimization semi-supervised learning method CITATION
For generative syntactic parsing , Roark and Bac-chiani CITATION have used the source domain data to construct a Dirichlet prior for MAP estimation of the PCFG for the target domain
ber of hidden components is not fixed , but emerges We begin by presenting three finite tree models , each naturally from the training data CITATION
The closely related infinite hidden Markov model is an HMM in which the transitions are modeled using an HDP , enabling unsupervised learning of sequence models when the number of hidden states is unknown CITATION
The infinite hidden Markov model (iHMM) or HDP-HMM CITATION is a model of sequence data with transitions modeled by an HDP
This is useful , because coarse-grained syntactic categories , such as those used in the Penn Treebank (PTB) , make insufficient distinctions to be the basis of accurate syntactic parsing CITATION
Hence , state-of-the-art parsers either supplement the part-of-speech (POS) tags with the lexical forms themselves CITATION , manually split the tagset into a finer-grained one CITATION , or learn finer grained tag distinctions using a heuristic learning procedure CITATION
But the introduction of nonparametric priors such as the Dirichletprocess CITATION enabled development of infinite mixture models , in which the num-Teh et al. CITATION proposed the hierarchical Dirichlet process (HDP) as a way of applying the Dirichlet process (DP) to more complex model forms , so as to allow multiple , group-specific , infinite mixture models to share their mixture components
8 Additionally , we compute the mutual information of the learned clusters with the gold tags , and we compute the cluster F-score CITATION
First , we use the standard approach of greedily assigning each of the learned classes to the POS tag with which it has the greatest overlap , and then computing tagging accuracy CITATION
For comparison , Haghighi and Klein CITATION report an unsupervised baseline of 41.3% , and a best result of 80.5% from using hand-labeled prototypes and distributional similarity
Earlier , Johnson et al. CITATION presented adaptor grammars , which is a very similar model to the HDP-PCFG
We use the generative dependency parser distributed with the Stanford factored parser CITATION for the comparison , since it performs simultaneous tagging and parsing during testing
The HDP-PCFG CITATION , developed at the same time as this work , aims to learn state splits for a binary-branching PCFG
In contrast , Liang et al. CITATION define a global DP over sequences , with the base measure defined over the global state probabilities ,  0; locally , each state has an HDP , with this global DP as the base measure
For both experiments , we used dependency trees extracted from the Penn Treebank CITATION using the head rules and dependency extractor from Yamada and Matsumoto CITATION
To generate  n we first generate an infinite sequence of variables  n' = (n k each of which is distributed according to the Beta distribution: Then  n = (n k)  = = 1 is defined as: (1 Following Pitman CITATION we refer to this process as n   GEM (a 0)
Teh , 2006 , p.c.) , to sample each m jk: sampleM (j , k) 1  if n jk = 0 2  then m jk = 0 3  else m jk = 1 4  for i ^ 2 to n jk 5  doifrand ()<^^T 6  then m jk = m jk + 1 7  return m jk Sampling /?
In many cases , improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology CITATION
This follows a conceptually similar approach by CITATION that uses a large named-entity dictionary , where the similarity between the candidate named-entity and its matching prototype in the dictionary is encoded as a feature in a supervised classifier
Therefore , an increasing attention has been recently given to semi-supervised learning , where large amounts of unlabeled data are used to improve the models learned from a small training set CITATION
This was used , for example , by CITATION in information extraction , and by CITATION in POS tagging
This decomposition applies both to discriminative linear models and to generative models such as HMMs and CRFs , in which case the linear sum corresponds to log likelihood assigned to the input/output pair by the model (for details see CITATION for the classification case and CITATION for the structured case)
For example , CITATION proposes Diagonal Transition Models for sequential labeling tasks where neighboring words tend to have the same labels
The second problem we consider is extracting fields from advertisements CITATION
CITATION and CITATION also report results for semi-supervised learning for these domains
CITATION extends the dictionary-based approach to sequential labeling tasks by propagating the information given in the seeds with contextual word similarity
We implement some global constraints and include unary constraints which were largely imported from the list of seed words used in CITATION
CITATION also worked on one of our data sets
1 The first task is to identify fields from citations CITATION
Another way to look the algorithm is from the self-training perspective CITATION
However , in the general case , semi-supervised approaches give mixed results , and sometimes even degrade the model performance CITATION
CITATION has suggested to balance the contribution of labeled and unlabeled data to the parameters
CITATION
This confirms results reported for the supervised learning case in CITATION
On the other hand , in the supervised setting , it has been shown that incorporating domain and problem specific structured information can result in substantial improvements CITATION
However CITATION showed that reasoning with more expressive , non-sequential constraints can improve the performance for the supervised protocol
We note that in the presence of constraints , the inference procedure (for finding the output y that maximizes the cost function) is usually done with search techniques (rather than Viterbi decoding , see CITATION for a discussion) , we chose beamsearch decoding
While CITATION showed the significance of using hard constraints , our experiments show that using soft constraints is a superior option
Conceptually , although not technically , the most related work to ours is CITATION that , in a somewhat ad-hoc manner uses soft constraints to guide an unsupervised model that was crafted for mention tracking
Crucially , the kind of lexical descriptions that we employ are those that are commonly devised within lexicon-driven approaches to linguistic syntax , e.g.Lexicalized Tree-Adjoining Grammar CITATION and Combinary Categorial Grammar CITATION
There are currently two supertagging approaches available: LTAG-based CITATION and CCG-based CITATION
One important way of portraying such lexical descriptions is via the supertags devised in the LTAG and CCG frameworks CITATION
The term "supertagging" CITATION refers to tagging the words of a sentence , each with a supertag
The LTAG-based supertagger of CITATION is a standard HMM tagger and consists of a (second-order) Markov language model over supertags and a lexical model conditioning the probability of every word on its own supertag (just like standard HMM-based POS taggers)
For the LTAG supertags experiments , we used the LTAG English supertagger 5 CITATION to tag the English part of the parallel data and the supertag language model data
Akin to POS tagging , the process of supertagging an input utterance proceeds with statistics that are based on the probability of a word-supertag pair given their Markovian or local context CITATION
Besides the difference in probabilities and statistical estimates , these two supertaggers differ in the way the supertags are extracted from the Penn Treebank , cf. CITATION
Only quite recently have CITATION and CITATION shown that incorporating some form of syntactic structure could show improvements over a baseline PBSMT system
Among the first to demonstrate improvement when adding recursive structure was CITATION , who allows for hierarchical phrase probabilities that handle a range of reordering phenomena in the correct fashion
The CCG supertagger CITATION is based on log-linear probabilities that condition a supertag on features representing its context
For the CCG supertag experiments , we used the CCG supertagger of CITATION and the Edinburgh CCG tools 6 to tag the English part of the parallel corpus as well as the CCG supertag language model data
Both the LTAG CITATION and the CCG supertag sets CITATION were acquired from the WSJ section of the Penn-II Treebank using hand-built extraction rules
Decoder The decoder used in this work is Moses , a log-linear decoder similar to Pharaoh CITATION , modified to accommodate supertag phrase probabilities and supertag language models
Within the field of Machine Translation , by far the most dominant paradigm is Phrase-based Statistical Machine Translation (PBSMT) CITATION
For example , CITATION demonstrated that adding syntax actually harmed the quality of their SMT system
The bidirectional word alignment is used to obtain phrase translation pairs using heuristics presented in CITATION and CITATION , and the Moses decoder was used for phrase extraction and decoding
The bidirectional word alignment is used to obtain lexical phrase translation pairs using heuristics presented in CITATION and CITATION
Coming right up to date , CITATION demonstrate that 'syntactified' target language phrases can improve translation quality for Chinese-English
While the research of CITATION has much in common with the approach proposed here (such as the syntactified target phrases) , there remain a number of significant differences
The NIST MT03 test set is used for development , particularly for optimizing the interpolation weights using Minimum Error Rate training CITATION
Firstly , rather than induce millions of xRS rules from parallel data , we extract phrase pairs in the standard way CITATION and associate with each phrase-pair a set of target language syntactic structures based on supertag sequences
Table 1 presents the BLEU scores CITATION of both systems on the NIST 2005 MT Evaluation test set
For less commonly used languages , one might use open source research systems CITATION
Also relevant is previous work that applied machine learning approaches to MT evaluation , both with human references CITATION and without CITATION
METEOR uses the Porter stemmer and synonym-matching via WordNet to calculate recall and precision more accurately CITATION
As its loss function , support vector regression uses an e-insensitive error function , which allows for errors within a margin of a small positive value , e , to be considered as having zero error (cf.Bishop CITATION , pp.339-344)
This can be seen as a form of confidence estimation on MT outputs CITATION
To remove the bias in the distributions of scores between different judges , we follow the normalization procedure described by Blatz et al. CITATION
We conducted experiments to determine the feasibility of the proposed approach and to address the following questions: (1) How informative are pseudo references in-and-of themselves? Does varying the number and/or the quality of the references have an impact on the metrics? (2) What are the contributions of the adequacy features versus the fluency features to the learning-based metric? (3) How do the quality and distribution of the training examples , together with the quality of the pseudo references , impact the metric training? (4) Do these factors impact the metric's ability in assessing sentences produced within a single MT system? How does that system's quality affect metric performance? The implementation of support vector regression used for these experiments is SVM-Light CITATION
To compare the relative quality of different metrics , we apply bootstrapping re-sampling on the data , and then use paired t-test to determine the statistical significance of the correlation differences CITATION
ROUGE utilizes 'skip n-grams' , which allow for matches of sequences of words that are not necessarily adjacent CITATION
BLEU is smoothed CITATION , and it considers only matching up to bigrams because this has higher correlations with human judgments than when higher-ordered n-grams are included
The HWC metrics compare dependency and constituency trees for both reference and machine translations CITATION
In addition to adapting the idea of Head Word Chains CITATION , we also compared the input sentence's argument structures against the treebank for certain syntactic categories
Reference-based metrics such as BLEU CITATION have rephrased this subjective task as a somewhat more objective question: how closely does the translation resemble sentences that are known to be good translations for the same source? This approach requires the participation of human translators , who provide the "gold standard" reference sentences
The relationship between word alignments and their impact on MT is also investigated in CITATION
Most current statistical models CITATION treat the aligned sentences in the corpus as sequences of tokens that are meant to be words; the goal of the alignment process is to find links between source and target words
To quickly (and approximately) evaluate this phenomenon , we trained the statistical IBM word-alignment model 4 CITATION , 1 using the GIZA++ software CITATION for the first two language pairs , and the Europarl corpus CITATION for the last one
They can be seen as extensions of the simpler IBM models 1 and 2 CITATION
We use a standard log-linear phrase-based statistical machine translation system as a baseline: GIZA++ implementation of IBM word alignment model 4 CITATION , 8 the refinement and phrase-extraction heuristics described in CITATION , minimum-error-rate training Table 2: Chinese-English corpus statistics CITATION using Phramer CITATION , a 3-gram language model with Kneser-Ney smoothing trained with SRILM CITATION on the English side of the training data and Pharaoh CITATION with default settings to decode
We also want to bootstrap on different word aligners; in particular , one possibility is to use the flexible HMM word-to-phrase model of Deng and Byrne CITATION in place of IBM model 4
We evaluate the reliability of these candidates , using simple metrics based on co-occurence frequencies , similar to those used in associative approaches to word alignment CITATION
Second , an increase in AER does not necessarily imply an improvement in translation quality CITATION and vice-versa CITATION
This very simple measure is frequently used in associative approaches CITATION
%: there is want to need not I^iS: in front of  : as soon as ;#: look at Figure 2: Examples of entries from the manually developed dictionary The intrinsic quality of word alignment can be assessed using the Alignment Error Rate (AER) metric CITATION , that compares a system's alignment output to a set of gold-standard alignment
The quality of the translation output is evaluated using BLEU CITATION
The experiments were carried out using the Chinese-English datasets provided within the IWSLT 2006 evaluation campaign CITATION , extracted from the Basic Travel Expression Corpus (BTEC) CITATION
For Chinese , the data provided were tokenized according to the output format of ASR systems , and human-corrected CITATION
Note that the need to consider segmentation and alignment at the same time is also mentioned in CITATION , and related issues are reported in CITATION
More importantly , however , this segmentation is often performed in a monolingual context , which makes the word alignment task more difficult since different languages may realize the same concept using varying numbers of words (see e.g. CITATION)
The log-linear model is also based on standard features: conditional probabilities and lexical smoothing ofphrases in both directions , and phrase penalty CITATION
To test the influence of the initial word segmentation on the process of word packing , we considered an additional segmentation configuration , based on an automatic segmenter combining rule-based and statistical techniques CITATION
These resources follow more or less the same format as the output of the word segmenter mentioned in Section 5.1.2 CITATION , so the experiments are carried out using this segmentation
It has been argued that METEOR correlates better with human judgment due to higher weight on recall than precision CITATION
Recently , confusion network decoding for MT system combination has been proposed CITATION
Powell's method CITATION is used to tune the system and feature weights simultaneously so as to optimize various automatic evaluation metrics on a development set
In this work , modified Powell's method as proposed by CITATION is used
Six MT systems were combined: three (A ,C ,E) were phrase-based similar to CITATION , two (B ,D) were hierarchical similar to CITATION and one (F) was syntax-based similar to CITATION
Combination of speech recognition outputs is an example of this approach CITATION
Also , a more heuristic alignment method has been proposed in a different system combination approach CITATION
In speech recognition , confusion network decoding CITATION has become widely used in system combination
In CITATION , different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ CITATION
Tuning is fully automatic , as opposed to CITATION where global system weights were set manually
Similar combination of multiple confusion networks was presented in CITATION
The same Powell's method has been used to estimate feature weights of a standard feature-based phrasal MT decoder in CITATION
The optimization of the system and feature weights may be carried out using -best lists as in CITATION
Currently , the most widely used automatic MT evaluation metric is the NIST BLEU-4 CITATION
This work was extended in CITATION by introducing system weights for word confidences
In CITATION , simple score was assigned to the word coming from the th-best hypothesis
In CITATION , the total confidence of the nth best confusion network hypothesis  , including NULL words , given the th source sentence  was given by where is the number of nodes in the confusion network for the source sentence  , is the number of translation systems , is the th system weight ,  c wn is the accumulated confidence for word produced by system between nodes and  , and is a weight for the number of NULL links along the hypothesis
The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in CITATION on the Arabic to English and Chinese to English NIST MT05 tasks
Compared to the baseline from CITATION , the new method improves the BLEU scores significantly
In ensemble learning , a collection of simple classifiers is used to yield better performance than any single classifier; for example boosting CITATION
A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) CITATION was used to align hy-potheses in CITATION
Minimum Bayes risk (MBR) was used to choose the skeleton in CITATION
This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities CITATION
It has been found that multiple hypotheses from each system may be used to improve the quality of the combination output CITATION
Translation edit rate (TER) CITATION has been proposed as more intuitive evaluation metric since it is based on the rate of edits required to transform the hypothesis into the reference
However , this would require time consuming evaluations such as human mediated TER post-editing CITATION
The TnT tagger CITATION and the TreeTagger CITATION are used for tagging and lemmatization
Motivated by the theoretical work by Chafe CITATION and Jacobs CITATION , we view the VF as the place for elements which modify the situation described in the sentence , i.e
Finally , the articles are parsed with the CDG dependency parser CITATION
The preferences summarized below have mo-tivated our choice of features:  constituents in the nominative case precede those in other cases , and dative constituents often precede those in the accusative case CITATION;  the verb arguments' order depends on the verb's  subcategorization properties CITATION;  constituents with a definite article precede those with an indefinite one CITATION;  pronominalized constituents precede non-pronominalized ones CITATION ;  animate referents precede inanimate ones CITATION;  short constituents precede longer ones CITATION;  the preferred topic position is right after the verb CITATION;  the initial position is usually occupied by scene-setting elements and topics CITATION
The sentence-initial position , which in German is the VF , has been shown to be cognitively more prominent than other positions CITATION
Inspired by the findings of the Prague School CITATION and Systemic Functional Linguistics CITATION , they focus on the role that information structure plays in constituent ordering
Harbusch et al. CITATION present a generation workbench , which has the goal of producing not the most appropriate order , but all grammatical ones
We suppose that this dificulty comes from the double function of the initial position which can either introduce the ad-dressation topic , or be the scene- or frame-setting position CITATION
We hypothesize that the reasons which bring a constituent to the VF are different from those which place it , say , to the beginning of the MF , for the order in the MF has been shown to be relatively rigid CITATION
Since our learner treats all values as nominal , we discretized the values of dep and len with a C4.5 classifier CITATION
Kruijff et al. CITATION describe an architecture which supports generating the appropriate word order for different languages
Kruijff-Korbayova et al. CITATION address the task of word order generation in the same vein
Similar to Langkilde & Knight CITATION we utilize statistical methods
Kendall's t , which has been used for evaluating sentence ordering tasks CITATION , is the second metric we use
E.g. , in text-to-text generation CITATION , new sentences are fused from dependency structures of input sentences
Ringger et al. CITATION aim at regenerating the order of constituents as well as the order within them for German and French technical manuals
Similar to Ringger et al. CITATION , we find the order with the highest probability conditioned on syntactic and semantic categories
Apart from acc and t , we also adopt the metrics used by Uchimoto et al. CITATION and Ringger et al. CITATION
According to the inv metric , our results are considerably worse than those reported by Ringger et al. CITATION
We retrained our system on a corpus of newspaper articles CITATION which is manually annotated but encodes no semantic knowledge
The work of Uchimoto et al. CITATION is done on the free word order language Japanese
For the fourth baseline (UCHIMOTO) , we utilized a maximum entropy learner (OpenNLP 8) and reim-plemented the algorithm of Uchimoto et al. CITATION
Uszkoreit CITATION addresses the problem from a mostly grammar-based perspective and suggests weighted constraints , such as [+nom] -< [+dat] , [+pro] -< [-pro] , [-focus] -< [+focus] , etc
Unlike overgeneration approaches CITATION which select the best of all possible outputs ours is more efficient , because we do not need to generate every permutation
It also compares reasonably with other more recent evaluations CITATION which derive their input data from the penn Treebank by transforming each sentence tree into a format suitable for the realiser CITATION
For instance , CITATION reports that the implementation of such a processor for Surge was the most time consuming part of the evaluation with the resulting component containing 4000 lines of code and 900 rules
The realiser presented here differs in mainly two ways from existing reversible realisers such as CITATION's CCG system or the HPSG ERG based realiser CITATION
The reason for this is that the grammar is compiled from a higher level description where tree fragments are first encapsulated into so-called classes and then explicitly combined (by inheritance , conjunction and disjunction) to produce the grammar elementary trees (cf. CITATION)
Thus for instance , both REALPRO CITATION and Surge CITATION assume that the input associates semantic literals with low level syntactic and lexical information mostly leaving the realiser to just handle inflection , word order , insertion of grammatical words and agreement
To associate semantic representations with natural language expressions , the FTAG is modified as proposed in CITATION
The proposal draws on ideas from CITATION and aims to determine whether for a given input (a set of TAG elementary trees whose semantics equate the input semantics) , syntactic requirements and resources cancel out
It could be used for instance , in combination with the parser and the semantic construction module described in CITATION , to support textual entailment recognition or answer detection in question answering
We rely on these features to associate one and the same semantic to large sets of trees denoting semantically equivalent but syntactically distinct configurations (cf. CITATION)
The basic surface realisation algorithm used is a bottom up , tabular realisation algorithm CITATION optimised for TAGs
Similarly , KPML CITATION assumes access to ideational , interpersonal and textual information which roughly corresponds to semantic , mood/voice , theme/rheme and focus/ground information
In order to ensure this determinism , NLG geared realisers generally rely on theories of grammar which systematically link form to function such as systemic functional grammar (SFG , CITATION) and , to a lesser extent , Meaning Text Theory (MTT , CITATION)
First , the paraphrase figures might seem low wrt to e.g. , work by CITATION which mentions several thousand outputs for one given input and an average number of realisations per input varying between 85.7 and 102.2
This does not seem to be the case in CITATION's approach where the count seems to include all sentences associated by the grammar with the input semantics
A Feature-based TAG (FTAG , CITATION) consists of a set of (auxiliary or initial) elementary trees and of two tree composition operations: substitution and ad-junction
A first possibility would be to draw on CITATION's proposal and compute the enriched input based on the traversal of a systemic network
Thus for instance , CITATION resorts to ad hoc "mapping tables" to associate substitution nodes with semantic indices and "fr-nodes" to constrain adjunction to the correct nodes
While there have been previous systems that encode generation as planning CITATION , our approach is distinguished from these systems by its focus on the grammatically specified contributions of each individual word (and the TAG tree it anchors) to syntax , semantics , and local pragmatics CITATION
It also allows us to benefit from the past and ongoing advances in the performance of off-the-shelf planners CITATION
Unlike some approaches CITATION , we do not have to distinguish between generating NPs and expressions of other syntactic categories
The context set of an intended referent is the set of all individuals that the hearer might possibly confuse it with CITATION
It is based on the well-known STRIPS language CITATION
The grammar formalism we use here is that of lex-icalized tree-adjoining grammars (LTAG; Joshi and Schabes CITATION)
In order to use the planner as a surface realization algorithm for TAG along the lines of Koller and Striegnitz CITATION , we attach semantic content to each elementary tree and require that the sentence achieves a certain communicative goal
However , this problem is NP-complete , by reduction of Hamiltonian Cycle - unsurprisingly , given that it encompasses realization , and the very similar realization problem in Koller and Striegnitz CITATION is NP-hard
PDDL CITATION is the standard input language for modern planning systems
In a scenario that involves multiple rabbits , multiple hats , and multiple individuals that are inside other individuals , but only one pair of a rabbit r inside a hat h , the expression "X takes the rabbit from the hat" is sufficient to refer uniquely to r and h CITATION
We share these advantages with systems such as SPUD CITATION
This makes our encoding more direct and transparent than those in work like Thomason and Hobbs CITATION and Stone et al. CITATION
We follow Stone et al. CITATION in formalizing the semantic content of a lexicalized elementary tree t as a finite set of atoms; but unlike in earlier approaches , we use the semantic roles in t as the arguments of these atoms
The three pragmatic predicates that we will use here are hearer-new , indicating that the hearer does not know about the existence of an individual and can't infer it CITATION , hearer-old for the opposite , and contextset
In addition to the semantic content , we equip every elementary tree in the grammar with a semantic requirement and a pragmatic condition CITATION
Supertag This is a variant of the approach above , but using supertags CITATION instead of PoS tags
For example , the metrics proposed in Bangalore et al. CITATION , such as Simple Accuracy and Generation Accuracy , measure changes with respect to a reference string based on the idea of string-edit distance
The judges were then presented with the 50 sentences in random order , and asked to score the sentences according to their own scale , as in magnitude estimation CITATION; these scores were then normalised in the range [0 ,1]
Regarding the interpretation of the absolute value of (Pearson's) correlation coefficients , both here and in the rest of the paper , we adopt Cohen's scale CITATION for use in human judgements , given in Table 1; we use this as most of this work is to do with human judgements of fluency
Those chosen were the Connexor parser , 2 the Collins parser CITATION , and the Link Grammar parser CITATION
For example , in statistical MT the translation model and the language model are treated separately , characterised as faithfulness and fluency respectively (as in the treatment in Jurafsky and Martin CITATION)
A neat solution to poor sentence-level evaluation proposed by Kulesza and Shieber CITATION is to use a Support Vector Machine , using features such as word error rate , to estimate sentence-level translation quality
Bleu CITATION is a canonical example: in matching n-grams in a candidate translation text with those in a reference text , the metric measures faithfulness by counting the matches , and fluency by implicitly using the reference n-grams as a language model
Quite a different idea was suggested in Wan et al. CITATION , of using the grammatical judgement of a parser to assess fluency , giving a measure independent of the language model used to generate the text
In terms of automatic evaluation , we are not aware of any technique that measures only fluency or similar characteristics , ignoring content , apart from that of Wan et al. CITATION
The consistency and magnitude of the first three parser metrics , however , lends support to the idea of Wan et al. CITATION to use something like these as indicators of generated sentence fluency
Similarly , the ultrasummarisa-tion model of Witbrock and Mittal CITATION consists of a content model , modelling the probability that a word in the source text will be in the summary , and a language model
In this model we violate the Markov assumption of independence in much the same way as Witbrock and Mittal CITATION in their combination of content and language model probabilities , by backtracking at every state in order to discourage repeated words and avoid loops
Zajic et al. CITATION use similar scales for summarisation
Coreference resolution on text datasets is well-studied (e.g. , CITATION)
We employ a set of verbal features that is similar to the features used by state-of-the-art coreference resolution systems that operate on text (e.g. , CITATION)
Evaluation metric Coreference resolution is often performed in two phases: a binary classification phase , in which the likelihood of corefer-ence for each pair of noun phrases is assessed; and a partitioning phase , in which the clusters of mutually-coreferring NPs are formed , maximizing some global criterion CITATION
The verbal features that we have included are a representative sample from the literature (e.g. , CITATION)
also consider training separate classifiers and combining their posteriors , either through weighted addition or multiplication; this is sometimes called "late fusion." Late fusion is also employed for gesture-speech combination in CITATION
All features are computed from hand and body pixel coordinates , which are obtained via computer vision; our vision system is similar to CITATION
The continuous-valued features were binned using a supervised technique CITATION
While people have little difficulty distinguishing between meaningful gestures and irrelevant hand motions (e.g. , self-touching , adjusting glasses) CITATION , NLP systems may be confused by such seemingly random movements
Markable noun phrases - those that are permitted to participate in coreference relations - were annotated by the first author , in accordance with the MUC task definition CITATION
To measure the similarity between gesture trajectories , we use dynamic time warping CITATION , which gives a similarity metric for temporal data that is invariant to speed
In addition , verbal language is different when used in combination with meaningful non-verbal communication than when it is used unimodally CITATION
Kehler finds that fully-specified noun phrases are less likely to receive multimodal support CITATION
Last , we note that NPs with adjectival modifiers were assigned negative weights , supporting the finding of CITATION that fully-specified NPs are less likely to receive multimodal support
Experiments in both CITATION and CITATION find no conclusive winner among early fusion , additive late fusion , and multiplicative late fusion
JS-div reports the Jensen-Shannon divergence , a continuous-valued feature used to measure the similarity in cluster assignment probabilities between the two gestures CITATION
The objective function (Equation 1) is optimized using a Java implementation of L-BFGS , a quasiNewton numerical optimization technique CITATION
However , non-verbal modalities are often noisy , and their interactions with speech are complex CITATION
Our non-verbal features attempt to capture similarity between the speaker's hand gestures; similar gestures are thought to suggest semantic similarity CITATION
Euclidean distance captures cases in which the speaker is performing a gestural "hold" in roughly the same location CITATION
Non-verbal meta features Research on gesture has shown that semantically meaningful hand motions usually take place away from "rest position ," which is located at the speaker's lap or sides CITATION
Indeed , the psychology literature describes a finite-state model of gesture , proceeding from "preparation ," to "stroke ," "hold ," and then "retraction" CITATION
Verbal meta features Meaningful gesture has been shown to be more frequent when the associated speech is ambiguous CITATION
The use of hidden variables in a conditionally-trained model follows CITATION
For example , Shriberg et al. CITATION explore the use of prosodic features for sentence and topic segmentation
While more flexible than the interpolation techniques described in CITATION , training modality-specific classifiers separately is still suboptimal compared to training them jointly , because independent training of the modality-specific classifiers forces them to account for data that they cannot possibly explain
Toyama and Horvitz CITATION introduce a Bayesian network approach to modality combination for speaker identification
Introduction With recent advances in spoken dialogue system technologies , researchers have turned their attention to more complex domains (e.g.tutoring CITATION , technical support CITATION , medication assistance CITATION)
Average (standard deviation) for objective metrics in the first problem Related work Discourse structure has been successfully used in non-interactive settings (e.g.understanding specific lexical and prosodic phenomena CITATION  , natural language generation CITATION , essay scoring CITATION as well as in interactive settings (e.g.predictive/generative models of postural shifts CITATION , generation/interpretation of anaphoric expressions CITATION , performance modeling CITATION)
Other visual improvements for dialogue-based computer tutors have been explored in the past (e.g.talking heads CITATION)
This information is implicitly encoded in the intentional structure of a discourse as proposed in the Grosz & Sidner theory of discourse CITATION
3  The Navigation Map (NM) We use the Grosz & Sidner theory of discourse CITATION to inform our NM design
2 ITSPOKE ITSPOKE CITATION is a state-of-the-art tutoring spoken dialogue system for conceptual physics
Thus , interacting with such systems can be characterized by an increased user cognitive load associated with listening to often lengthy system turns and the need to integrate the current information to the discussion overall CITATION
However , implementing the NM in a new domain requires little expertise as previous work has shown that na ve users can reliably annotate the information needed for the NM CITATION
While a somewhat similar graphical representation of the discourse structure has been explored in one previous study CITATION , to our knowledge we are the first to test its benefits (see Section 6)
This theory has inspired several generic dialogue managers for spoken dialogue systems (e.g. CITATION)
One related study is that of CITATION
Results for Q1-6 Questions Q1-6 were inspired by previous work on spoken dialogue system evaluation (e.g. CITATION) and measure user's overall perception of the system
This situation is very similar to the training process of translation models in statistical machine translation CITATION , where parallel corpus is used to find the mappings between words from different languages by exploiting their co-occurrence patterns
J^Pr(Wj |o k) = 1 , Vk j =1 This optimization problem can be solved by the EM algorithm CITATION
Studies have also shown that eye gaze has a potential to improve resolution of underspecified referring expressions in spoken dialog systems CITATION and to disambiguate speech input CITATION
Given the recent advances in eye tracking technology CITATION , integrating non-intrusive and high performance eye trackers with conversational interfaces becomes feasible
Motivated by psycholinguistic studies CITATION and recent investigations on computational models for language acquisition and grounding CITATION , we are particularly interested in two unique questions related to multimodal conversational systems: (1) In a multimodal conversation that involves more complex tasks (e.g. , both user initiated tasks and system initiated tasks) , is there a reliable temporal alignment between eye gaze and spoken references so that the coupled inputs can be used for automated vocabulary acquisition and interpretation? (2) If such an alignment exists , how can we model this alignment and automatically acquire and interpret the vocabularies? To address the first question , we conducted an empirical study to examine the temporal relationships between eye fixations and their corresponding spoken references
Additionally , before speaking a word , the eyes usually move to the objects to be mentioned CITATION
Previous psycholinguistics studies have shown that the direction of gaze carries information about the focus of the user's attention CITATION
In research on multimodal interactive systems , recent work indicates that the speech and gaze integration patterns can be modeled reliably for individual users and therefore be used to improve multimodal system performances CITATION
In addition , visual properties of the interface also affect user gaze behavior and thus influence the predication of attention CITATION based on eye gaze
Recent work has shown that the effect of eye gaze in facilitating spoken language processing varies among different users CITATION
Recent studies have shown that multisensory information (e.g. , through vision and language processing) can be combined to effectively acquire words to their perceptually grounded objects in the environment CITATION
The perceived visual context influences spoken word recognition and mediates syntactic processing CITATION
Figure 1 Multimodal interface on tablet In this paper we explore the application of multimodal interface technologies (See Andr  CITATION for an overview) to the creation of more effective systems used to search and browse for entertainment content in the home
These interfaces are cumbersome and do not scale well as the range of content available increases CITATION
An important advantage of speech is that it makes it easy to combine multiple constraints over multiple dimensions within a single query CITATION
A number of previous systems have investigated the addition of unimodal spoken search queries to a graphical electronic program guide CITATION; Goto et al. , 2003; Wittenburg et al. , 2006)
Others have gone beyond unimodal speech input and added multimodal commands combining speech with pointing CITATION
This develops and extends upon the multimodal architecture underlying the MATCH system CITATION
Speech recognition results , pointing gestures made on the display , and handwritten inputs , are all passed to a multimodal understanding server which uses finite-state multimodal language proc-essing techniques CITATION to interpret and integrate the speech and gesture
However , as also reported in previous work CITATION , recognition accuracy remains a serious problem
The past few years have seen considerable improvement in the performance of unsupervised parsers CITATION and , for the first time , unsupervised parsers have been able to improve on the right-branching heuristic for parsing English
Some of these subsets were used for scoring in CITATION
Table 1 gives two baselines and the parsing results for WSJ10 , WSJ40 , Negra10 and Negra40 for recent unsupervised parsing algorithms: CCM and DMV+CCM CITATION , U-DOP CITATION and UML-DOP CITATION
There are several algorithms for doing so CITATION , which cluster words into classes based on the most frequent neighbors of each word
This restriction is inspired by psycholin-guistic research which suggests that humans process language incrementally CITATION
When Klein and Manning induce the parts-of-speech , they do so from a much larger corpus containing the full WSJ treebank together with additional WSJ newswire CITATION
This can either be semi-supervised parsing , using both annotated and unannotated data CITATION or unsupervised parsing , training entirely on unan-notated text
This problem is known in psycholinguistics as the problem of reanalysis CITATION
For large datasets , we use an ensemble technique inspired by Bagging CITATION
In particular , we consider an algorithm proposed by Camerini et al. CITATION which has a worst-case complexity of O(km log(n)) , where k is the number of parses we want , n is the number of words in the input sentence , and m is the number of edges in the hypothesis graph
The k-best MST algorithm we introduce in this paper is the algorithm described in Camerini et al. CITATION
Algorithm 1 is a version of the MST algorithm as presented by Camerini et al. CITATION; subtleties of the algorithm have been omitted
We have introduced the Camerini et al. CITATION k-best MST algorithm and have shown how to efficiently train MaxEnt models for dependency parsing
Many of the model features have been inspired by the constituency-based features presented in Charniak and Johnson CITATION
Other DP solutions use constituency-based parsers to produce phrase-structure trees , from which dependency structures are extracted CITATION
An efficient algorithm for generating the k-best parse trees for a constituency-based parser was presented in Huang and Chiang CITATION; a variation of that algorithm was used for generating projective dependency trees for parsing in Dreyer et al. CITATION and for training in McDonald et al. CITATION
The DP algorithms are generally variants of the CKY bottom-up chart parsing algorithm such as that proposed by Eisner CITATION
2 In order to explore a rich set of syntactic features in the MST framework , we can either approximate the optimal non-projective solution as in McDonald and Pereira CITATION , or we can use the constrained MST model to select a subset of the set of dependency parses to which we then apply less-constrained models
Unlike the training procedure employed by McDonald et al. CITATION and McDonald and Pereira CITATION , we provide positive and negative examples in the training data
A second labeling stage can be applied to get labeled dependency structures as described in CITATION
The Maximum Spanning Tree algorithm 1 was recently introduced as a viable solution for non-projective dependency parsing CITATION
McDonald et al. CITATION introduced a model for dependency parsing based on the Edmonds/Chu-Liu algorithm
Many of the features above were introduced in McDonald et al. CITATION; specifically , the node-type , inside , and edge features
We have adopted the conditional Maximum Entropy (MaxEnt) modeling paradigm as outlined in Char-niak and Johnson CITATION and Riezler et al. CITATION
Work on statistical dependency parsing has utilized either dynamic-programming (DP) algorithms or variants of the Edmonds/Chu-Liu MST algorithm (see Tarjan CITATION)
This can be reduced to O(kn 2) in dense graphs 4 by choosing appropriate data structures CITATION
Recently , a number of data-driven distortion models , based on lexical features and relative distance , have been proposed to compensate for this weakness CITATION
Following CITATION , we conduct a targeted evaluation; we only draw our evaluation pairs from the uncohesive subset targeted by our constraint
Fox CITATION showed that cohesion is held in the vast majority of cases for English-French , while Cherry and Lin CITATION have shown it to be a strong feature for word alignment
Fox CITATION demonstrated and counted cases where cohesion was not maintained in hand-aligned sentence-pairs , while Cherry and Lin CITATION showed that a soft cohesion constraint is superior to a hard constraint for word alignment
The most successful attempts at syntax-enhanced phrasal SMT have directly targeted movement modeling: Zens et al. CITATION modified a phrasal decoder with ITG constraints , while a number of researchers have employed syntax-driven source reordering before decoding begins CITATION
Our experimental set-up is modeled after the human evaluation presented in CITATION
Following CITATION , we provide the annotators with only short sentences: those with source sentences between 10 and 25 tokens long
These approaches were eventually superseded by tree transducers and tree substitution grammars , which allow translation events to span subtree units , providing several advantages , including the ability to selectively produce uncohesive translations CITATION
Syntactic cohesion 1 is the notion that all movement occurring during translation can be explained by permuting children in a parse tree CITATION
Previous approaches to measuring the cohesion of a sentence pair have worked with a word alignment CITATION
If one assumes arbitrary movement is possible , that alone is sufficient to show the problem to be NP-complete CITATION
We test our cohesion-enhanced Moses decoder trained using 688K sentence pairs of Europarl French-English data , provided by the SMT 2006 Shared Task CITATION
Phrase-based decoding CITATION is a dominant formalism in statistical machine translation
Early experiments with syntactically-informed phrases CITATION , and syntactic re-ranking of K-best lists CITATION produced mostly negative results
Restricting phrases to syntactic constituents has been shown to harm performance CITATION , so we tighten our definition of a violation to disregard cases where the only point of overlap is obscured by our phrasal resolution
We compare against an unmodified baseline decoder , as well as a decoder enhanced with a lexical reordering model CITATION
We modify the Moses decoder CITATION to translate head-annotated sentences
Following Lin and Cherry CITATION , we define a head span to be the projection of a single token e i onto the target phrase sequence: spanH (e i  ,T ,a m) = [a i  ,a i] and the subtree span to be the projection of the subtree rooted at ei: spanS (e i ,T ,a r( l) =   min  a , ,  max a k Consider the simple phrasal translation shown in Figure 1 along with a dependency tree for the English source
English dependency trees are provided by Minipar CITATION
Word alignments are provided by GIZA++ CITATION with grow-diag-final combination , with infrastructure for alignment combination and phrase extraction provided by the shared task
We first present our soft cohesion constraint's effect on BLEU score CITATION for both our dev-test and test sets
Weights for the log-linear model are set using MERT , as implemented by Venugopal and Vogel CITATION
Early methods for syntactic SMT held to this assumption in its entirety CITATION
This will take the form of a check performed each time a hypothesis is extended , similar to the ITG constraint for phrasal SMT CITATION
One approach is to leverage underlying word alignment quality such as in Ayan and Dorr CITATION
We measure translation performance by the BLEU CITATION and METEOR CITATION scores with multiple translation references
In a statistical generative word alignment model CITATION , it is assumed that (i) a random variable a specifies how each target word fj is generated by (therefore aligned to) a source 1 word e aj; and (ii) the likelihood function f (f , a|e) specifies a generative procedure from the source sentence to the target sentence
The language model is a statistical trigram model estimated with Modified Kneser-Ney smoothing CITATION using only English sentences in the parallel training data
Other methods do not depend on word alignments only , such as directly modeling phrase alignment in a joint generative way CITATION? pursuing information extraction perspective CITATION , or augmenting with modelbased phrase pair posterior CITATION
On the other hand , there are valid translation pairs in the training corpus that are not learned due to word alignment errors as shown in Deng and Byrne CITATION
The likelihood of those generative procedures can be accumulated to get the likelihood of the phrase pair CITATION
In the word-alignment derived phrase extraction approach , precision can be improved by filtering out most of the entries by using a statistical significance test CITATION
This is also the place where linguistic constraints can be applied , say to avoid non-compositional phrases CITATION
Second , some n-grams themselves carry no linguistic meaning; their phrase translations can be misleading , for example non-compositional phrases CITATION
In the final step 4 (line 15) , parameters {A k , t } are discriminatively trained on a development set using the downhill simplex method CITATION
By combining word alignments in two directions using heuristics CITATION , a single set of static word alignments is then formed
Two different word alignment models are trained as the baseline , one is symmetric HMM word alignment model , the other is IBM Model-4 as implemented in the GIZA++ toolkit CITATION
The translation probability can also be discriminatively trained such as in Tillmann and Zhang CITATION
WPPCR was used as one of the scores in CITATION for phrase extraction
The generic phrase training algorithm follows an information retrieval perspective as in CITATION but aims to improve both precision and recall with the trainable log-linear model
Methods have been proposed , based on syntax , that take advantage of linguistic constraints and alignment of grammatical structure , such as in Yamada and Knight CITATION and Wu CITATION
We then train HMM word alignment models CITATION in two directions simultaneously by merging statistics collected in the Algorithm 1 A Generic Phrase Training Procedure E-step from two directions motivated by Zens et al. CITATION with 5 iterations
Since most phrases appear only a few times in training data , a phrase pair translation is also evaluated by lexical weights CITATION?r term weighting CITATION as additional features to avoid overestimation
We used the SRI Language Modeling Toolkit CITATION to train a five-gram model with modified Kneser-Ney smoothing CITATION
Based on the source syntax parse tree , for each measure word , we identified its head word by using a toolkit from CITATION which can heuristically identify head words for sub-trees
We used an SMT system similar to Chiang CITATION , in which FBIS corpus is used as the bilingual training data
We ran GI-ZA++ CITATION on the training corpus in both directions with IBM model 4 , and then applied the refinement rule described in CITATION to obtain a many-to-many word alignment for each sentence pair
The most relevant work based on statistical methods to our research might be statistical technologies employed to model issues such as morphology generation CITATION
In most statistical machine translation (SMT) models CITATION , some of measure words can be generated without modification or additional processing
In addition to precision and recall , we also evaluate the Bleu score CITATION changes before and after applying our measure word generation method to the SMT output
In our work , the Berkeley parser CITATION was employed to extract syntactic knowledge from the training corpus
Specific to our ITG case , the M step becomes: (i+i) exp(pP(E(X ?[XX]) + ax )) p (i+i) exp(pp(E (X) + sax)) exp(pP(E(X - (X X)) + ax)) exp(pP(E (X) + sax)) ' exp(pp(E (X ?C) + ax )) P (l+ i)(e/f) exp(pp(E(X) + sax)) exp(pP(E (e/f) + ac)) exp(pp(E (C) + ma c))' where ip is the digamma function CITATION , s = 3?s the number of right-hand-sides for X , and m is the number of observed phrase pairs in the data
These word-level alignments are most often obtained using Expectation Maximization on the conditional generative models of Brown et al. CITATION and Vogel et al. CITATION
The traditional estimation method for word alignment models is the EM algorithm CITATION which iteratively updates parameters to maximize the likelihood of the data
The heuristic method is based on the Non-Compositional Constraint of Cherry and Lin CITATION
Kurihara and Sato CITATION describe VB for PCFGs , showing the only need is to change the M step of the EM algorithm
Minimum Error Rate training CITATION over BLEU was used to optimize the weights for each of these models over the development test data
Like Zhang and Gildea CITATION , it is used to prune bitext cells rather than score phrases
Our pruning differs from Zhang and Gildea CITATION in two major ways
The tic-tac-toe pruning algorithm CITATION uses dynamic programming to compute the product of inside and outside scores for all cells in O(n 4) time
Figure 2 compares the speed of the fast tic-tac-toe algorithm against the algorithm in Zhang and Gildea CITATION
We applied the decompounding algorithm proposed in Adda-Decker CITATION to our corpus to extract such compounds
Our experiments are based on word lattice output from the LIMSI German broadcast news transcription system CITATION , which employs 4-gram backoff language models
The evaluation scheme was taken from McTait and Adda-Decker CITATION
Beutler et al. CITATION pursued a similar approach
In order to compute the probability of a parse tree , it is transformed to a flat dependency tree similar to the syntax graph representation used in the TIGER treebank Brants et al CITATION
Other linguistically inspired language models like Chelba and Jelinek CITATION and Roark CITATION have been applied to continuous speech recognition
To extract such word clusters we used suffix arrays proposed in Ya-mamoto and Church CITATION and the pointwise mutual information measure
More accurate statistical models of natural language have mainly been developed in the field of statistical parsing , e.g.Collins CITATION , Charniak CITATION and Ratnaparkhi CITATION
Our grammar incorporates many ideas from existing linguistic work , e.g.Miiller CITATION , Muller CITATION , Crysmann CITATION , Crysmann CITATION
Our main source of dictionary information was Duden CITATION
This improvement is statistically significant on a level of < 0.1% for both the Matched Pairs Sentence-Segment Word Error test (MAPSSWE) and McNemar's test CITATION
Our particular variant requires that constituents (phrases) be continuous , but it provides a mechanism for dealing with discontinuities as present e.g.in the German main clause , see Kaufmann and Pfister CITATION
We used the Head-driven Phrase Structure Grammar (HPSG , see Pollard and Sag CITATION) formalism to develop a precise large-coverage grammar for German
Natural language processing research has addressed a number of these issues as individual problems: automatic punctuation CITATION , text segmentation CITATION disfluency repair CITATION and error correction CITATION
Following Strzalkowski and Brandow CITATION and Peters and Drexel CITATION we have implemented a transformation-based learning (TBL) algorithm CITATION
The recognition output is auto-punctuated by a method similar in spirit to the one proposed by Liu et al. CITATION before being passed to the transformation model
Deviating from Peters and Drexel CITATION , in the special case of an empty target sequence , i.e
Again deviating from Peters and Drexel CITATION , we consider two rules as overlapping if the left-hand-side of one is a contiguous subsequence of the other
The work of Ringger and Allen CITATION is similar in spirit to this method , but uses a factored source-channel model
before , during , etc.) CITATION
Queries are generated artificially using a method similar to Berger and Lafferty CITATION and used in Fleischman and Roy CITATION
Recent work in automatic image annotation CITATION and natural language processing CITATION , however , have demonstrated the advantages of using hierarchical Bayesian models for related tasks
We use the system of Bouthemy et al. CITATION which computes the camera motion using the parameters of a two-dimensional affine model to fit every pair of sequential frames in a video
The traditional text-only language models (which are also used below as baseline comparisons) are generated with the SRI language modeling toolkit CITATION using Chen and Goodman's modified Kneser-Ney discounting and interpolation CITATION
The method is based on the use of grounded language models to repre-sent the relationship between words and the non-linguistic context to which they refer CITATION
Although these correlations are not perfect , experiments have shown that baseball events can be classified using such features CITATION
CITATION
Thus , for a robot operating in a laboratory setting , words for colors and shapes may be grounded in the outputs of its computer vision system CITATION; while for a simulated agent operating in a virtual world , words for actions and events may be mapped to representations of the agent's plans or goals CITATION
Previous work has examined applying models often used in MT to the paired corpus described above CITATION
We follow previous work in sports video processing CITATION and define an event in a baseball video as any sequence of shots starting with a pitching-scene and continuing for four subsequent shots
Because these transcriptions are not necessarily time synched with the audio , we use the method described in Hauptmann and Witbrock CITATION to align the closed captioning to the announcers' speech
Recent work in video surveillance has demonstrated the benefit of representing complex events as temporal relations between lower level subevents CITATION
Recognizing speech in broadcast video is a necessary precursor to many multimodal applications such as video search and summarization CITATION
Shot detection and segmentation is a well studied problem; in this work we use the method of Tardini et al. CITATION
Although performance is often reasonable in controlled environments (such as studio news rooms) , automatic speech recognition (ASR) systems have significant difficulty in noisier settings (such as those found in live sports broadcasts) CITATION
Such video IR systems often use speech transcriptions to index segments of video in much the same way that words are used to index text documents CITATION
The WEKA machine learning package is used to train a boosted decision tree to classify these frames into one of three categories: pitching-scene , field-scene , other CITATION
Ando and Lee's CITATION kanji segmenter.) On the other hand , modelling only partial words helps the segmenter handle long , infrequent words
Finite-state models CITATION might be more compact
Brent CITATION and Venkataraman CITATION present incremental splitting algorithms with BF about 82% 3 on the Bernstein-Ratner (BR87) corpus of infant-directed English with disfluencies and interjections removed CITATION
Child-directed speech displays helpful features such as shorter phrases and fewer reductions CITATION
Learning to segment words is an old problem , with extensive prior work surveyed in CITATION
To build unsupervised algorithms , Brent and Cartwright suggested CITATION inferring phonotac-tic constraints from phone sequences observed at phrase boundaries
Feature-based or gestural phonology CITATION might help model segmental variation
Simple supervised algorithms perform extremely well CITATION , but don't address our main goal: learning how to segment
Statistics of phone trigrams provide sufficient information to segment adult conversational speech (dictionary transcriptions with simulated phonology) with about 90% precision and 93% recall CITATION , see also CITATION
Early results using neural nets by Cairns et al. CITATION and Christiansen et al CITATION are discouraging
See also CITATION
Word segmentation experiments by Christiansen and Allen CITATION and Harrington et al. CITATION simulated the effects of pronunciation variation and/or recognizer error.
Attempts to segment transcriptions without pauses , e.g. CITATION , have worked poorly
Disfluen-cies in conversational speech create pauses where you might not expect them , e.g.immediately following the definite article CITATION
The other two English dictionary transcriptions were produced in a similar way from the Buckeye corpus CITATION and Mississippi State's corrected version of the LDC's Switchboard transcripts CITATION
The most recent algorithm CITATION achieves a BF of 85.8% using a Dirichlet Process bigram model , estimated using a Gibbs sampling algorithm
Supervised phonotactic methods date back at least to CITATION , see also CITATION
This issue was noted by Harrington et al. CITATION who used a list of known very short words to detect these cases
Prosody , stress , and other sub-phonemic cues might disambiguate some problem situations CITATION
Some words are "massively" reduced CITATION , going well beyond standard phonological rules
Claims that humans can extract words without pauses seem to be based on psychological experiments such as CITATION which conflate words and morphemes
For example , Figure 1 shows a transcribed phrase from the Buckeye corpus CITATION and the automatically segmented output
Even then , explicit boundaries seem to improve performance CITATION
Segmentation by adults is sensitive to phono-tactic constraints CITATION
The Spanish corpus was produced in a similar way from the Callhome Spanish dataset CITATION , removing all accents
Query expansion CITATION is a commonly used strategy to bridge the vocabulary gaps by expanding original queries with related terms
The more common words the definitions of two terms have , the more similar these terms are CITATION
Most information retrieval models CITATION compute relevance scores based on matching of terms in queries and documents
Model Axiomatic approaches have recently been proposed and studied to develop retrieval functions CITATION
In CITATION , several axiomatic retrieval functions have been derived based on a set of basic formalized retrieval constraints and an inductive definition of the retrieval function space
In this paper , we use the best performing function derived in axiomatic retrieval models , i.e , F2-EXP in CITATION with a fixed parameter value (b = 0.5)
Expanded terms are often selected from either co-occurrence-based thesauri CITATION or handcrafted thesauri CITATION or both CITATION
In this paper , we re-examine the problem of query expansion using lexical resources with the recently proposed axiomatic approaches CITATION
According to the retrieval performance , the proposed similarity function is significantly better than simple mutual information based similarity function , while it is comparable to the function proposed in CITATION
To overcome this limitation , in CITATION , we proposed a set of semantic term matching constraints and modified the previously derived axiomatic functions to make them satisfy these additional constraints
In our previous study CITATION , term similarity function s is derived based on the mutual information of terms over collections that are constructed under the guidance of a set of term semantic similarity constraints
The parameter sensitivity is similar to the observations described in CITATION and will not be discussed in this paper
s MIBL uses the collection itself to compute the mutual information , while s MIImp uses the working sets con-structed based on several constraints CITATION
We first compare the retrieval performance of query expansion with different similarity functions using short keyword (i.e. , title-only) queries , because query expansion techniques are often more effective for shorter queries CITATION
In this paper , we study several term similarity functions that exploit various information from two lexical resources , i.e. , WordNet and dependency-thesaurus constructed by Lin CITATION , and then incorporate these similarity functions into the axiomatic retrieval framework
In this section , we discuss a set of term similarity functions that exploit the information stored in two lexical resources: WordNet CITATION and dependency-based thesaurus CITATION
Another lexical resource we study in the paper is the dependency-based thesaurus provided by Lin 1 CITATION
The most commonly used lexical resource is WordNet CITATION , which is a hand-crafted lexical system developed at Princeton University
However , previous studies failed to show any significant gain in retrieval performance when queries are expanded with terms selected from WordNet CITATION
By incorporating this similarity function into the axiomatic retrieval models , we show that query expansion using the information from only WordNet can lead to significant improvement of retrieval performance , which has not been shown in the previous studies CITATION
Voorhees CITATION showed that using WordNet for word sense disambiguation degrade the retrieval performance
Stemming is related to query expansion or query reformulation CITATION , although the latter is not limited to word variants
Because err(W) is a convex function of W , it has a global minimum and obtains its minimum when the gradient is zero CITATION
Therefore , we use the forward-backward algorithm CITATION to calculate P(e ij) in a more efficient way
There are several regression models , ranging from the simplest linear regression model to non-linear alternatives , such as a neural network () , a Regression SVM CITATION
This inconsistency may result in severe problems when the scales of feature values vary dramatically CITATION
Then we do corpus analysis to filter out the words which are clustered incorrectly , according to word distributional similarity , following CITATION
UMASS: This is the result reported in CITATION using Porter stemming for both document and query terms
Most stemmers , such as the Porter stemmer CITATION and Krovetz stemmer CITATION , deal with stemming by stripping word suffixes according to a set of morphological rules
Among them , the Porter stemmer CITATION is the most widely used
The second feature is an extension to point-wise mutual information CITATION , defined as follows: ^P(controlling...acidic ,rain I window)^ P(controlling) P(acidic)P(rain) where P(controlling...acidic...rainlwindow) is the co-occurrence probability of the trigram containing acidic within a predefined window (50 words)
The Indri 2.5 search engine CITATION is used as our basic retrieval system
To better determine stemming rules , Xu and Croft CITATION propose a selective stemming method based on corpus analysis
Xu and Croft CITATION create equivalence clusters of words which are morphologically similar and occur in similar contexts
This approach is similar to the work of Xu and Croft CITATION , and can be considered as another state-of-the-art result
Question answering CITATION relates to question search
Conventional vector space models are used to calculate the statistical similarity and WordNet CITATION is used to estimate the semantic similarity
Note that the root node of a question tree is associated with empty string as the definition of prefix tree requires CITATION
Sneiders CITATION proposed template based FAQ retrieval systems
The MDL-based tree cut model was originally introduced for handling the problem of generalizing case frames using a thesaurus CITATION
Jeon and Bruce CITATION proposed a mixture model for fixing the lexical chasm between questions
For example , Jeon et al. CITATION compared four different retrieval methods , i.e
MDL is a principle of data compression and statistical estimation from information theory CITATION
FAQ Finder CITATION heuristically combines statistical similarities and semantic similarities between questions to rank FAQs
Harabagiu et al. CITATION used a Question Answer Database (known as QUAB) to support interactive question answering
A BaseNP is defined as a simple and non-recursive noun phrase CITATION
Lai et al. CITATION proposed an approach to automatically mine FAQs from the Web
Previous work has shown that modeling the relation between personality and language is far from trivial CITATION , suggesting that the control of personality is a harder problem than the control of data-driven variation dimensions
One line of work has primarily focused on gram-maticality and naturalness , scoring the overgener-ation phase with a SLM , and evaluating against a gold-standard corpus , using string or tree-match metrics CITATION
While handcrafted rule-based approaches are limited to variation along a small number of discrete points CITATION , we learn models that predict parameter values for any arbitrary value on the variation dimension scales
Following Mairesse and Walker CITATION , two expert judges (not the authors) familiar with the Big Five adjectives (Table 1) evaluate the personality of each utterance using the Ten-Item Personality Inventory CITATION , and also judge the utterance's naturalness
Subjects evaluate the naturalness and personality of each utterance using the TIPI CITATION
We present a new method for generating linguistic variation projecting multiple personality traits continuously , by combining and extending previous research in statistical natural language generation CITATION
Langkilde and Knight CITATION first applied SLMs to statistical natural language generation (SNLG) , showing that high quality paraphrases can be generated from an underspecified representation of meaning , by first applying a very undercon-strained , rule-based overgeneration phase , whose outputs are then ranked by an SLM scoring phase
Over the last 20 years , statistical language models (SLMs) have been used successfully in many tasks in natural language processing , and the data available for modeling has steadily grown CITATION
both introverted and extraverted personality types CITATION
See CITATION for more detail
Section 3.2 shows that humans accurately perceive the intended variation , and Section 3.3 compares P ersonage-PE (trained) with P ersonage CITATION
We start with the P ersonage generator CITATION , which generates recommendations and comparisons of restaurants
Mairesse and Walker CITATION show that this approach generates utterances that are perceptibly different along the extraversion dimension
Table 9 shows that the average naturalness is 3.98 out of 7 , which is significantly lower (p < .05) than the naturalness of handcrafted and randomly generated utterances reported by Mairesse and Walker CITATION
isard et al. CITATION and Mairesse and Walker CITATION also propose a personality generation method , in which a data-driven personality model selects the best utterance from a large candidate set
Even though the parameters of P ersonage-PE were suggested by psychological studies CITATION , some of them are not modeled successfully by our approach , and thus omitted from Tables 3 and 4
Third , there are many studies linking personality to linguistic variables CITATION
These parameters are derived from psychological studies identifying linguistic markers of the Big Five traits CITATION
These correlations are unexpectedly high; in corpus analyses , significant correlations as low as .05 to .10 are typically observed between personality and linguistic markers CITATION
Another thread investigates SNLG scoring models trained using higher-level linguistic features to replicate human judgments of utterance quality CITATION
A third SNLG approach eliminates the overgeneration phase CITATION
There is only one other similar evaluation of an SNLG CITATION
We compare various learning algorithms using the Weka toolkit CITATION
Similar strategies with parse trees are pursued in CITATION , and error templates are utilized in CITATION for a word processor
Research on automatic grammar correction has been conducted on a number of different parts-of-speech , such as articles CITATION and prepositions CITATION
Automatic error detection has been performed on other parts-of-speech , e.g. , articles CITATION and prepositions CITATION
For example , the sentence "My father is *work in the laboratory" is parsed CITATION as: The progressive form "working" is substituted with its bare form , which happens to be also a noun
After parsing the corpus CITATION , we artificially introduced verb form errors into these sentences , and observed the resulting "disturbances" to the parse trees
Using these patterns , we introduced verb form errors into Aquaint , then re-parsed the corpus CITATION , and compiled the changes in the "disturbed" trees into a catalog
Errors in verb forms have been covered as part of larger systems such as CITATION , but we believe that their specific research challenges warrant more detailed examination
For example , in the Japanese Learners of English corpus CITATION , errors related to verbs are among the most frequent categories
A maximum entropy model , using lexical and POS features , is trained in CITATION to recognize a variety of errors
ual evaluation of HKUST is 0.76 , corresponding to "substantial agreement" between the two evalu-ators CITATION
Hand-crafted error production rules (or "mal-rules") , augmenting a context-free grammar , are designed for a writing tutor aimed at deaf students CITATION
An approach combining a hand-crafted context-free grammar and stochastic probabilities is pursued in CITATION , but it is designed for a restricted domain only
The OpenCCG surface realizer is based on Steed-man's CITATION version of CCG elaborated with Baldridge and Kruijff's multi-modal extensions for lexically specified derivation control CITATION and hybrid logic dependency semantics CITATION
Internally , such graphs are represented using Hybrid Logic Dependency Semantics (HLDS) , a dependency-based approach to representing linguistic meaning developed by Baldridge and Kruijff CITATION
A relatively recent technique for lexical category assignment is supertagging CITATION , a preprocessing step to parsing that assigns likely categories based on word and part-of-speech (POS) contextual information
We have introduced a novel type of supertagger , which we have dubbed a hypertagger , that assigns CCG category labels to elementary predications in a structured semantic representation with high accuracy at several levels of tagging ambiguity in a fashion reminiscent of CITATION
Assigned categories are instantiated in OpenCCG's chart realizer where , together with a treebank-derived syntactic grammar CITATION and a factored language model CITATION , they constrain the English word-strings that are chosen to express the LF
OpenCCG implements a symbolic-statistical chart realization algorithm CITATION combining (1) a theoretically grounded approach to syntax and semantic composition with (2) factored language models CITATION for making choices among the options left open by the grammar
In HLDS , hybrid logic CITATION terms are used to describe dependency graphs
A separate transformation then uses around two dozen generalized templates to add logical forms to the categories , in a fashion reminiscent of CITATION
To illustrate the input to OpenCCG , consider the semantic dependency graph in Figure 1 , which is taken from section 00 of a Propbank-enhanced version of the CCGbank CITATION
Even with the current incomplete set of semantic templates , the hy-pertagger brings realizer performance roughly up to state-of-the-art levels , as our overall test set B LEU score CITATION slightly exceeds that of Cahill and van Genabith CITATION , though at a coverage of 96% instead of98%
In this regard , our approach is more similar to the ones pursued more recently by Carroll , Oepen and Velldal CITATION , Nakanishi et al. CITATION and Cahill and van Genabith CITATION with HPSG and LFG grammars
Our approach follows Langkilde-Geary CITATION and Callaway CITATION in aiming to leverage the Penn Treebank to develop a broad-coverage surface re-alizer for English
One way of performing lexical assignment is simply to hypothesize all possible lexical categories and then search for the best combination thereof , as in the CCG parser in CITATION or the chart realizer in CITATION
Supertagging has been more recently extended to a multitagging paradigm in CCG CITATION , leading to extremely efficient parsing with state-of-the-art dependency recovery CITATION
Clark CITATION notes in his parsing experiments that the POS tags of the surrounding words are highly informative
White et al. CITATION describe an ongoing effort to engineer a grammar from the CCGbank CITATION ?a corpus of CCG derivations derived from the Penn Treebank ?suitable for realization with OpenCCG
While these models are considerably smaller than the ones used in CITATION , the training data does have the advantage of being in the same domain and genre (using larger n-gram models remains for future investigation)
We caution , however , thatitremains unclear how meaningful it is to directly compare these scores when the realizer inputs vary considerably in their specificity , as Langkilde-Geary's CITATION experiments dramatically illustrate
In the two-stage mode , a packed forest of all possible realizations is created in the first stage; in the second stage , the packed representation is unpacked in bottom-up fashion , with scores assigned to the edge for each sign as it is unpacked , much as in CITATION
Moreover , the overall Bleu CITATION and Meteor CITATION scores , as well as numbers of exact string matches (as measured against to the original sentences in the CCGbank) are higher for the hypertagger-seeded realizer than for the preexisting realizer
We used Zhang Le's maximum entropy toolkit 4 for training the hypertagging model , which uses an implementation of Limited-memory BFGS , an approximate quasi-Newton optimization method from the numerical optimization literature CITATION
Example (1) shows how numbered semantic roles , taken from PropBank CITATION when available , are added to the category of an active voice , past tense transitive verb , where *pred* is a placeholder for the lexical predicate; examples (2) and (3) show how more specific relations are introduced in the category for determiners and the category for the possessive's , respectively
In lexicalized grammatical formalisms such as Lexicalized Tree Adjoining Grammar CITATION , Combinatory Categorial Grammar CITATION and Head-Driven Phrase-Structure Grammar CITATION , it is possible to separate lexical category assignment ?the assignment of informative syntactic categories to linguistic objects such as words or lexical predicates ?from the combinatory processes that make use of such categories ?such as parsing and surface realization
The language models were created using the SRILM toolkit CITATION on the standard training sections (2-21) of the CCGbank , with sentence-initial words (other than proper names) uncapital-ized
2 Second , we compare a hypertagger-augmented version of OpenCCG's chart realizer with the preexisting chart realizer CITATION that simply instantiates the chart with all possible CCG categories (subject to frequency cutoffs) for each input LF predicate
Following White et al. CITATION , we use factored tri-gram models over words , part-of-speech tags and supertags to score partial and complete realizations
In White et al.'s CITATION initial investigation of scaling up OpenCCG for broad coverage realization , test set   grammar complete oracle / best dev (00)  dev?9.1%/47.8% train?7.5%/22.6% all categories observed more often than a threshold frequency were instantiated for lexical predicates; for unseen words , a simple smoothing strategy based on the part of speech was employed , assigning the most frequent categories for the POS
Additionally , to realize a wide range of paraphrases , OpenCCG implements an algorithm for efficiently generating from disjunctive logical forms CITATION
Informally , a packed parse forest , or forest in short , is a compact representation of all the derivations (i.e. , parse trees) for a given sentence under a context-free grammar CITATION
Such a forest has a structure of a hypergraph CITATION , where items like NP 0  3 are called nodes , and deductive steps like (*) correspond to hyperedges
Both tasks can be done efficiently by forest-based algorithms based on k-best parsing CITATION
Basically , cube pruning works bottom up in a forest , keeping at most k +LM items at each node , and uses the best-first expansion idea from the Algorithm 2 of Huang and Chiang CITATION to speed up the computation
For k-best search after getting 1-best derivation , we use the lazy Algorithm 3 of Huang and Chiang CITATION that works backwards from the root node , incrementally computing the second , third , through the kth best alternatives
However , a k-best list , with its limited scope , often has too few variations and too many redundancies; for example , a 50-best list typically encodes a combination of 5 or 6 binary ambiguities (since 2 5 6) , and many subtrees are repeated across different parses CITATION
We use the pruning algorithm of CITATION that is very similar to the method based on marginal probability CITATION , except that it prunes hyperedges as well as nodes
Following Huang CITATION , we modify the parser to output a packed forest for each sentence
The corresponding BLEU score of Pharaoh CITATION is 0.2182 on this dataset
We use the standard minimum error-rate training CITATION to tune the feature weights to maximize the system's BLEU score on the dev set
Such approaches have been shown to be effective in log-linear word-alignment models where only a small supervised corpus is available CITATION
Promising features might include those over source side reordering rules CITATION or source context features CITATION
For this reason , to our knowledge , all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures , such that spurious ambiguity is lessened or removed entirely CITATION , or else ignore the problem and treat derivations as translations CITATION
To our knowledge no systems directly address Problem 1 , instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list CITATION , or else making local independence assumptions which side-step the issue CITATION
The standard solution is to approximate the maximum probability translation using a single derivation CITATION
A synchronous context free grammar (SCFG) consists of paired CFG rules with co-indexed nonterminals CITATION
Both the global models CITATION use fairly small training sets , and there is no evidence that their techniques will scale to larger data sets
2 This results in the following log-likelihood objective and corresponding gradient: L =  ?logp A (e|f) + J>gp o(A fc) (4) d L a 2 (5) In order to train the model , we maximise equation (4) using L-BFGS CITATION
Our findings echo those observed for latent variable log-linear models successfully used in monolingual parsing CITATION
This method has been demonstrated to be effective for (non-convex) log-linear models with latent variables CITATION
This is an instance of the ITG alignment algorithm CITATION
Charniak and Johnson CITATION use a PCFG to do a pass of inside-outside parsing to reduce the state space of a subsequent lexicalized n-best parsing algorithm to produce parses that are further re-ranked by a MaxEnt model
In the past few years , a "standard model" of scope underspecification has emerged: A range of formalisms from Underspecified DRT CITATION to dominance graphs CITATION have offered mechanisms to specify the "semantic material" of which the semantic representations are built up , plus dominance or outscoping relations between these building blocks
In this paper , we consider dominance graphs CITATION as one representative of this class
Furthermore , there are algorithms for determinizing weighted tree automata CITATION , which could be applied as preprocessing steps for wRTGs
The algorithms generalize easily to weights that are taken from an arbitrary ordered semiring CITATION and to computing minimal-weight rather than maximal-weight configurations
Underspecification CITATION has become the standard approach to dealing with scope ambiguity in large-scale hand-written grammars
Redundancy elimination CITATION is the problem of deriving from an USR U another USR U ' , such that the readings of U ' are a proper subset of the read-ings of U  , but every reading in U is semantically equivalent to some reading in U '
Regular tree grammars CITATION are a standard approach for specifying sets of trees in theoretical computer science , and are closely related to regular tree transducers as used e.g.in recent work on statistical MT CITATION and grammar formalisms CITATION
See Comon et al. CITATION for more details
The Rondane treebank is a "Redwoods style" treebank CITATION containing MRS-based underspecified representations for sentences from the tourism domain , and is distributed together with the English Resource Grammar (ERG) CITATION
On the theoretical side , Ebert CITATION has shown that none of the major underspecification formalisms are expressively complete , i.e
Because every finite tree language is regular , RTGs constitute an expressively complete underspecifica-tion formalism in the sense of Ebert CITATION: They can represent arbitrary subsets of the original set of readings
Ebert CITATION proves that no expressively complete underspecifi-cation formalism can be compact , i.e
The precise definition of dominance nets is not important here , but note that virtually all underspecified descriptions that are produced by current grammars are nets CITATION
A weighted regular tree grammar (wRTG) CITATION is a 5-tuple G = (S ,N ,Z , R , c) such that G' = (S , N , Z , R) is a regular tree grammar and c : R ?R is a function that assigns each production rule a weight
Furthermore , we show how to define a PCFG-style cost model on RTGs and compute best readings of deterministic RTGs efficiently , and illustrate this model on a machine learning based model of scope preferences CITATION
Weighted dominance graphs can be used to encode the standard models of scope preferences CITATION
Tree automata are related to tree transducers as used e.g.in statistical machine translation CITATION exactly like finite-state string automata are related to finite-state string transducers , i.e
For example , Knight and Graehl CITATION present an algorithm to extract the best derivation of a wRTG in time O(t + n log n) where n is the number of nonterminals and t is the number of rules
One can then strengthen the underspecified description to efficiently eliminate subsets of readings that were not intended in the given context CITATION; so when the individual readings are eventually computed , the number of remaining readings is much smaller and much closer to the actual perceived ambiguity of the sentence
Indeed , for one particular grammar formalism it has even been shown that the parse chart contains an isomorphic image of a dominance chart CITATION
We show that the "dominance charts" proposed by Koller and Thater CITATION can be naturally seen as regular tree grammars; using their algorithm , classical underspecified descriptions (dominance graphs) can be translated into RTGs that describe the same sets of readings
This simplifies semantics construction , and current algorithms CITATION support the efficient enumeration of readings from an USR when it is necessary
Koller and Thater CITATION demonstrate how to compute a dominance chart from a dominance graph D by tabulating how a subgraph can be decomposed into smaller subgraphs by removing what they call a "free fragment"
In the worst case , the dominance chart of a dominance graph with n fragments has O(2 n) production rules CITATION , i.e
This has been a very successful approach , but recent algorithms for eliminating subsets of readings have pushed the expres-sive power of these formalisms to their limits; for instance , Koller and Thater CITATION speculate that further improvements over their (incomplete) redundancy elimination algorithm require a more expressive formalism than dominance graphs
We exploit this increase in expressive power in presenting a novel redundancy elimination algorithm that is simpler and more powerful than the one by Koller and Thater CITATION; in our algorithm , redundancy elimination amounts to intersection of regular tree languages
Ebert shows that the classical dominance-based underspecification formalisms , such as MRS , Hole Semantics , and dominance graphs , are all expressively incomplete , which Koller and Thater CITATION speculate might be a practical problem for algorithms that strengthen USRs to remove unwanted readings
Koller and Thater CITATION define semantic equivalence in terms of a rewrite system that specifies under what conditions two quantifiers may exchange their positions without changing the meaning of the semantic representation
Based on this definition , Koller and Thater CITATION present an algorithm (henceforth , KT06) that deletes rules from a dominance chart and thus removes subsets of readings from the USR
4 of Koller and Thater CITATION completely , whereas KT06 won't
We use a slightly weaker version of the rewrite system that Koller and Thater CITATION used in their evaluation
For instance , both Combinatory Categorial Grammars CITATION and synchronous grammars CITATION represent syntactic and semantic ambiguity as part of the same parse chart
An important class of dominance graphs are hy-pernormally connected dominance graphs , or dominance nets CITATION
It is also useful in applications beyond semantic construction , e.g.in discourse parsing CITATION
The problem of computing the best tree is NP-complete CITATION
Since CITATION , numerous works have used patterns for discovery and identification of instances of semantic relationships (e.g. , CITATION)
To improve results , some systems utilize additional manually constructed semantic resources such as WordNet (WN) CITATION
Among the 15 systems presented by the 14 SemEval teams , some utilized the manually provided WordNet tags for the dataset pairs (e.g. , CITATION)
For reference , the best results overall CITATION are also shown
Many other works manually develop a set of heuristic features devised with some specific relationship in mind , like a WordNet-based meronymy feature CITATION or size-of feature CITATION
The winning algorithms were LWL CITATION , SMO CITATION , and K* CITATION (there were 7 tasks , and different algorithms could be selected for each task)
Some other systems that avoided using the labels used WN as a supporting resource for their algorithms CITATION
Table 1 shows our results , along with the best Task 4 result not using WordNet labels CITATION
To specify patterns , following CITATION we classify words into high-frequency words (HFWs) and content words (CWs)
For each nominal pair (w 1 ,w 2) in a given sentence S , we use a method similar to CITATION to extract words that have a shared meaning with w 1 or w 2
In CITATION we present an approach to extract pattern clusters from an untagged corpus
In CITATION we describe the algorithm at length , discuss its behavior and parameters in detail , and evaluate its intrinsic quality
This corpus was extracted from the web starting from open directory links , comprising English web pages with varied topics and styles CITATION
Common choices include variations of SVM CITATION , decision trees and memory-based learners
For example , in noun compounds many different semantic relationships are encoded by the same simple form CITATION: 'dog food' denotes food consumed by dogs , while 'summer morn-ing' denotes a morning that happens in the summer
Recently , SemEval-07 Task 4 CITATION proposed a benchmark dataset that includes a subset of 7 widely accepted nominal relationship (NR) classes , allowing consistent evaluation of different NR classification algorithms
The most recent dataset has been developed for SemEval 07 Task 4 CITATION
In our evaluation we have selected the setup and data from SemEval-07 Task 4 CITATION
Task 4 CITATION involves classification of relationships between simple nominals other than named entities
A leading method for utilizing context information for classification and extraction of relationships is that of patterns CITATION
Several different relationship hierarchies have been proposed CITATION
A wide variety of features are used by different algorithms , ranging from simple bag-of-words frequencies to WordNet-based features CITATION
Moldovan et al. CITATION proposed a different scheme with 35 classes
Since the SemEval dataset is of a very specific nature , we have also applied our classification framework to the CITATION dataset , which contains 600 pairs labeled with 5 main relationship types
We have used the exact evaluation procedure described in CITATION , achieving a class f-score average of 60.1 , as opposed to 54.6 in CITATION and 51.2 in CITATION
Many relationship classification methods utilize some language-dependent preprocessing , like deep or shallow parsing , part of speech tagging and named entity annotation CITATION
Strategies were developed for discovery of multiple patterns for some specified lexical relationship CITATION and for unsuper-vised pattern ranking CITATION
Other resources used for relationship discovery include Wikipedia CITATION , thesauri or synonym sets CITATION and domain-specific semantic hierarchies like MeSH CITATION
Rosenfeld and Feldman CITATION discover relationship instances by clustering entities appearing in similar contexts
Relationship classification is known to improve many practical tasks , e.g. , textual entailment CITATION
Freely available tools like Weka CITATION allow easy experimentation with common learning algorithms CITATION
All occurrences of these verbs with a subject noun were next extracted from a RASP parsed CITATION version of the British National Corpus (BNC)
Following previous work CITATION , we optimized its parameters on a word-based semantic similarity task
In addition , Bullinaria and Levy CITATION found that these parameters perform well on a number of other tasks such as the synonymy task from the Test of English as a Foreign Language (TOEFL)
Examples include automatic thesaurus extraction CITATION , word sense discrimination CITATION and disambiguation CITATION , collocation extraction CITATION , text segmentation CITATION  , and notably information retrieval CITATION
NLP tasks that could benefit from composition models include paraphrase identification and context-dependent language modeling CITATION
In order to establish which ones fit our data better , we examined whether the correlation coefficients achieved differ significantly using a t-test CITATION
While vector addition has been effective in some applications such as essay grading CITATION and coherence assessment CITATION , there is ample empirical evidence that syntactic relations across and within sentences are crucial for sentence and discourse processing CITATION and modulate cognitive behavior in sentence priming CITATION and inference tasks CITATION
For example , assuming that individual words are represented by vectors , we can compute the meaning of a sentence by taking their mean CITATION
Specifically , they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Con-rath CITATION measure
Vector-based models of word meaning CITATION have become increasingly popular in natural language processing (NLP) and cognitive science
In cognitive science vector-based models have been successful in simulating semantic priming CITATION and text comprehension CITATION
This is illustrated in the example below taken from Landauer et al. CITATION
Previous applications of vector addition to document indexing CITATION or essay grading CITATION were more concerned with modeling the gist of a document rather than the meaning of its sentences
Moreover , the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments CITATION and word association norms CITATION
Computational models of semantics which use symbolic logic representations CITATION can account naturally for the meaning of phrases or sentences
multi-task learning CITATION in which the task (and label set) is allowed to vary from source to target
Most of this prior work deals with supervised transfer learning , and thus requires labeled source domain data , though there are examples of unsupervised CITATION , semi-supervised CITATION , and transductive approaches CITATION
Some of the first formulations of the transfer learning problem were presented over 10 years ago CITATION
Other techniques have tried to quantify the generalizability of certain features across domains CITATION , or tried to exploit the common structure of related problems CITATION
These are: abstracts from biological journals [UT CITATION , Yapex CITATION]; news articles [MUC6 CITATION , MUC7 CITATION]; and personal e-mails [CSPACE CITATION]
When the task being learned varies (say , from identifying person names to identifying protein names) , the problem is called multi-task learning CITATION
One recently proposed method CITATION for transfer learning in Maximum Entropy models 1 involves modifying the /t's of this Gaussian prior
To avoid overfitting the training data , these A's are often further constrained by the use of a Gaussian prior CITATION with diagonal co-variance , N(/x , a 2) , which tries to maximize: where f3 > 0 is a parameter controlling the amount of regularization , and N is the number of sentences in the training set
Representing feature spaces with this kind oftree , besides often coinciding with the explicit language used by common natural language toolkits CITATION , has the added benefit of allowing a model to easily back-off , or smooth , to decreasing levels of specificity
We used a standard natural language toolkit CITATION to compute tens of thousands of binary features on each of these tokens , encoding such information as capitalization patterns and contextual information from surrounding words
When only the type of data being examined is allowed to vary (from news articles to e-mails , for example) , the problem is called domain adaptation CITATION
Daume allows an extra degree of freedom among the features of his domains , implicitly creating a two-level feature hierarchy with one branch for general features , and another for domain specific ones , but does not extend his hierarchy further CITATION)
In this work , we will base our work on Conditional Random Fields (CRF's) CITATION , which are now one of the most preferred sequential models for many natural language processing tasks
Recent work using so-called meta-level priors to transfer information across tasks CITATION , while related , does not take into explicit account the hierarchical structure ofthese meta-level features often found in NLP tasks
It has been shown empirically that , while the significance of particular features might vary between domains and tasks , certain generalized classes of features retain their importance across domains CITATION
One common way of addressing the transfer learning problem is to use a prior which , in conjunction with a probabilistic model , allows one to specify a priori beliefs about a distribution , thus biasing the results a learning algorithm would have produced had it only been allowed to see the training data CITATION
Similarly , work on hierarchical penalization CITATION in two-level trees tries to produce models that rely only on a relatively small number of groups of variable , as structured by the tree , as opposed to transferring knowledge between branches themselves
Almost all the current event extraction systems focus on processing single documents and , except for coreference resolution , operate a sentence at a time CITATION
We use a state-of-the-art English IE system as our baseline CITATION
Mann CITATION encoded specific inference rules to improve extraction of CEO (name , start year , end year) in the MUC management succession task
In addition , Patwardhan and Ri-loff CITATION also demonstrated that selectively applying event patterns to relevant regions can improve MUC event extraction
We then use the INDRI retrieval system CITATION to obtain the top N (N=25 in this pa-per 3) related documents
Yangarber et al. CITATION applied cross-document inference to correct local extraction results for disease name , location and start/end time
Several recent studies involving specific event types have stressed the benefits of going beyond traditional single-document extraction; in particular , Yangarber CITATION has emphasized this potential in his work on medical information extraction
Heng Ji?alph Grishman Computer Science Department New York University New York , NY 10003 , USA (hengji , grishman)@cs.nyu.edu Abstract We apply the hypothesis of "One Sense Per Discourse" CITATION to information extraction (IE) , and extend the scope of "discourse" from one single document to a cluster of topically-related documents
The trigger labeling task described in this paper is in part a task of word sense disambiguation (WSD) , so we have used the idea of sense consistency introduced in CITATION , extending it to operate across related documents
The correlated topic model CITATION is one way to account for relationships between hidden topics; more structured representations , such as hierarchies , may also be considered
We employ Gibbs sampling , previously used in NLP by Finkel et al. CITATION and Goldwater et al. CITATION , among others
Our approach relates to previous work on property extraction from reviews CITATION
for a discussion of similarity metrics , see Lin CITATION
For this purpose , we use the Rand Index CITATION , a measure of cluster similarity
Recent work has examined coupling topic models with explicit supervision CITATION
become widely available CITATION
These range from supervised classification CITATION to instantiations of the noisy-channel model CITATION , to clustering CITATION , and methods inspired by information retrieval CITATION
Barnard et al. CITATION propose a hierarchical latent model in order to account for the fact that some words are more general than others
More sophisticated graphical models CITATION have also been employed including Gaussian Mixture Models (GMM) and Latent Dirichlet Allocation (LDA)
Specifically , we use Latent Dirichlet Allocation (LDA) as our topic model CITATION
Duygulu et al. CITATION improve on this model by treating image regions and keywords as a bi-text and using the EM algorithm to construct an image region-word dictionary
Typically , the k-best words are taken to be the automatic annotations for a test image I CITATION where k is a small number and the same for all images
Our evaluation follows the experimental methodology proposed in Duygulu et al. CITATION
For instance , resources like WordNet CITATION can be used to expand the annotations by exploiting information about is-a relationships
Finally , relevance models originally developed for information retrieval , have been successfully applied to image annotation CITATION
We are more interested in modeling the presence or absence of words in the annotation and thus use the multiple-Bernoulli distribution to generate words CITATION
Using a grid avoids unnecessary errors from image segmentation algorithms , reduces computation time , and simplifies parameter estimation CITATION
Standard latent semantic analysis (LSA) and its probabilistic variant (PLSA) have been applied to this task CITATION
Specifically , we extend and modify Lavrenko's CITATION continuous relevance model to suit our task
Our work is an extension of the continuous relevance annotation model put forward in Lavrenko et al. CITATION
The continuous relevance image annotation model CITATION generatively learns the joint probability distribution P(V , W) of words W and image regions V
When estimating P(V I\s) , the probability of image regions and words , Lavrenko et al. CITATION reasonably assume a generative Gaussian kernel distribution for the image regions: where N vi is the number of regions in image I , v r the feature vector for region r in image i , n s v the number of regions in the image of latent variable s , v i the feature vector for region i in s's image , k the dimension of the image feature vectors and ?the feature covari-ance matrix
Lavrenko et al. CITATION estimate the word probabilities P(W I\s) using a multinomial distribution
Our third baseline is Lavrenko et al.'s CITATION continuous relevance model
We compare the annotation performance of the model proposed in this paper (ExtModel) with Lavrenko et al.'s CITATION original continuous relevance model (Lavrenko03) and two other simpler models which do not take the image into account (tf* idfand Doc-Title)
Incidentally , LDA can be also used to rerank the output of Lavrenko et al.'s CITATION model
4 Interestingly , the latter yields precision similar to Lavrenko et al. CITATION
The co-occurrence model CITATION collects co-occurrence counts between words and image features and uses them to predict annotations for new images
We estimate P est (w s d) using maximum likelihood estimation CITATION: P est (w\s d ) = -?(7) num sd where num wsi denotes the frequency of w in the accompanying document of latent variable s and num sd the number of all tokens in the document
We reduce the search space , by scoring each document word with its tf * idf weight CITATION and adding the n-best candidates to our caption vocabulary
The first baseline is based on tf * idf CITATION
The documents and captions were part-of-speech tagged and lemmatized with Tree Tagger CITATION.Words other than nouns , verbs , and adjectives were discarded
The earliest approaches are closely related to image classification CITATION , where pictures are assigned a set of simple descriptions such as indoor , outdoor , landscape , people , animal
reliably CITATION
LDA represents documents as a mixture of topics and has been previously used to perform document classification CITATION and ad-hoc information retrieval CITATION with good results
Maximum Entropy Models CITATION seek to maximise the conditional probability of classes , given certain observations (features)
This phenomenon , together with others used to express forms of authorial opinion , is often classified under the notion of subjectivity CITATION , CITATION
In contrast to the findings of Wiebe et al. (CITATION) , who addressed the broader task of subjectivity learning and found that the density of other potentially subjective cues in the context benefits classification accuracy , we observed that the co-occurence of speculative cues in a sentence does not help in classifying a term as speculative or not
Since the benefits from combining classifiers that always make similar decisions is minimal , the two (or more) base-learners should complement each other CITATION
Recent experiments assessing system portability across different domains , conducted by Aue and Gamon CITATION , demonstrated that sentiment annotation classifiers trained in one domain do not perform well on other domains
Such approaches work well in situations where large labeled corpora are available for training and validation (e.g. , movie reviews) , but they do not perform well when training data is scarce or when it comes from a different domain CITATION , topic CITATION or time period CITATION
For instance , Aue and Gamon CITATION proposed training on a samll number of labeled examples and large quantities of unlabelled in-domain data
Research on sentiment annotation is usually conducted at the text CITATION or at the sentence levels CITATION
3.3 Establishing a Baseline for a Corpus-based System (CBS) Supervised statistical methods have been very successful in sentiment tagging of texts: on movie review texts they reach accuracies of 85-90% CITATION
A number of methods has been proposed in order to overcome this system portability limitation by using out-of-domain data , unlabelled in-domain corpora or a combination of in-domain and out-of-domain examples CITATION
Consistent with findings in the literature CITATION , on the large corpus of movie review texts , the in-domain-trained system based solely on unigrams had lower accuracy than the similar system trained on bigrams
Drezde et al. CITATION applied structural correspondence learning CITATION to the task of domain adaptation for sentiment classification of product reviews
It also strongly depends on the similarity between the domains as has been shown by CITATION
On other domains , such as product reviews , the performance of systems that use general word lists is comparable to the performance of supervised machine learning approaches CITATION
To our knowledge , the only work that describes the application of statistical classifiers (SVM) to sentence-level sentiment classification is CITATION 1
In sentiment tagging and related areas , Aue and Gamon CITATION demonstrated that combining classifiers can be a valuable tool in domain adaptation for sentiment analysis
Since the structure of WordNet glosses is fairly different from that of other types of corpora , we developed a system that used the list of human-annotated adjectives from CITATION as a seed list and then learned additional unigrams from WordNet synsets and glosses with up to 88% accuracy , when evaluated against General Inquirer CITATION (GI) on the intersection of our automatically acquired list with GI
In order to assign the membership score to each word , we did 58 system runs on unique non-intersecting seed lists drawn from manually annotated list of positive and negative adjectives from CITATION
?A set of 1200 product review (PR) sentences extracted from the annotated corpus made available by Bing Liu CITATION (<a href="http://www.cs.uic.edu/">http://www.cs.uic.edu/ liub/FBS/FBS.html)
For this we used four different data sets of sentences annotated with sentiment tags: ?A set of movie review snippets (further: movie) from CITATION
But such general word lists were shown to perform worse than statistical models built on sufficiently large in-domain training sets of movie reviews CITATION
The results reported by CITATION for binary classification of sentences in a related domain of subjectivity tagging (i.e. , the separation of sentiment-laden from neutral sentences) suggest that statistical classifiers can perform well on this task: the authors have reached 74.9% accuracy on the MPQA corpus CITATION
Similarly , Tan et al. CITATION suggested to combine out-of-domain labeled examples with unla-belled ones from the target domain in order to solve the domain-transfer problem
In order to maximize the utility of the examples from the target domain , these examples were selected using Similarity Ranking and Relative Similarity Ranking algorithms CITATION
For example , it has been observed that texts often contain multiple opinions on different topics CITATION , which makes assignment of the overall sentiment to the whole document problematic
The NOSs were then normalized into the interval from -1 to +1 using a sigmoid fuzzy membership function CITATION 4
Despite its limited scale , prior work in sentence compression relied heavily on this particular corpus for establishing results CITATION
In the context of sentence compression , a linear programming based approach such as Clarke and Lapata CITATION is certainly one that deserves consideration
Thus , unlike McDonald CITATION , Clarke and Lap-ata CITATION and Cohn and Lapata CITATION , we do not insist on finding a globally optimal solution in the space of 2 n possible compressions for an n word long sentence
1 But how do we find compressions that are grammatical? To address the issue , rather than resort to statistical generation models as in the previous literature CITATION , we pursue a particular rule-based approach we call a 'dependency truncation ,' which as we will see , gives us a greater control over the form that compression takes
Our approach is broadly in line with prior work CITATION , in that we make use of some form of syntactic knowledge to constrain compressions we generate
DPM was first introduced in CITATION , later explored by a number of people CITATION
For better or worse , much of prior work on sentence compression CITATION turned to a single corpus developed by Knight and Marcu CITATION (K&M , henceforth) for evaluating their approaches
What sets this work apart from them , however , is a novel use we make of Conditional Random Fields (CRFs) to select among possible compressions CITATION
In the experiment described later , we set a = 0.1 for DPM , following Morooka et al. CITATION , who found the best performance with that setting for a
Nonetheless , there is some cost that comes with the straightforward use of CRFs as a discriminative classifier in sentence compression; its outputs are often ungrammatical and it allows no control over the length of compression they generates CITATION
If it is , the whole scheme of ours would fall under what is known as 'Linear Programming CRFs' CITATION
We extracted lead sentences both from the brief and from its source article , and aligned them , using what is known as the Smith-Waterman algorithm CITATION , which produced 1 ,401 pairs of summary and source sentence
A part of our system makes use of a modeling toolkit called GRMM CITATION
In any case , we need some extra rules on G(S) to take care of language specific issues (cf.Vandeghinste and Pan CITATION for English)
Recently , Blei and McAuliffe CITATION proposed an approach for joint sentiment and topic modeling that can be viewed as a supervised LDA (sLDA) model that tries to infer topics appropriate for use in a given classification or regression problem
The Multi-Grain Latent Dirichlet Allocation model (MG-LDA) is an extension of Latent Dirichlet Allocation (LDA) CITATION
Parallel to this study Branavan et al. CITATION also showed that joint models of text and user annotations benefit extractive summarization
In this study , we look at the problem of aspect-based sentiment summarization CITATION
Text excerpts are usually extracted through string matching CITATION , sentence clustering CITATION , or through topic models CITATION
Gibbs sampling is an example of a Markov Chain Monte Carlo algorithm CITATION
Following Titov and McDonald CITATION we use a collapsed Gibbs sampling algorithm that was derived for the MG-LDA model based on the Gibbs sampling method proposed for LDA in CITATION
However , CITATION demonstrated that an efficient collapsed Gibbs sampler can be constructed , where only assignments z need to be sampled , whereas the dependency on distributions 0 and p can be integrated out analytically
These simple techniques are capable of modeling local topics without more expensive modeling of topic transitions used in CITATION
2 Aspect identification has also been thoroughly studied CITATION , but again , ontologies and users often provide this information negating the need for automation
When labeled data exists , this problem can be solved effectively using a wide variety of methods available for text classification and information extraction CITATION
A closely related model to ours is that of Mei et al. CITATION which performs joint topic and sentiment modeling of collections
For details on computing gradients for loglinear graphical models with Gibbs sampling we refer the reader to CITATION
Sentiment classification is a well studied problem CITATION and in many domains users explicitly provide ratings for each aspect making automated means unnecessary
However , it has been observed that ratings for different aspects can be correlated CITATION , e.g. , very negative opinion about room cleanliness is likely to result not only in a low rating for the aspect rooms , but also is very predictive of low ratings for the aspects service and dining
The first part is based on Multi-Grain Latent Dirichlet Allocation CITATION , which has been previously shown to build topics that are representative of ratable aspects
As was demon-strated in Titov and McDonald CITATION , the topics produced by LDA do not correspond to ratable aspects of entities
It was demonstrated in Titov and McDonald CITATION that ratable aspects will be captured by local topics and global topics will capture properties of reviewed items
This factor is proportional to the conditional distribution used in the Gibbs sampler of the MG-LDA model CITATION
Other local topics , as for the MG-LDA model , correspond to other aspects discussed in reviews (breakfast , prices , noise) , and as it was previously shown in Titov and McDonald CITATION , aspects for global topics correspond to the types of reviewed items (hotels in Russia , Paris hotels) or background words
For a broader review of WSD in NLP applications , see Resnik CITATION
This problem of identifying the correct sense of a word in context is known as word sense disambiguation (WSD: Agirre and Edmonds CITATION)
Following Atterer and Schutze CITATION , we wrote a script that , given a parse tree , identifies instances of PP attachment ambiguity and outputs the (v ,n1 ,p ,n2) quadruple involved and the attachment decision
This evaluation methodology coincides with that of Atterer and Schutze CITATION
Note that Atterer and Schutze CITATION have shown that the Bikel parser performs as well as the state-of-the-art in PP attachment , which suggests our method improves over the current state-of-the-art
We provide the first definitive results that word sense information can enhance Penn Treebank parser performance , building on earlier results of Bikel CITATION and Xiong et al. CITATION
The most closely related research is that of Bikel CITATION , who merged the Brown portion of the Penn Tree-bank with SemCor (similarly to our approach in Section 4.1) , and used this as the basis for evaluation of a generative bilexical model for joint WSD and parsing
Note that this dataset is smaller than the one described by Bikel CITATION in a similar exercise , the reason being our simple and conservative approach taken when merging the resources
Parsing As our baseline parsers , we use two state-of-the-art lexicalised parsing models , namely the Bikel parser CITATION and Charniak parser CITATION
Tighter integration of semantics into the parsing models , possibly in the form of discriminative reranking models CITATION , is a promising way forward in this regard
For example , a number of different parsers have been shown to benefit from lexicalisation , that is , the conditioning of structural features on the lexical head of the given constituent CITATION
This extraction system uses Collins' rules (based on treep CITATION) to locate the heads of phrases
Other notable examples of the successful incorporation of lexical semantics into parsing , not through word sense information but indirectly via selectional preferences , are Dowding et al. CITATION and Hektoen CITATION
The method we use to predict the first sense is that of McCarthy et al. CITATION , which was obtained using a thesaurus automatically created from the British National Corpus (BNC) applying the method of Lin CITATION , coupled with WordNet-based similarity measures
The only successful applications of word sense information to parsing that we are aware of are Xiong et al. CITATION and Fujita et al. CITATION
Note also that our baseline results for the Table 6: Parsing results with ASR (* indicates that the recall or precision is significantly better than baseline; the best performing method in each column is shown in bold) Table 7: PP attachment results with ASR (* indicates that the recall or precision is significantly better than baseline; the best performance in each column is shown in bold) dataset are almost the same as previous work parsing the Brown corpus with similar models CITATION , which suggests that our dataset is representative of this corpus
The only publicly-available resource with these two characteristics at the time of this work was the subset of the Brown Corpus that is included in both SemCor CITATION and the Penn Tree-bank (PTB)
Li and Abe CITATION , McCarthy and Carroll CITATION , Xiong et al. CITATION , Fu-jita et al. CITATION)
Traditionally , the two parsers have been trained and evaluated over the WSJ portion of the Penn Treebank (PTB: Marcus et al. CITATION)
Prepositional phrase attachment (PP attachment) is the problem of determining the correct attachment site for a PP , conventionally in the form of the noun or verb in a V NP PP structure CITATION
Disambiguating each word relative to its context of use becomes increasingly difficult for fine-grained representations CITATION
The best published results over RRR are those of Stetina and Nagao CITATION , who employ WordNet sense predictions from an unsuper-vised WSD method within a decision tree classifier
The fact that the improvement is larger for PP attachment than for full parsing is suggestive of PP attachment being a parsing subtask where lexical semantic information is particularly important , supporting the findings of Stetina and Nagao CITATION over a standalone PP attachment task
However , it has been argued that Spanish causative verbs do not in fact take objects CITATION
> The schema in (1) is also found in the widely-studied Romance causative construction CITATION , illustrated in (22): (22)?Nos hizo   leer El Senor de los Anillos
The logic also ensures that the new rules are subject to modalities consistent with those defined by Baldridge and Kruijff CITATION
This treats CCG as a compilation of CTL proofs , providing a principled , grammar-internal basis for restrictions on the CCG rules , transferring language-particular restrictions on rule application to the lexicon , and allowing the CCG rules to be viewed as grammatical universals CITATION
The rules of this multimodal version of CCG CITATION are derived as theorems of a Categorial Type Logic (CTL , Moortgat CITATION)
The most commonly used are Karttunnen's chart subsumption check CITATION and Eisner's normal-form constraints CITATION
Furthermore , CCG augmented with  D is compatible with Eisner NF CITATION , a standard technique for controlling derivational ambiguity in CCG-parsers , and also with the modalized version of CCG CITATION
It has been used for a variety of tasks , such as wide-coverage parsing CITATION , sentence realization CITATION , learning semantic parsers CITATION , dialog systems CITATION , grammar engineering CITATION , and modeling syntactic priming CITATION
We show two ways to derive the D rules: one based on unary composition and the other based on a logical characterization of CCG's rule base CITATION
The  D rules are well-behaved; we show this by deriving them both from unary composition and from the logic defined by Baldridge CITATION
(11) s vp/np base your verdict on s/(vp/np) s/(s/np) s/vp what you can (s/(vp/np))\(s/(vp/np)) (x\x)/x s/(vp/np) and s/(s/np) s/vp what you must not The category for and is marked for non-associativity with * , and thus combines with other expressions only by function application CITATION
The implication is that outputs of  B 1+ rules are inert , using the terminology of Baldridge CITATION
In this section , we present an alternate formulation of Eisner NF with Baldridge's CITATION CTL basis for CCG
In Baldridge's CITATION system , only proofs involving the ARP and ALP rules produce inert categories
These are instead determined by syntactic , semantic , and pragmatic factors , such as transitivity , word order , animacy , gender , social prestige , and referential specificity CITATION
The standard CCG analysis for English auxiliary verbs is the type exemplified in (16) CITATION , interpreted as a unary operator over sentence meanings CITATION: (16)?can h (s\np)/(s\np) : AP etAx .<0>P(x) However , this type is empirically underdetermined , given a widely-noted set of generalizations suggesting that auxiliaries and raising verbs take no subject argument at all CITATION
Combining (20) with a type-raised subject presents another instance of the structure in (1) , where that question words are represented as variable-binding operators CITATION:
Several techniques have been proposed for the problem CITATION
Following Jacobson CITATION , a more empirically-motivated assignment is (20): s (20)?can h s/s : Xp t
Applying  D to an argument sequence is equivalent to compound application of binary  B: (37)?((Df )g)h)x = (/g)(hx) (38)?(((BB)f )g)h)x = ((B(/g))h)x = (fg)(hx) Syntactically , binary  B is equivalent to application of unary  B to the primary functor A , followed by applying the secondary functor r to the output of  B by means of function application CITATION: (39) x/y y/z (x/z)/(y/z) x/z (40) x/y (x/w)/z The rules for  D correspond to application of  B to both the primary and secondary functors , followed by function application: (41) x/(y/z) y/w ( x/(w/z))/((y/z)/(w/z)) (y/z)/(w/z) x/ ( w/z ) As with  B n ,  D n- 1 can be derived by iterative application of  B to both primary and secondary functors
For applications that call for increased incremental-ity (e.g. , aligning visual and spoken input incrementally CITATION) , CCG rules that do not produce inert categories can be derived a CTL basis that does not require ? ant for associativity and permutation
It was noted by Pickering and Barry CITATION for English , but to the best of our knowledge it has not been treated in the s CCG literature , nor noted in other languages
Combinatory Categorial Grammar (CCG , Steedman CITATION) is a compositional , semantically transparent formalism that is both linguistically expressive and computationally tractable
This supports elegant analyses of several phenomena (e.g. , coordination , long-distance extraction , and intonation) and allows incremental parsing with the competence grammar CITATION
Inert slashes are Baldridge's CITATION encoding in OpenCCG 3 of his CTL interpretation of Steedman's CITATION antecedent-government feature
Following Wittenburg CITATION , we remedy this by adding a set of rules based on the D combinator of combinatory logic CITATION
CCG's flexibility is useful for linguistic analyses , but leads to spurious ambiguity CITATION due to the associativity introduced by the  B and  T rules
Wittenburg CITATION originally proposed using rules based on  D as a way to reduce spurious ambiguity , which he achieved by eliminating  B rules entirely and replacing them with variations on  D
The parser uses a two-stage system , first employing a supertagger CITATION to propose lexical categories for each word , and then applying the cky chart parsing algorithm
To remove this variable , we carry out a second evaluation against the Briscoe and Carroll CITATION reannotation ofDep-Bank CITATION , as described in Clark and Curran CITATION
This evaluation is particularly relevant for nps , as the Briscoe and Carroll CITATION corpus has been annotated for internal np structure
Lewin CITATION experiments with detecting base -nps using ner information , while Buyko et al. CITATION use a crf to identify a guest comedian Victor Borge NP[nb}/N N/N  N/N  N/N ~N~ NP (a) N N N guest comedian Victor Borge NP[nb  \I N N  I N NP N N   (NP\NP  )I (NP\NP) NP\NP -> -> NP\NP NP (b) Figure 4: CCGbank derivations for apposition with dt coordinate structure in biological named entities
N N/N N lung N/N N ??? ???  \?  \ I N    ???    ??? deaths deaths  lung cancer N N/N N/N N deaths (N/N )/(N/N ) cancer  deaths  lung  cancer?ung cancer Figure 2: (a) Original right-branching CCGbank (b) Left-branching (c) Left-branching with new supertags 2.2  CCG parsing The C&C ccg parser CITATION is used to perform our experiments , and to evaluate the effect of the changes to CCGbank
Clark and Curran CITATION has a full description of the C&C parser's pre-existing features , to which we have added a number of novel ner-based features
Our experiments are run with the C&C ccg parser CITATION , and will evaluate the changes made to CCGbank , as well as the effectiveness of the ner features
Vadas and Curran CITATION carry out supervised experiments using this data set of 36 ,584 NPs , outperforming the Collins CITATION parser
This is unexpected , because possessives were already bracketed properly when CCGbank was originally created CITATION
We generate the two forms of output that CCGbank contains: AUTO files , which represent the tree structure of each sentence; and PARG files , which list the word-word dependencies CITATION
The flat structure described by the Penn Treebank can be seen in this example: (NP (NN lung)  (NN cancer)  (NNS deaths)) CCGbank CITATION is the primary English corpus for Combinatory Cate-gorial Grammar (ccg) CITATION and was created by a semi-automatic conversion from the penn Treebank
N N/N N  i? \ cotton   conj N  i? \ and  N/N N  ? I N N/N N/N   N/N [conj ] N fibers cotton conj   N / N acetate  fibers?nd acetate Figure 1: (a) Incorrect ccg derivation from Hockenmaier and Steedman CITATION (b) The correct derivation Parsing of nps is typically framed as np bracketing , where the task is limited to discriminating between left and right-branching NPs of three nouns only: ?(crude oil) prices - left-branching ?world (oil prices) - right-branching Lauer CITATION presents two models to solve this problem: the adjacency model , which compares the association strength between words 1-2 to words 2-3; and the dependency model , which compares words 1-2 to words 1-3
Heads are then assigned using heuristics adapted from Hockenmaier and Steedman CITATION
Honnibal and Curran CITATION have also made changes to CCGbank , aimed at better differentiating between complements and adjuncts
Finally , we evaluate against DepBank CITATION
This is because their training data , the Penn Treebank CITATION , does not fully annotate np structure
Nakov and Hearst CITATION use search engine hit counts and extend the query set with typographical markers
PropBank CITATION is used as a gold-standard to inform these decisions , similar to the way that we use the Vadas and Curran CITATION data
Combinatory Categorial Grammar (ccg) CITATION is a type-driven , lexicalised theory of grammar
We apply an automatic conversion process using the gold-standard np data annotated by Vadas and Curran CITATION
Recently , Vadas and Curran CITATION annotated internal NP structure for the entire Penn Treebank , providing a large gold-standard corpus for np bracketing
The Vadas and Curran CITATION annotation scheme inserts NML and JJP brackets to describe the correct np structure , as shown below: We use these brackets to determine new goldstandard CCG derivations in Section 3
This section describes the process of converting the Vadas and Curran CITATION data to ccg derivations
For example , we would insert the NML bracket shown below: This simple heuristic captures np structure not explicitly annotated by Vadas and Curran CITATION
Vadas and Curran CITATION describe using ne tags during the annotation process , suggesting that ner-based features will be helpful in a statistical model
Vadas and Curran CITATION experienced a similar drop in performance on Penn Tree-bank data , and noted that the F-score for nml and jjp brackets was about 20% lower than the overall figure
The first contribution of this paper is the application of the Vadas and Curran CITATION data to Combinatory Categorial Grammar
In particular , we implement new features using ner tags from the BBN Entity Type Corpus CITATION
We draw ne tags from the BBN Entity Type Corpus CITATION , which describes 28 different entity types
Another group of related work focuses on summarizing sentences through a series of deletions CITATION
A relatively straight-forward extension of the inside-outside algorithm for chart-parses allows us to learn and perform inference in our compact representation (a similar algorithm is presented in CITATION)
Features derived from a syntactic parse of the sentence have proven particularly useful CITATION
Another area of related work in the semantic role labeling literature is that on tree kernels CITATION
We compared to a strong Baseline SRL system that learns a logistic regression model using the features of Pradhan et al. CITATION
Table 2 shows results of these three systems on the Conll-2005 task , plus the top-performing system CITATION for reference
Approaches include incorporating a subcategorization feature CITATION , such as the one used in our baseline; and building a model which jointly classifies all arguments of a verb CITATION
Graph-ranking algorithms , e.g. , Page-Rank CITATION , are then applied to rank those sentences
This problem and its influence on email summarization were studied in CITATION and CITATION
As a second contribution of this paper , we study several ways to measure the cohesion between parent and child sentences in the quotation graph: clue words (re-occurring words in the reply) CITATION , semantic similarity and cosine similarity
In our recent study CITATION , we built a fragment quotation graph to represent an email conversation and developed a ClueWordSummarizer (CWS) based on the concept of clue words
One is the generalization of the CWS algorithm in CITATION and one is the well-known PageRank algorithm
Particularly , when the weight of the edge is based on clue words as in Equation 1 , this method is equivalent to Algorithm CWS in CITATION
Those discussions can be viewed as conversations via emails and are valuable for the user as a personal information repositoryCITATION
Other than for email summarization , other document summarization methods have adopted graphranking algorithms for summarization , e.g. , CITATION , CITATION and CITATION
In many applications , it has been shown that sentences with subjective meanings are paid more attention than factual onesCITATIONCITATION
Department of Computer Science University of British Columbia Vancouver , BC , Canada {carenini , rng , xdzhou}@cs.ubc.ca With the ever increasing popularity of emails , it is very common nowadays that people discuss specific issues , events or tasks among a group of people by emailsCITATION
?OpBear: The list of opinion bearing words in CITATION
Since this idea is borrowed from the pyramid metric by Nenkova et al. CITATION , we call it the sentence pyramid precision
Specifically , we use the package by CITATION , which includes several methods to compute the semantic similarity
We use the MEAD package to segment the text into 1 ,394 sentences CITATION
Meanwhile , most existing email summarization approaches use quantitative features to describe the conversation structure , e.g. , number of recipients and responses , and apply some general multi-document summarization methods to extract some sentences as the summary CITATION CITATION
Our experiments showed that CWS had a higher accuracy than the email summarization approach in CITATION and the generic multi-document summarization approach MEAD CITATION
The major source of this list is from CITATION with additional words from other sources
A large amount of work has been done on determining the level of subjectivity of text CITATION
Similar to the issue-response relationship , Shrestha et al. CITATION proposed methods to identify the question-answer pairs from an email thread
?OpFind: The list of subjective words in CITATION
Most of the existing methods dealing with email conversations use the email thread to represent the email conversation structure , which is not accurate in many cases CITATION
For example , in (2) , the daughters list RB TO JJ NNS is a daughters list with no correlates in the treebank; it is erroneous because close to wholesale needs another layer of structure , namely adjective phrase (ADJP)  CITATION
The description for tagging titles in the guidelines (Bies et al. , 1995 , p
To understand this , we have to realize that most modifiers are adjoined at the sentence level when there is any doubt about their attachment  CITATION
QP is "used for multiword numerical expressions that occur within NP (and sometimes ADJP) , where the QP corresponds frequently to some kind of complex determiner phrase"  CITATION
When extracting rules from constituency-based tree-banks employing flat structures , grammars often limit the set of rules CITATION , due to the large number of rules CITATION and "leaky" rules that can lead to mis-analysis CITATION
Parse reranking techniques , for instance , rely on knowledge about features other than those found in the core parsing model in order to determine the best parse CITATION
Instead of examining and comparing rules in their entirety , this method abstracts a rule to its component parts , similar to features using information about n-grams of daughter nodes in parse reranking models CITATION
Although frequency-based criteria are often used , these are not without problems because low-frequency rules can be valid and potentially useful rules CITATION , and high-frequency rules can be erroneous CITATION
This method can be extended to increase recall , by treating similar daughters lists as equivalent CITATION
Using this strict equivalence to identify ad hoc rules is quite successful CITATION , but it misses a significant number of generalizations
Not only are errors inherently undesirable for obtaining an accurate grammar , but training on data with erroneous rules can be detrimental to parsing performance CITATION
To define dissimilarity , we need a notion of similarity , and , a starting point for this is the error detection method outlined in Dickinson and Meurers CITATION
This captures the property that identical daughters lists with different mothers are distinct CITATION
Although statistical techniques have been employed to detect anomalous annotation CITATION , these methods do not account for linguistically-motivated generalizations across rules , and no full evaluation has been done on a treebank
Infrequent rules in one genre may be quite frequent in another CITATION and their frequency may be unrelated to their usefulness for parsing CITATION
This issue is of even more importance when considering the task of porting a parser trained on one genre to another genre CITATION
Active learning techniques also require a scoring function for parser confidence CITATION , and often use uncertainty scores of parse trees in order to select representative samples for learning CITATION
Since most natural language expressions are endocentric , i.e. , a category projects to a phrase of the same category CITATION , daughters lists with more than one possible mother are flagged as potentially containing an error
This is in the spirit of Kveton and Oliva CITATION , who define invalid bigrams for POS annotation sequences in order to detect annotation errors.
For example , IN NP 1 has nine different mothers in the Wall Street Journal (WSJ) portion of the Penn Treebank CITATION , six of which are errors
If a treebank grammar is used CITATION , then one needs to isolate rules for ungram-matical data , to be able to distinguish grammatical from ungrammatical input
Thus , identifying ad hoc rules can also provide feedback on annotation schemes , an especially important step if one is to use the treebank for specific applications CITATION , or if one is in the process of developing a treebank
This is true of precision grammars , where analyses can be more or less preferred CITATION , and in applications like intelligent computer-aided language learning , where learner input is parsed to detect what is correct or not (see , e.g. , Vandeventer Faltin , 2003 , ch
The input for the segmentation task is however highly ambiguous for Semitic languages , and surface forms (tokens) may admit multiple possible analyses as in CITATION
Morphological dis-ambiguators that consider a token in context (an utterance) and propose the most likely morphological analysis of an utterance (including segmentation) were presented by Bar-Haim  et al. CITATION , Adler and Elhadad CITATION , Shacham and Wintner CITATION , and achieved good results (the best segmentation result so far is around 98%)
A possible probabilistic model for assigning probabilities to complex analyses of a surface form may be P (REL ,  VB fmnh , context) = P (RELf)P (VB|mnh , REL)P (REL , VB| context) and indeed recent sequential disambiguation models for Hebrew CITATION and Arabic CITATION present similar models
In sequential tagging models such as CITATION weights are assigned according to a language model based on linear context
Using a wide-coverage morphological analyzer based on CITATION should cater for a better coverage , and incorporating lexical probabilities learned from a big (unannotated) corpus (cf. CITATION) will make the parser more robust and suitable for use in more realistic scenarios
This is by now a fairly standard representation for multiple morphological segmentation of Hebrew utterances CITATION
Tsarfaty CITATION used a morphological analyzer CITATION , a PoS tagger CITATION , and a general purpose parser CITATION in an integrated framework in which morphological and syntactic components interact to share information , leading to improved performance on the joint task
To evaluate the performance on the segmentation task , we report SEG , the standard harmonic means for segmentation Precision and Recall F 1 (as defined in Bar-Haim  et al. CITATION; Tsarfaty CITATION) as well as the segmentation accuracy SEG Tok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith CITATION)
REL+VB) (cf. CITATION) and probabilities are assigned to different analyses in accordance with the likelihood of their tags (e.g. , "fmnh is 30% likely to be tagged NN and 70% likely to be tagged REL+VB")
This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g. CITATION
One way to approach this discrepancy is to assume a preceding phase of morphological segmentation for extracting the different lexical items that exist at the token level (as is done , to the best of our knowledge , in all parsing related work on Arabic and its dialects CITATION)
Cohen and Smith CITATION followed up on these results and pro-posed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own
Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar , a data-driven lexicon , and a linguistically motivated unknown-tokens handling our model outperforms CITATION and CITATION on the joint task and achieves state-of-the-art results on a par with current respective standalone models
Cohen and Smith CITATION later on based a system for joint inference on factored , independent , morphological and syntactic components of which scores are combined to cater for the joint inference task
To facilitate the comparison of our results to those reported by CITATION we use their data set in which 177 empty and "malformed" 7 were removed
We used BitPar CITATION , an efficient general purpose parser , 10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis
Finally , model GT v = 2 includes parent annotation on top of the various state-splits , as is done also in CITATION
Table 2 compares the performance of our system on the setup of Cohen and Smith CITATION to the best results reported by them for the same tasks
In Modern Hebrew (Hebrew) , a Semitic language with very rich morphology , particles marking conjunctions , prepositions , complementizers and rela-tivizers are bound elements prefixed to the word CITATION
In our third model GT ppp we also add the distinction between general PPs and possessive PPs following Goldberg and Elhadad CITATION
The current work treats both segmental and super-segmental phenomena , yet we note that there may be more adequate ways to treat super-segmental phenomena assuming Word-Based morphology as we explore in CITATION
We use the HSPELL 9 CITATION wordlist as a lexeme-based lexicon for pruning segmentations involving invalid segments
Morphological analyzers for Hebrew that analyze a surface form in isolation have been proposed by Segal CITATION , Yona and Wintner CITATION , and recently by the knowledge center for processing Hebrew CITATION
Such resources exist for Hebrew CITATION , but unfortunately use a tagging scheme which is incom-patible with the one of the Hebrew Treebank
The development of the very first Hebrew Tree-bank CITATION called for the exploration of general statistical parsing methods , but the application was at first limited
We use the Hebrew Treebank , CITATION , provided by the knowledge center for processing Hebrew , in which sentences from the daily newspaper "Ha'aretz" are morphologically segmented and syntactically annotated
The joint morphological and syntactic hypothesis was first discussed in CITATION and empirically explored in CITATION
Tsarfaty CITATION was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank
Tsarfaty and Sima'an CITATION have reported state-of-the-art results on Hebrew unlexicalized parsing (74.41%) albeit assuming oracle morphological segmentation
In our forth model GT nph we add the definiteness status of constituents following Tsarfaty and Sima'an CITATION
We conjecture that this trend may continue by incorporating additional information , e.g. , three-dimensional models as proposed by Tsarfaty and Sima'an CITATION
Tsarfaty CITATION argues that for Semitic languages determining the correct morphological segmentation is dependent on syntactic context and shows that increasing information sharing between the morphological and the syntactic components leads to improved performance on the joint task
In our second model GT vpi we also distinguished finite and non-finite verbs and VPs as proposed in CITATION
Both CITATION have shown that a single integrated framework outperforms a completely streamlined implementation , yet neither has shown a single generative model which handles both tasks
Finally , Adda-Decker and Lamel CITATION demonstrated that both French and English ASR systems had more trouble with male speakers than female speakers , and found several possible explanations , including higher rates of disfluencies and more reduction
Also , like Adda-Decker and Lamel CITATION , we find that male speakers have higher error rates than females , though in our data set the difference is more striking (3.6% absolute , compared to their 2.0%)
This last result sheds some light on the work of Adda-Decker and Lamel CITATION , who suggested several factors that could explain males' higher error rates
We fit our models using the lme4 package CITATION of R CITATION
One possibility is that female speech is more easily recognized because females tend to have expanded vowel spaces CITATION , a factor that is associated with greater intelligibility CITATION and is characteristic of genres with lower ASR error rates CITATION
Previous work on recognition of spontaneous monologues and dialogues has shown that infrequent words are more likely to be misrecognized CITATION and that fast speech increases error rates CITATION
In the word-level analyses of Fosler-Lussier and Morgan CITATION and Shinozaki and Fu-rui CITATION , only substitution and deletion errors were considered , so we do not know how including insertions might affect the results
In Hirschberg et al.'s CITATION analysis of two human-computer dialogue systems , misrecog-nized turns were found to have (on average) higher maximum pitch and energy than correctly recognized turns
Hirschberg et al.'s CITATION work suggests that prosodic factors can impact error rates , but leaves open the question of which factors are important at the word level and how they influence recognition of natural conversational speech
Classes were identified using a POS tagger CITATION trained on the tagged Switchboard corpus
We also see a tendency towards higher IWER for very slow speech , consistent with Shinozaki and Furui CITATION and Siegler and Stern CITATION
For our analysis , we used the output from the SRI/ICSI/UW RT-04 CTS system CITATION on the NIST RT-03 development set
Rachmaninoff , Rafael and Brokoviev Ref3 composers including Bach , Mozart , Schopen , Beethoven , missing name Raphael , Rahmaniev and Brokofien Ref4 composers such as Bach , Mozart , missing name Beethoven , Schumann , Rachmaninov , Raphael and Prokofiev The task of transliterating names (independent of end-to-end MT) has received a significant amount of research , e.g. , CITATION
Unlike various generative approaches CITATION , we do not synthesize an English spelling from scratch , but rather find a translation in very large lists of English words (3.4 million) and phrases (47 million)
We follow previous work in using the Brent corpus consists of 9790 transcribed utterances (33 ,399 words) of child-directed speech from the Bernstein-Ratner corpus CITATION in the CHILDES database CITATION
Second , we can generalize over arbitrary subtrees rather than local trees in much the way done in DOP or tree substitution grammar CITATION , which leads to adaptor grammars
PCFG estimation procedures have been used to model the supervised and unsupervised acquisition of syllable structure CITATION; and the best performance in unsupervised acquisition is obtained using a grammar that encodes linguistically detailed properties of syllables whose rules are inferred using a fairly complex algorithm CITATION
Following Goldwater and Johnson CITATION , the grammar differentiates between OnsetI , which expands to word-initial onsets , and Onset , Sentence Word Word Onsetl Nucleus CodaF Onsetl Nucleus CodaF W?    t  s    D? s Figure 6: A parse of "what's this" produced by the unigram syllable adaptor grammar of Figure 7
For example , the adaptor grammars for syllable structure presented in sections 3.3 and 3.6 learn more information about syllable onsets and codas than the PCFGs presented in Goldwater and Johnson CITATION
We evaluated the f-score of the recovered word constituents CITATION
Johnson et al. CITATION presented an adaptor grammar that defines a unigram model of word segmentation and showed that it performs as well as the unigram DP word segmentation model presented by CITATION
As reported in Goldwater et al. CITATION and Goldwater et al. CITATION , a unigram word segmentation model tends to undersegment and misanalyse collocations as individual words
based unsupervised morphological analysis model presented by Goldwater et al. CITATION
Goldwater et al. CITATION showed that modeling dependencies between adjacent words dramatically improves word segmentation accuracy
It's straight forward to design an adaptor grammar that can capture a finite number of concatenative paradigm classes CITATION
It may be possible to adapt efficient split-merge samplers CITATION?nd Variational Bayes methods CITATION?or DPs to adaptor grammars and other linguistic applications of HDPs
(In fact , the inference procedure for adaptor grammars described in Johnson et al. CITATION relies on a PCFG approximation that contains a rule for each subtree generalization in the adaptor grammar)
Adaptor grammars CITATION are a non-parametric Bayesian extension of Probabilistic Context-Free Grammars (PCFGs) which in effect learn the probabilities of entire subtrees
This section introduces adaptor grammars as an extension of PCFGs; for a more detailed exposition see Johnson et al. CITATION
Johnson et al. CITATION describe an MCMC procedure for inferring the adapted tree distributions Ga , and Johnson et al. CITATION describe a Bayesian inference procedure for the PCFG rule parameters 6 using a Metropolis-Hastings MCMC procedure; implementations are available from the author's web site
Johnson et al. CITATION presented an adaptor grammar for segmenting verbs into stems and suffixes that implements the DP-Sentence
The MCMC sampler of Johnson et al. CITATION used here is satifactory for small and medium-sized problems , but it would be very useful to have more efficient inference procedures
For example , the Bayesian unsupervised PCFG estimation procedure devised by Stolcke CITATION uses a model-merging procedure to propose new sets of PCFG rules and a Bayesian version of the EM procedure to estimate their weights
There are several different ways to define DPs; one of the most useful is the characterization of the conditional or sampling distribution of a draw from DP(a , H) in terms of the Polya urn or Chinese Restaurant Process CITATION
Technically this grammar implements a Hierarchical Dirichlet Process (HDP) CITATION because the base distribution for the Word DP is itself constructed from the Stem and Suffix distributions , which are themselves generated by DPs
Asahara and Motsumoto CITATION proposed using characters instead of morphemes as the unit to alleviate the effect of segmentation errors in morphological analysis and we also used their character-based method
Though there may be slight differences , these features are based on the standard ones proposed and used in previous studies on Japanese NER such as those by Asahara and Motsumoto CITATION , Nakano and Hirai CITATION , and Yamada CITATION
In addition , the clustering methods used , such as HMMs and Brown's algorithm CITATION , seem unable to adequately capture the semantics of MNs since they are based only on the information of adjacent words
They constructed word clusters by using HMMs or Brown's clustering algorithm CITATION , which utilize only information from neighboring words
Chu et al. CITATION presented the MapReduce framework for a wide range of machine learning algorithms , including the EM algorithm
Since building and maintaining high-quality gazetteers by hand is very expensive , many methods have been proposed for automatic extraction of gazetteers from texts CITATION
For example , we can use automatically extracted hyponymy relations CITATION , or automatically induced MN clusters CITATION
Defining sentences in a dictionary or an encyclopedia have long been used as a source of hyponymy relations CITATION
19 Recently , Inui et al. CITATION investi-gated the relation between the size and the quality of a gazetteer and its effect
For instance , Kazama and Torisawa CITATION used the hyponymy relations extracted from Wikipedia for the English NER , and reported improved accuracies with such a gazetteer
We also compared the cluster gazetteers with the Wikipedia gazetteer constructed by following the method of CITATION
Kazama and Torisawa CITATION extracted hyponymy relations from the first sentences (i.e. , defining sentences) of Wikipedia articles and then used them as a gazetteer for NER
Although this Wikipedia gazetteer is much smaller than the English version used by Kazama and Torisawa CITATION that has over 2 ,000 ,000 entries , it is the largest gazetteer that can be freely used for Japanese NER
We follow the method used by Kazama and Torisawa CITATION , which encodes the matching with a gazetteer entity using IOB tags , with the modification for Japanese
In the context of tagging , there are several studies that utilized word clusters to prevent the data sparseness problem CITATION
Inducing features for taggers by clustering has been tried by several researchers CITATION
We used MeCab as a morphological analyzer and CaboCha 14 CITATION as the dependency parser to find the boundaries of the bunsetsu
The corpus we used for collecting dependencies was a large set (76 million) of Web documents , that were processed by a dependency parser , KNP CITATION
We use Conditional Random Fields (CRFs) CITATION to perform this tagging
There are several studies that used automatically extracted gazetteers for NER CITATION
Shinzato et al. CITATION constructed gazetteers with about 100 ,000 entries in total for the "restaurant" domain; Talukdar et al. CITATION used gazetteers with about 120 ,000 entries in total , and Nadeau et al. CITATION used gazetteers with about 85 ,000 entries in total
Newman et al. CITATION presented parallelized Latent Dirichlet Allocation (LDA)
Rooth et al. CITATION and Torisawa CITATION showed that the EM-based clustering using verb-MN dependencies can produce semantically clean MN clusters
This study , on the other hand , utilized MN clustering based on verb-MN dependencies CITATION
Using models such as Semi-Markov CRFs CITATION , which handle the features on overlapping regions , is one possible direction
The system recently proposed by Sasano and Kurohashi CITATION is currently the best system for the IREX dataset
In our experiments , we used the IREX dataset CITATION to demonstrate the usefulness of cluster gazetteers
We parallelized the algorithm of CITATION using the Message Passing Interface (MPI) , with the prime goal being to distribute parameters and thus enable clustering with a large vocabulary
To learn p(n\c) and p(c) for Japanese , we use the EM-based clustering method presented by Torisawa CITATION
The exception , which we noticed recently , is a study by Wolfe et al. CITATION , which describes how each node stores only those parameters relevant to the training data on each node
We consider 10 measures , noted in the table as J&C CITATION , Resnik CITATION , Lin CITATION , W&P CITATION , L&C CITATION , H&SO CITATION , Path (counts edges between synsets) , Lesk CITATION , and finally Vector and Vector Pair CITATION
These kinds of measurements can help with problems such as identifying relevant sentences for extractive text summarization , or possibly paraphrase identification CITATION
We use WordNet 3.0 , the latest version CITATION
The 1911 and 1987 Thesauri were compared with WordNet 3.0 on the three data sets containing pairs of words with manually assigned similarity scores: 30 pairs CITATION , 65 pairs CITATION and 353 pairs 3 CITATION
Even on the largest set CITATION , however , the differences between Roget's Thesaurus and the Vector method are not statistically significant at the p < 0.05 level for either thesaurus on a two-tailed test 4
Other methods of determining sentence semantic relatedness expand term relatedness functions to create a sentence relatedness function CITATION
In CITATION , an even better system was proposed , with a correlation of 0.853
Lexical chains have also been developed using the 1987 Roget's Thesaurus CITATION
Kennedy and Szpakowicz CITATION show how disambiguating one of these relations , hypernymy , can help improve the semantic similarity functions in CITATION
Two terms in the same semicolon group score 16 , in the same paragraph - 14 , and so on CITATION
We used the system from CITATION for identifying synonyms with Roget's
The 1987 data come from Penguin's Roget's Thesaurus CITATION
We used three data sets for this application: 80 questions taken from the Test of English as a Foreign Language (TOEFL) CITATION , 50 questions - from the English as a Second Language test (ESL) CITATION and 300 questions - from the Reader's Digest Word Power Game (RDWP) CITATION
We also proposed a new method of representing the meaning of sentences or other short texts using either WordNet or Roget's Thesaurus , and tested it on the data set provided by Li et al. CITATION
We worked with a data set from CITATION
For the system in CITATION , where this data set was first introduced , a correlation of 0.816 with the human annotators was achieved
On the CITATION and CITATION data sets the best system did not show a statistically significant improvement over the 1911 or 1987 Roget's Thesauri , even at p < 0.1 for a two-tailed test
Much like CITATION , the data set used here is not large enough to determine if any system's improvement is statistically significant
Some work has been done on adding new terms and relations to WordNet CITATION and FACTOTUM CITATION
We used Pedersen's Semantic Distance software package CITATION
They took a subset of the term pairs from CITATION and chose sentences to represent these terms; the sentences are definitions from the Collins Cobuild dictionary CITATION
The idea of using a bridge (i.e. , full-form) to obtain translation entries for unseen words (i.e. , abbreviation) is similar to the idea of using paraphrases in MT (see Callison-Burch et al. CITATION and references therein) as both are trying to introduce generalization into MT
On the other hand , integrating an additional component into a baseline SMT system is notoriously tricky as evident in the research on integrating word sense disambiguation (WSD) into SMT systems: different ways of integration lead to conflicting conclusions on whether WSD helps MT performance CITATION
According to Chang and Lai CITATION , approximately 20% of sentences in a typical news article have abbreviated words in them
To create the baseline , we make use of the dominant abbreviation patterns shown in Table 5 , which have been reported in Chang and Lai CITATION
For the statistics on manually collected examples , please refer to Chang and Lai CITATION
Recently , Chang and Lai CITATION , Chang and Teng CITATION , and Lee CITATION have investigated this task
To identify their abbreviations , one can employ an HMM model CITATION
In comparison , Chang and Teng CITATION reports a precision of 50% over relations between single-word full-forms and single-character abbreviations
To handle different directions of translation between Chinese and English , we built two tri-gram language models with modified Kneser-Ney smoothing CITATION using the SRILM toolkit CITATION
While the research in statistical machine translation (SMT) has made significant progress , most SMT systems CITATION rely on parallel corpora to extract translation entries
However , since most of statistical translation models CITATION are symmetrical , it is relatively easy to train a translation system to translate from English to Chinese , except that we need to train a Chinese language model from the Chinese monolingual data
We carry out experiments on a state-of-the-art SMT system , i.e. , Moses CITATION , and show that the abbreviation translations consistently improve the translation performance (in terms of BLEU CITATION) on various NIST MT test sets
We integrate our method into a state-of-the-art phrase-based baseline translation system , i.e. , Moses CITATION , and show that the integrated system consistently improves the performance of the baseline system on various NIST machine translation test sets
In general , Chinese abbreviations are formed based on three major methods: reduction , elimination and generalization CITATION
Lee CITATION gives a summary about how Chinese abbreviations are formed and presents many examples
At last , the goal that we aim to exploit monolingual corpora to help MT is in-spirit similar to the goal of using non-parallel corpora to help MT as aimed in a large amount of work (see Munteanu and Marcu CITATION and references therein)
Moreover , our approach integrates the abbreviation translation component into the baseline system in a natural way , and thus is able to make use of the minimum-error-rate training CITATION to automatically adjust the model parameters to reflect the change of the integrated system over the baseline system
Once we obtain the augmented phrase table , we should run the minimum-error-rate training CITATION with the augmented phrase table such that the model parameters are properly adjusted
The feature functions are combined under a log-linear framework , and the weights are tuned by the minimum-error-rate training CITATION using BLEU CITATION as the optimization metric
We use MT02 as the development set 4 for minimum error rate training (MERT) CITATION
Using the toolkit Moses CITATION , we built a phrase-based baseline system by following the standard procedure: running GIZA++ CITATION in both directions , applying refinement rules to obtain a many-to-many word alignment , and then extracting and scoring phrases using heuristics CITATION
The MT performance is measured by lower-case 4-gram BLEU CITATION
Following studies on automatic SCF extraction CITATION , we apply a statistical test (Binomial Hypothesis Test) to the unfiltered-Levin-SCF to filter out noisy SCFs , and denote the resulting SCF set as filtered-Levin-SCF
It is therefore unsurprising that much work on verb classification has adopted them as features CITATION
However , some of the functions words , prepositions in particular , are known to carry great amount of syntactic information that is related to lexical meanings of verbs CITATION
One way to avoid these high-dimensional spaces is to assume that most of the features are irrelevant , an assumption adopted by many of the previous studies working with high-dimensional semantic spaces CITATION
SCF and DR: These more linguistically informed features are constructed based on the grammatical relations generated by the C&C CCG parser CITATION
Many scholars hypothesize that the behavior of a verb , particularly with respect to the expression of arguments and the assignment of semantic roles is to a large extent driven by deep semantic regularities CITATION
When the information about a verb type is not available or sufficient for us to draw firm conclusions about its usage , the information about the class to which the verb type belongs can compensate for it , addressing the pervasive problem of data sparsity in a wide range of NLP tasks , such as automatic extraction of subcategorization frames CITATION , semantic role labeling CITATION , natural language generation for machine translation CITATION , and deriving predominant verb senses from unlabeled data CITATION
Although the problem of data sparsity is alleviated to certain extent (3) , these features do not generally improve classification performance CITATION
Other methods for combining syntactic information with lexical information have also been attempted CITATION
Joanis et al. CITATION demonstrates that the general feature space they devise achieves a rate of error reduction ranging from 48% to 88% over a chance baseline accuracy , across classification tasks of varying difficulty
JOANIS07: We use the feature set proposed in Joanis et al. CITATION , which consists of 224 features
For example , Schulte im Walde CITATION uses 153 verbs in 30 classes , and Joanis et al. CITATION takes on 835 verbs and 15 verb classes
Although there exist several manually-created verb lexicons or ontologies , including Levin's verb taxonomy , VerbNet , and FrameNet , automatic verb classification (AVC) is still necessary for extending existing lexicons CITATION , building and tuning lexical information specific to different domains CITATION , and bootstrapping verb lexicons for new languages CITATION
Although dependency relations have been widely used in automatic acquisition of lexical information , such as detection of polysemy CITATION and WSD CITATION , their utility in AVC still remains untested
The software performs the so-called 1-of-k classification CITATION
We also lemmatize each word using the English lemmatizer as described in Minnen et al. CITATION , and use lemmas as features instead of words
Co-occurrence (CO): CO features mostly convey lexical information only and are generally considered not particularly sensitive to argument structures CITATION
In order to reduce undue influence of outlier features , we employ the four normalization strategies in table 4 , which help reduce the range of extreme values while having little effect on others CITATION
Trying to overcome the problem of data sparsity , Schulte im Walde CITATION explores the additional use of selectional preference features by augmenting each syntactic slot with the concept to which its head noun belongs in an ontology (e.g.WordNet)
A preparatory study on the capitalization of Portuguese BN has been performed by CITATION
Results from previous experiment are still worse than results achieved by other work on the area CITATION (about 94% precision and 88% recall), specially in terms of recall
The modeling approach here described is discriminative, and is based on maximum entropy (ME) models, firstly applied to natural language problems in CITATION
The capitalization problem can be seen as a sequence tagging problem CITATION, where each lower-case word is associated to a tag that describes its capitalization form
Concerning this subject, CITATION shows that, as the time gap between training and test data increases, the performance of a named tagger based on co-training CITATION decreases
Evaluation results may be influenced when taking such words into account CITATION
The capitalization task, also known as truecasing CITATION, consists of rewriting each word of an input text with its proper case information
CITATION builds a trigram language model (LM) with pairs (word, tag), estimated from a corpus with case information, and then uses dynamic programming to disambiguate over all possible tag assignments on a sentence
The evaluation is performed using the metrics: Precision, Recall and SER (Slot Error Rate) CITATION
In fact, subtitling of BN has led us into using a baseline vocabulary of 100K words combined with a daily modification of the vocabulary CITATION and a re-estimation of the language model
Other related work includes a bilingual capitalization model for capitalizing machine translation (MT) outputs, using conditional random fields (CRFs) reported by CITATION
Whereas the more commonly applied Akaike Information Criterion CITATION requires the number of estimated parameters to be determined exactly, the DIC facilitates the evaluation of mixed-effects models by relaxing this requirement
A growing body of work in cognitive science characterizes human readers as some kind of probabilistic parser CITATION
This observation is consistent with Brants and Crocker's CITATION observation that accuracy can be maintained even when restricted to 1% of the memory required for exhaustive parsing
This basic notion has proved remarkably applicable across sentence types and languages CITATION
The length of time that a reader's eyes spend fixated on a particular word in a sentence is known to be affected by a variety of word-level factors such as length in characters, n-gram frequency and empirical predictability CITATION
From a cognitive perspective, the utility of small k parsers for modeling comprehension difficulty lends credence to the view that the human processor is a single-path analyzer CITATION
Hale CITATION suggests this quantity as an index of psycholinguistic difficulty
The parser's outputs define a relation on word pairs CITATION
Our methodology imposes this requirement by fitting a kind of regression known as a linear mixed-effects model to the total reading times associated with each sentence-medial word in the Potsdam Sentence Corpus (PSC) CITATION
When more than one transition is applicable, the parser decides between them by consulting a probability model derived from the Negra and Tiger newspaper corpora CITATION
From the theoretical side, we calculate word-by-word surprisal predictions from a family of incremental dependency parsers for German based on Nivre CITATION; these parsers differ only in the size k of the beam used in the search for analyses of longer and longer sentence-initial substrings
Perhaps human parsing is boundedly rational in the sense of the bound imposed by Stack3 CITATION
We evaluated the change in relative 7 quality of fit due to surprisal with the Deviance Information Criterion (DIC) discussed in Spiegelhalter et al.CITATION
Because of this, CITATION defines the tree kernel algorithm whose computational complexity does not depend on m
Recently, the graph-based method (LexRank) is applied successfully to generic, multi-document summarization CITATION
In CITATION, the concept of graph-based centrality is used to rank a set of sentences, in producing generic multi-document summaries
Initiatives such as PropBank (PB) CITATION have made possible the design of accurate automatic Semantic Role Labeling (SRL) systems like ASSERT CITATION
We carried out automatic evaluation of our summaries using ROUGE CITATION toolkit, which has been widely adopted by DUC for automatic summarization evaluation
A topic-sensitive LexRank is proposed in CITATION
To apply LexRank to query-focused context, a topic-sensitive version of LexRank is proposed in CITATION
So far, linguistic cues have played an important role in research of subjectivity recognition (e.g.CITATION), sentiment analysis (e.g.CITATION), and emotion studies (e.g.CITATION)
Wiebe CITATION further adapted this definition of subjectivity to be "the linguistic expression of private states CITATION"
They have shown that subjectivity annotations can be helpful for word sense disambiguation when a word has distinct subjective senses and objective senses CITATION
First, non-objectivity cannot be clearly identified without knowledge about its source CITATION
Second, non-objectivity always lies in a context, which cannot be ignored CITATION
We have examined sentence extraction agreement between experts using the prevalence-adjusted bias-adjusted (PABA) kappa to account for prevalence of judgments and conflicting biases amongst experts CITATION
The gazetteer feature checks named entities from each sentence against the Alexandria Digital Library (ADL) Gazetteer CITATION
Knowledge maps consist of nodes containing rich concept descriptions interconnected using a limited set of relationship types CITATION
We use ROUGE CITATION to assess summary quality using common n-gram counts and longest common subsequence (LCS) measures
Lin and Pantel CITATION discover concepts using clustering by committee to group terms into conceptually related clusters
On-toLearn extracts candidate domain terms from texts using a syntactic parse and updates an existing ontology with the identified concepts and relationships CITATION
Learning research indicates that knowledge maps may be useful for learners to understand the macro-level structure of an information space CITATION
Finally, MEAD is a widely used MDS and evaluation platform CITATION
We have implemented an extractive summarizer for educational science content, COGENT, based on MEAD version 3.11 CITATION
Knowledge Puzzle focuses on n-gram identification to produce a list of candidate terms pruned using information extraction techniques to derive the ontology CITATION
For some restricted combinatorial spaces of alignments those that arise in ITG-based phrase models CITATION or local distortion models CITATION inference can be accomplished using polynomial time dynamic programs
However, for more permissive models such as Marcu and Wong CITATION and DeNero et al.CITATION, which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited
Then, we can formally define the set of bijective phrase alignments: A = |J   dj = e ;   |J   f ki = f > Both the conditional model of DeNero et al.CITATION and the joint model of Marcu and Wong CITATION operate in A, as does the phrase-based decoding framework of Koehn et al.CITATION
DeNero et al.CITATION instead proposes an exponential-time dynamic program to systematically explore A, which can in principle solve either O or ?NP NP NP CITATION RB VVZ DT JJ JJ NN TO RB VV DT , WDT MD IN NN VV DT NN CC $
Using an off-the-shelf ILP solver, 4 we were able to quickly and reliably find the globally optimal phrase alignment under  <fi(eij, fki) derived from the Moses pipeline CITATION
Forced decoding arises in online discriminative training, where model updates are made toward the most likely derivation of a gold translation CITATION
The existence of a polynomial time algorithm for ?implies a polynomial time algorithm for S, because A = 3  Complexity of Inference in  A For the space A of bijective alignments, problems ?and O have long been suspected of being NP-hard, first asserted but not proven in Marcu and Wong CITATION
Marcu and Wong CITATION describes an approximation to O
CPM is #P-complete CITATION, so S (and hence ? is #P-hard
We selected a subset of the verbs annotated in the OntoNotes project CITATION that had at least 50 instances
The approaches to obtaining this kind of knowledge can be based on extracting it from ele c-tronic dictionaries such as WordNet CITATION, using Named Entity (NE) tags, or a combi-nation of both CITATION
An automatic VSD system usually has at its disposal a diverse set of features among which the semantic features play an important role: verb sense distinctions often depend on the distinctions in the semantics of the target verb's arguments CITATION
Hindle CITATION grouped nouns into thesaurus-like lists based on the similarity of their syntactic contexts
To collect this data, we utilized two resources: (1) MaltParser CITATION - a high-efficiency dependency parser; (2) English Gigaword - a large corpus of 5.7M news articles
Other researches attacked the problem of unsupervised extraction of world knowledge: Schubert CITATION reports a method for extracting general facts about the world from tree-banked Brown corpus
Schutze CITATION used bag-of-words contexts for sense discrimination
Another approach to fine-grained tagging captures grammatical structures with tree-based tags, such as "supertags" in the tree-adjoining grammar of Bangalore and Joshi CITATION
Like previous Icelandic work CITATION, morphological analyzers disambiguate words before statistical tagging in Arabic CITATION and Czech CITATION
Given these challenges, the most successful tagger is IceTagger CITATION, a linguistic rule based system with several linguistic resources: a morphological analyzer, a series of local rules and heuristics for handling PPs, verbs, and forcing agreement
Our BoostedMERT should not be confused with other boosting algorithms such as CITATION
We used a standard phrase-based statistical MT system CITATION to generated N-best lists CITATION on  Developments,  Developments, and  Evaluation sub-sets
These two models can be combined with the entity grid described by Lapata and Barzilay CITATION for significant improvement
These models typically view a sentence either as a bag of words CITATION or as a bag of entities associated with various syntactic roles CITATION
Since the correct labeling depends on the coref-erence relationships between the NPs, we need some way to guess at this; we take all NPs with the same head to be coreferent, as in the non-coreference version of CITATION 2
As a baseline, we adopt the entity grid CITATION
In the discrimination task CITATION, a document is compared with a random permutation of its sentences, and we score the system correct ifitindicates the original as more coherent 4
As mentioned, Barzilay and Lapata CITATION uses a coreference system to attempt to improve the entity grid, but with mixed results
Previous work has focused on the AIRPLANE corpus CITATION, which contains short announcements of airplane crashes written by and for domain experts
Models of coherence have been used to impose an order on sentences for multidocument summarization CITATION, to evaluate the quality of human-authored essays CITATION, and to insert new information into existing documents CITATION
Therefore we also test our systems on the task of insertion CITATION, in which we remove a sentence from a document, then find the point of insertion which yields the highest coherence score
We construct a maximum-entropy classifier using syntactic and lexical features derived from Uryupina CITATION, and a publicly available learning tool CITATION
These patterns have been studied extensively, by linguists CITATION and in the field of coreference resolution
Features such as full names, appositives, and restrictive relative clauses are associated with the introduction of unfamiliar entities into discourse CITATION
Another issue is that NPs whose referents are familiar tend to resemble discourse-old NPs, even though they have not been previously mentioned CITATION
We use a model which probabilistically attempts to describe these preferences CITATION
(This takes more work than simply resolving the pronouns conditioned on the text.) The model of Ge et al.CITATION provides the requisite probabilities: P (a i ,T i \d~ 1) =P (a i \h(a i ),m(a ,i)) Here h(a) is the Hobbs distance CITATION, which measures distance between a pronoun and prospective antecedent, taking into account various factors, such as syntactic constraints on pronouns
The model is trained using a small hand-annotated corpus first used in Ge et al.CITATION
Centering theory CITATION describes additional constraints about which entities in a discourse can be pronominalized: if there are pronouns in a segment, they must include the backward-looking center
As noted by studies since Hawkins CITATION, there are marked syntactic differences between the two classes
We evaluate our models using two tasks, both based on the assumption that a human-authored document is coherent, and uses the best possible ordering of its sentences (see Lapata CITATION)
The system of Nenkova and McKeown CITATION works in the opposite direction
Classifiers in the literature include CITATION
For the discourse-new classification task, the model's most important feature is whether the head word of the NP to be classified has occurred previously (as in Ng and Cardie CITATION and Vieira and Poesio CITATION)
This model outperforms a variety of word overlap and semantic similarity models, and is used as a component in the state-of-the-art system of Soricut and Marcu CITATION
Both of these models are very different from the lexical and entity-based models currently used for this task CITATION, and are probably capable of improving the state of the art
Our first model distinguishes discourse-new from discourse-old noun phrases, using features based on Uryupina CITATION
The b 3 scorer CITATION was proposed to overcome several shortcomings of the MUC scorer
Other work on global models of coreference (as opposed to pairwise models) has included: Luo et al.CITATION who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree; Mc-Callum and Wellner CITATION who defined several conditional random field-based models; Ng CITATION who took a reranking approach; and Culotta et al.CITATION who use a probabilistic first-order logic model
Much work that followed improved upon this strategy, by improving the features CITATION, the type of classifier CITATION, and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent CITATION
More recently, Denis and Baldridge CITATION utilized an integer linear programming (ILP) solver to better combine the decisions made by these two complementary classifiers, by finding the globally optimal solution according to both classifiers
When describing our model, we build upon the notation used by Denis and Baldridge CITATION
Prior work CITATION has generated training data for pairwise classifiers in the following manner
The coref-ilp model of Denis and Baldridge CITATION took a different approach at test time: for each mention they would work backwards and add a link for all previous mentions which the classifier deemed coreferent
4 We added named entity (NE) tags to the data using the tagger of Finkel et al.CITATION
In addition to the MUC and b 3 scorers, we also evaluate using cluster f-measure CITATION, which is the standard f-measure computed over true/false coreference decisions for pairs of mentions; the Rand index CITATION, which is pairwise accuracy of the clustering; and variation of information CITATION, which utilizes the entropy of the clusterings and their mutual information (and for which lower values are better)
Ng and Cardie CITATION and Ng CITATION highlight the problem of determining whether or not common noun phrases are anaphoric
Much recent work on coreference resolution, which is the task of deciding which noun phrases, or mentions, in a document refer to the same real world entity, builds on Soon et al.CITATION
Our feature set was simple, and included many features from CITATION, including the pronoun, string match, definite and demonstrative NP, number and gender agreement, proper name and appositive features
Our Soon-style baseline used the same training and testing regimen as Soon et al.CITATION
We also added part of speech (POS) tags to the data using the tagger of Toutanova et al.CITATION, and used the tags to decide if mentions were plural or singular
The MUC scorer CITATION is a popular coreference evaluation metric, but we found it to be fatally flawed
System utterances were generated using a simple template-based algorithm and synthesised using the speech synthesis system Cerevoice CITATION, which has been shown to be intelligible to older users CITATION
Older people are a user group with distinct needs and abilities CITATION that present challenges for user modelling
They are also in line with findings of tests of deployed Interactive Voice Response systems with younger and older users CITATION, which show the diversity of older people's behaviour
In order to learn good policies, the behaviour of the SUs needs to cover the range of variation seen in real users CITATION
Our data comes from a fully annotated corpus of 447 interactions of older and younger users with a Wizard-of-Oz (WoZ) appointment scheduling system CITATION
Currently one of the standard methods for evaluating the quality of a SU is to run a user simulation on a real corpus and measure how often the action generated by the SU agrees with the action observed in the corpus CITATION
Given a history of system and user actions (n-1 actions) the SU generates an action based on a probability distribution learned from the training data CITATION
The actions generated by our SUs were compared to the actions observed in the corpus using five metrics proposed in the literature CITATION: perplexity (PP), precision, recall, expected precision and expected recall
A detailed description of the corpus design, statistics, and annotation scheme is provided in CITATION
There are 28 distinct user speech acts CITATION
This finding supports the principle of "inclusive design" CITATION: designers should consider a wide range of users when developing a product for general use
The only statistical spoken dialogue system for older people we are aware of is Nursebot, an early application of statistical methods (POMDPs) within the context of a medication reminder system CITATION
We then evaluate these models using standard metrics CITATION and compare our findings with the results of statistical corpus analysis
Finally, a linear model is trained using a variation of the averaged perceptron CITATION algorithm
We use a linear classifier trained with a regularized perceptron update rule CITATION as implemented in SNoW, CITATION
CITATION bootstrap with a classifier used interchangeably with an un-supervised temporal alignment method
We evaluated our approach in two settings; first, we compared our system to a baseline system described in CITATION
Previous works usually take a generative approach, CITATION
The idea of selectively sampling training samples has been wildly discussed in machine learning theory CITATION and has been applied successfully to several NLP applications CITATION
Other approaches exploit similarities in aligned bilingual corpora; for example, CITATION combine two unsupervised methods
DictEx We extend the phrase table with entries from a manually created dictionary - the English glosses of the Buckwalter Arabic morphological analyzer CITATION
Our transliteration system is rather simple: it uses the transliteration similarity measure described by Freeman et al.CITATION to select a best match from a large list of possible names in English
More details are available in a technical report CITATION
We tokenize using the Mada morphological disambiguation system CITATION, and Tokan, a general Arabic tokenizer CITATION
There is much work on name transliteration and its integration in larger MT systems CITATION
We decode using Pharaoh CITATION
Word alignment is done with GIZA++ CITATION
The first 200 sentences in the 2002 MTEval test set were used for Minimum Error Training (MERT) CITATION
Okuma et al.CITATION describe a dictionary-based technique for translating OOV words in SMT
6 We report results in terms of case-insensitive 4-gram BLEU CITATION scores
This is especially true for languages with rich morphology such as Spanish, Catalan, and Serbian CITATION and Arabic CITATION
We address some of these challenges in our baseline system by removing all diacritics, normalizing Alif and Ya forms, and tokenizing Arabic text in the highly competitive Arabic Treebank scheme CITATION
All evaluated systems use the same surface trigram language model, trained on approximately 340 million words from the English Gigaword corpus CITATION using the SRILM toolkit CITATION
Vilar et al.CITATION address spelling-variant OOVs in MT through online re-tokenization into letters and combination with a word-based system
Some previous approaches anticipate OOV words that are potentially morphologically related to in-vocabulary (INV) words CITATION
Jewish Law documents written in Hebrew are known to be rich in ambiguous abbreviations CITATION
In our previous research CITATION, we developed a prototype abbreviation disambiguation system for Jewish Law documents written in Hebrew, without using any ML method
The numerical sum of the numerical values attributed to the Hebrew letters forming the abbreviation CITATION
The one sense per discourse hypothesis (OS) was introduced by Gale et al.CITATION
Systems developed by Pakhomov CITATION, Yu et al.CITATION and Gaudan et al.CITATION achieved 84% to 98% accuracy
Yosef CITATION
Active learning (AL) can be employed to reduce the costs of corpus annotation CITATION
The easiest solution is to normalize by sentence length, as has been done previously CITATION
We follow Engelson & Dagan CITATION in the implementation of vote entropy for sentence selection using these models
In the context of parse tree annotation, Hwa CITATION estimates cost using the number of constituents needing labeling and Osborne & Baldridge CITATION use a measure related to the number of possible parses
One exception is CITATION (discussed later) which compares the cost of manual rule writing with AL-based annotation for noun phrase chunking
In contrast to the model presented by Ngai and Yarowsky CITATION, which predicts monetary cost given time spent, this model estimates time spent from characteristics of a sentence
We also consider another selection algorithm introduced in CITATION that eliminates the overhead of entropy computations altogether by estimating per-sentence uncertainty with 1 ?P(|), where f is the Viterbi (best) tag sequence
In prior work, we describe such a cost model for POS annotation on the basis of the time required for annotation CITATION
Perhaps the best known are Query by Committee (QBC) CITATION and uncertainty sampling (or Query by Uncertainty, QBU) CITATION
Our implementation of QBC employs a committee of three MEMM taggers to balance computational cost and diversity, following Tomanek et al.CITATION
The features used in this work are typical for modern MEMM POS tagging and are mostly based on work by Toutanova and Manning CITATION
Though there has been a growing interest in semi-supervised learning CITATION, it is in an early phase of development
Previous domain resources include WordNet CITATION and HowNet CITATION, among others
In this study, the 12 domains in Table 1 are used following CITATION (H&K hereafter) 1
Previous text categorization methods like Joachims CITATION and Schapire and Singer CITATION are mostly based on machine learning
Ko and Seo CITATION automatically collect training data using a large amount of unlabeled data and a small amount of seed information
This is consistent with Kornai et al.CITATION, who claim that only positive evidence matter in categorization
Liu et al.CITATION prepare representative words for each class, by which they collect initial training data to build classifier
Magnini et al.CITATION show the effectiveness of domain information for WSD
Piao et al.CITATION use domain tags to extract MWEs
We have implemented a Mixture Model POMDP architecture as a multi-state version of the DIPPER "Information State Update" dialogue manager CITATION
An alternative is to apply automatically learned reordering rules to the test sentences before decoding CITATION
Our translation system is based on the CMU SMT decoder as described in CITATION
We used the Pharaoh/Moses package CITATION to extract and score phrase pairs using the grow-diag-final extraction method
To accelerate the training of word alignment models we implemented a distributed version of GIZA++ CITATION, based on the latest version of GIZA++ and a parallel version developed at Peking University CITATION
In this paper we report results using the BLEU metric CITATION, however as the evaluation criterion in GALE is HTER CITATION, we also report in TER CITATION
Every outgoing edge of a node is scored with the relative frequency of the pattern used on the following sub path (For details see CITATION)
We trained separate open vocabulary language models for each source and interpolated them using the SRI Language Modeling Toolkit CITATION
Our preprocessing steps include tokenization on the English side and for Chinese: automatic word segmentation using the revised version of the Stanford Chinese Word Segmenter 2 CITATION from 2007, replacement of traditional by simplified Chinese characters and 2-byte to 1-byte ASCII character normalization
In order to find an optimal set of weights, we use MER training as described in CITATION, which uses rescoring of the top n hypotheses to maximize an evaluation metric like BLEU or TER
Note that our contributions in this paper could be applied to arbitrary lattice topologies.) For example, Bangalore et al.CITATION show how to build a confusion network following a multistring alignment procedure of several MT outputs
The BLEU oracle sentences were found using the dynamic-programming algorithm given in Dreyer et al.CITATION and measured using Philipp Koehn'seval-uation script
In Model II, the semi-supervised setup, the training data is used to initialize the Expectation-Maximization (EM) algorithm CITATION and the unlabeled data, described in Section 3.1, updates the initial estimates
225 words were selected for manual annotation as homograph or non-homograph by random sampling of words that were on the above list and used in prior psycholinguistic studies of homographs CITATION or on the Academic Word List CITATION
However, making fine-grained sense distinctions for words with multiple closely-related meanings is a subjective task CITATION, which makes it difficult and error-prone
Lexical ambiguity resolution is an important research problem for the fields of information retrieval and machine translation CITATION
Fine-grained sense distinctions aren't necessary for many tasks, thus a possibly-simpler alternative is lexical disambiguation at the level of homographs CITATION
The above two work was further advanced by Bunescu and Mooney CITATION who argued that the information to extract a relation between two entities can be typically captured by the shortest path between them in the dependency graph
In the feature-based framework, Kambhatla CITATION employed ME models to combine diverse lexical, syntactic and semantic features derived from word, entity type, mention level, overlap, dependency and parse tree
Zelenko et al CITATION proposed a kernel over two parse trees, which recursively matched nodes from roots to leaves in a top-down manner
Based on his work, Zhou et al CITATION further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list
Later, Zhang et al CITATION developed a composite kernel that combined parse tree kernel with entity kernel and Zhou et al CITATION experimented with a context-sensitive kernel by automatically determining context-sensitive tree spans
Che et al CITATION defined an improved edit distance kernel over the original Chinese string representation around particular entities
It is not difficult to list all of those characters that have the same or similar pronunciations, e.g., "t^AI" and ""fAI", if we have a machine readable lexicon that provides information about pronunciations of characters and when we ignore special patterns for tone sandhi in Chinese CITATION
Bong-Foo Chu, selected a set of 24 basic elements in Chinese characters, and proposed a set of rules to decompose Chinese characters into elements that belong to this set of building blocks CITATION
Figure 4 illustrates possible layouts of the components in Chinese characters that were adopted by the Cangjie method CITATION
There are more than 22000 different characters in large corpus of Chinese documents CITATION, so directly computing the similarity between images of these characters demands a lot of computation
Taft, Zhu, and Peng CITATION investigated the effects of positions of radicals on subjects' lexical decisions and naming responses
Yeh and Li CITATION studied how similar characters influenced the judgments made by skilled readers of Chinese
In this case, the "best" answer may be chosen by the votes, or alternatively by automatically predicting answer quality (e.g., CITATION or CITATION)
Category Features: We hypothesized that user behavior (and asker satisfaction) varies by topical question category, as recently shown in reference CITATION
While information seeker satisfaction has been studied in ad-hoc IR context (see CITATION for an overview), previous studies have been limited by the lack of realistic user feedback
In our recent work CITATION we have introduced a general model for predicting asker satisfaction in community question answering
For more detailed treatment of user interactions in CQA see CITATION
We now briefly review our ASP (Asker Satisfaction Prediction) framework that learns to classify whether a question has been satisfactorily answered, originally introduced in CITATION
Furthermore, while automatic complex QA has been an active area of research, ranging from simple modification to factoid QA technique (e.g., CITATION) to knowledge intensive approaches for specialized domains, the technology does not yet exist to automatically answer open domain, complex, and subjective questions
Classification Algorithms: We experimented with a variety of classifiers in the Weka framework CITATION
However more recent results have shown that it can indeed improve parser performance CITATION
Two previous papers would seem to address this issue: the work by Bacchiani et al.CITATION and McClosky et al.CITATION
2 A close second (1% behind) was the parser of Bikel CITATION
While self-training has worked in several domains, the early results on self-training for parsing were negative CITATION
Section three describes our main experiment on standard test data CITATION
Clegg and Shepherd CITATION do not provide separate precision and recall numbers
In contrast, the out-of-vocabulary rate of biomedical abstracts given the same lexicon is significantly higher at about 25% CITATION
Lease and Charniak CITATION achieve their results using small amounts of hand-annotated biomedical part-of-speech-tagged data and also explore other possible sources or information
Marcus et al., 1993)) and then self-training on a second type of data in order to adapt the parser to the second domain
So, for example, McClosky et al.CITATION found that the data from the handannotated WSJ data should be considered at least five times more important than NANC data on an event by event level
However, several very good current parsers were not available when this paper was written (e.g., the Berkeley Parser CITATION)
Bacchiani and Roark train the Roark parser CITATION on trees from the Brown treebank and then self-train and test on data from Wall Street Journal
1 Steedman et al.CITATION generally found that self-training does not work, but found that it does help if the baseline results were sufficiently bad
Speech repairs are common in spontaneous speech - one study found 30% of dialogue turns contained repairs CITATION and another study found one repair every 4.8 seconds CITATION
Recent advances in recognizing spontaneous speech with repairs CITATION have used parsing approaches on transcribed speech to account for the structure inherent in speech repairs at the word level and above
I I Figure 1 : Standard tree repair structure, with -UNF propagation as in CITATION shown in brackets
Figure 1 also shows, in brackets, the augmented annotation used by Hale et al.CITATION
The approach used by CITATION works because the information about the transition to an error state is propagated up the tree, in the form of the -UNF tags
With this representation, the problem noticed by Hale and colleagues CITATION has been solved in a different way, by incrementally building up left-branching rather than right-branching structure, so that only a single special error rule is required at the end of the constituent
To make a fair comparison to the CYK baseline of CITATION, the recognizer was given correct part-of-speech tags as input along with words
The evaluation of this system was performed on the Switchboard corpus, using the mrg annotations in directories 2 and 3 for training, and the files sw4004.mrg to sw4153.mrg in directory 4 for evaluation, following Johnson and CharniakCITATION
The TAG system CITATION achieves a higher EDIT-F score, largely as a result of its explicit tracking of overlapping words between reparanda and alterations
In order to obtain a linguistically plausible right-corner transform representation of incomplete constituents, the Switchboard corpus is subjected to a pre-process transform to introduce binary-branching nonterminal projections, and fold empty categories into nonterminal symbols in a manner similar to that proposed by Johnson CITATION and Klein and Manning CITATION
The speech repair terminology used here follows that of Shriberg CITATION
The incomplete constituent categories created by the right corner transform are similar in form and meaning to non-constituent categories used in Combinatorial Categorial Grammars (CCGs) CITATION
The classifiers generally mimic human judgements in that accuracy is much lower in the three-way classification task - a pattern concurring with past observations (cf.Esuli and Sebastiani CITATION; Andreevskaia and Bergler CITATION)
Non-neutral adjectives were extracted from WordNet and assigned fuzzy sentiment category member-ship/centrality scores and tags in Andreevskaia and Bergler CITATION
The semi-supervised learning method in Esuli and Sebastiani CITATION involves constructing a training set of non-neutral words using WordNet synsets, glosses and examples by iteratively adding syn- and antonyms to it and learning a term classifier on the glosses of the terms in the training set
Esuli and Sebastiani CITATION used the method to cover objective (n) cases
Hatzivassiloglou and McKeown CITATION clustered adjectives into (+) and (-) sets based on conjunction constructions, weighted similarity graphs, minimum-cuts, supervised learning, and clustering
Kaji and Kitsuregawa CITATION describe a method for harvesting sentiment words from non-neutral sentences extracted from Japanese web documents based on structural layout clues
Since not all constituents are of equal importance, the sentiment salience of each subconstituent is estimated using a subset of the grammatical polarity rankings and compositional processes proposed in Moilanen and Pulman CITATION
Riloff et al.CITATION mined subjective nouns from unannotated texts with two bootstrapping algorithms that exploit lexico-syntactic extraction patterns and manually-selected subjective seeds
Takamura et al.CITATION apply to words' polarities a physical spin model inspired by the behaviour of electrons with a (+) or (-) direction, and an iterative term-neighbourhood matrix which models magnetisation
A popular, more general unsupervised method was introduced in Turney and Littman CITATION which induces the polarity of a word from its Pointwise Mutual Information (PMI) or Latent Semantic Analysis (LSA) scores obtained from a web search engine against a few paradigmatic (+) and (-) seeds
Strong adjectival subjectivity clues were mined in Wiebe CITATION with a distributional similarity-based word clustering method seeded by hand-labelled annotation
This is the reason why our kernels on linguistic structures improve it by 63%, which is a remarkable result for an IR task CITATION
Question Answering (QA) is an IR task where the major complexity resides in question processing and answer extraction CITATION rather than document retrieval (a step usually carried out by off-the shelf IR engines)
representations: (1) linear kernels on the bag-of-words (BOW) or bag-of-POS-tags (POS) features, (2) the String Kernel (SK) CITATION on word sequences (WSK) and POStag sequences (POSSK), (3) the Syntactic Tree Kernel (STK) CITATION on syntactic parse trees (PTs), (4) the Shallow Semantic Tree Kernel (SSTK) CITATION and the Partial Tree Kernel (PTK) CITATION on PASs
Then, two PAS-based trees: Shallow Semantic Trees for SSTK and Shallow Semantic Trees for PTK, both based on PropBank structures CITATION are automatically generated by our SRL system CITATION
The experimental datasets were created by submitting the 138 TREC 2001 test questions labeled as "description" in CITATION to our basic QA system, YourQA CITATION and by gathering the top 20 answer paragraphs
As a kernel operator, we applied the sum between kernels 5 that yields the joint feature space of the individual kernels CITATION
Although typical methods are based exclusively on word similarity between query and answer, recent work, e.g.CITATION has shown that shallow semantic information in the form of predicate argument structures (PASs) improves the automatic detection of correct answers to a target question
Since the dictionary we use, BAMA CITATION, also includes diacritics (orthographic marks not usually written), we extend this approach to the diacritization task in CITATION
Hajic et al.CITATION implement the approach of Hajic CITATION for Arabic
Hajic CITATION is the first to use a dictionary as a source of possible morphological analyses (and hence tags) for an inflected word form
We use an implementation of a Downhill Simplex Method in many dimensions based on the method developed by Nelder and Mead CITATION to tune the weights applied to each feature
We also build 4-gram lexeme models using an open-vocabulary language model with Kneser-Ney smoothing, by means of the SRILM toolkit CITATION
These 19 features consist of the 14 morphological features shown in Figure 1, which MADA predicts using 14 distinct Support Vector Machines trained on ATB3-Train (as defined by Zitouni et al.CITATION)
